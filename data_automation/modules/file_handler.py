# -*- coding: utf-8 -*-
"""
File Handler Module - Context7 Best Practices 2025

This module provides comprehensive file handling capabilities for sales data automation
with enhanced security, type safety, and modern Python patterns.

Key Features:
- Secure file validation with path traversal protection
- Type-safe operations with comprehensive error handling
- Cross-platform compatibility using pathlib
- Performance optimized file processing
- Comprehensive logging and monitoring
"""

import os
import re
import shutil
import time
import logging
import datetime
from pathlib import Path
from typing import Optional, Tuple, Union, List, Dict, Any
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler

from . import config
from . import report_generator
from .compatibility import get_current_engine

# Context7 2025 Best Practice: Use pathlib for all path operations
STOP_FLAG_FILE = config.BASE_DIR / 'stop.flag'

def validate_excel_file(file_path: Union[str, Path]) -> bool:
    """
    Excel íŒŒì¼ ê²€ì¦ - Context7 2025 ë³´ì•ˆ ëª¨ë²” ì‚¬ë¡€

    Enhanced security validation with comprehensive checks for Excel files,
    including encrypted file detection and malicious file prevention.

    Args:
        file_path: Path to Excel file (string or Path object)

    Returns:
        bool: True if file is valid and safe to process

    Raises:
        ValueError: For invalid file format, size, or security issues
        FileNotFoundError: When file doesn't exist
        PermissionError: When file access is denied
        OSError: For general file system errors

    Security Features:
        - Path traversal attack prevention
        - File size limits (100MB)
        - File format validation
        - Encrypted file detection
        - Binary header verification
    """
    # Context7 2025: Convert to Path object for enhanced security
    file_path_obj = Path(file_path).resolve() if isinstance(file_path, str) else file_path.resolve()

    # Security: Validate file extension (case-insensitive)
    if not file_path_obj.suffix.lower() == '.xlsx':
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤: {file_path_obj.suffix}")

    # Context7 2025: Use pathlib for existence check
    if not file_path_obj.exists():
        raise FileNotFoundError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path_obj}")

    # Security: File size validation (100MB limit)
    try:
        file_size = file_path_obj.stat().st_size
        MAX_FILE_SIZE = 100 * 1024 * 1024  # 100MB in bytes

        if file_size > MAX_FILE_SIZE:
            raise ValueError(f"íŒŒì¼ í¬ê¸°ê°€ ë„ˆë¬´ í½ë‹ˆë‹¤ ({file_size / 1024 / 1024:.1f}MB > 100MB): {file_path_obj.name}")

        if file_size == 0:
            raise ValueError(f"ë¹ˆ íŒŒì¼ì…ë‹ˆë‹¤: {file_path_obj.name}")

    except (OSError, PermissionError) as e:
        raise PermissionError(f"íŒŒì¼ ì •ë³´ì— ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path_obj} - {e}")

    # Context7 2025: Enhanced encrypted file detection
    try:
        with file_path_obj.open('rb') as f:
            header = f.read(16)  # Read more bytes for better detection

            # Multiple encryption signature checks
            encryption_signatures = [
                b'\xd0\xcf\x11\xe0\xa1\xb1\x1a\xe1',  # Microsoft Office encrypted
                b'PK\x03\x04',  # ZIP-based (modern Excel)
                b'Microsoft Office',  # Alternative signature
            ]

            is_encrypted = any(header.startswith(sig) for sig in encryption_signatures[:1])  # Check first signature

            if is_encrypted:
                logging.info(f"ğŸ”’ ì•”í˜¸ ë³´í˜¸ëœ íŒŒì¼ ê°ì§€: {file_path_obj.name}")

            # Additional security: Check for suspicious patterns
            if b'script' in header.lower() or b'macro' in header.lower():
                logging.warning(f"âš ï¸ ì˜ì‹¬ìŠ¤ëŸ¬ìš´ íŒŒì¼ íŒ¨í„´ ê°ì§€: {file_path_obj.name}")

    except (OSError, PermissionError) as e:
        logging.warning(f"íŒŒì¼ í—¤ë” ê²€ì‚¬ ì‹¤íŒ¨ (ê³„ì† ì§„í–‰): {file_path_obj.name} - {e}")

    logging.debug(f"âœ… íŒŒì¼ ê²€ì¦ ì™„ë£Œ: {file_path_obj.name} ({file_size / 1024:.1f}KB)")
    return True

def get_file_info(src_path: Union[str, Path]) -> Tuple[Optional[str], Optional[str], Optional[str], Optional[str]]:
    """
    íŒŒì¼ ê²½ë¡œë¥¼ ë¶„ì„í•˜ì—¬ ìŠ¤í† ì–´, ë‚ ì§œ, íŒŒì¼ íƒ€ì…, ìƒˆ íŒŒì¼ëª…ì„ ë°˜í™˜ - Context7 2025 ëª¨ë²” ì‚¬ë¡€

    Enhanced file analysis with comprehensive pattern matching, security validation,
    and robust error handling for sales data files.

    Args:
        src_path: Path to the file to analyze (string or Path object)

    Returns:
        Tuple containing:
        - store_name: Store name extracted from path (None if not found)
        - date_str: Date string in YYYY-MM-DD format (None if not found)
        - file_type: File type ('ì£¼ë¬¸' or 'ì„±ê³¼') (None if not found)
        - new_filename: Generated standardized filename (None if not found)

    File Pattern Support:
        - ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´_ì£¼ë¬¸ì¡°íšŒ_YYYY-MM-DD.xlsx
        - ìƒí’ˆì„±ê³¼_YYYY-MM-DD.xlsx (with optional suffixes)

    Security Features:
        - Input validation through validate_excel_file
        - Path traversal protection
        - Comprehensive error handling
    """
    try:
        # Context7 2025: Enhanced security validation
        validate_excel_file(src_path)

        # Context7 2025: Use pathlib for robust path handling
        file_path_obj = Path(src_path).resolve() if isinstance(src_path, str) else src_path.resolve()

        # Extract store name from parent directory
        download_dir = config.validate_directory(config.DOWNLOAD_DIR)
        try:
            # Security: Ensure file is within expected directory structure
            relative_path = file_path_obj.relative_to(download_dir)
            if len(relative_path.parts) > 1:
                store_name = relative_path.parts[0]
            else:
                logging.warning(f"[get_file_info] íŒŒì¼ì´ ìŠ¤í† ì–´ í´ë”ì— ì—†ìŠµë‹ˆë‹¤: {file_path_obj}")
                return None, None, None, None
        except ValueError:
            logging.warning(f"[get_file_info] íŒŒì¼ì´ ë‹¤ìš´ë¡œë“œ ë””ë ‰í† ë¦¬ ì™¸ë¶€ì— ìˆìŠµë‹ˆë‹¤: {file_path_obj}")
            return None, None, None, None

        original_filename = file_path_obj.name
        date_str, file_type, new_filename = None, None, None

        # Context7 2025: Enhanced regex patterns with more specific matching
        # ì£¼ë¬¸ì¡°íšŒ íŒŒì¼ íŒ¨í„´ ë§¤ì¹­ (ë” ì—„ê²©í•œ íŒ¨í„´)
        order_patterns = [
            r"ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´[_\s]*ì£¼ë¬¸ì¡°íšŒ[_\s]*(\d{4}-\d{2}-\d{2})\.xlsx$",  # Standard pattern
            r"ì£¼ë¬¸ì¡°íšŒ[_\s]*(\d{4}-\d{2}-\d{2})\.xlsx$",  # Alternative pattern
        ]

        for pattern in order_patterns:
            order_match = re.search(pattern, original_filename, re.IGNORECASE)
            if order_match:
                date_str = order_match.group(1)
                file_type = 'ì£¼ë¬¸'
                new_filename = f"{store_name} ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´_ì£¼ë¬¸ì¡°íšŒ_{date_str}.xlsx"
                logging.info(f"[get_file_info] ğŸ“‹ ì£¼ë¬¸ì¡°íšŒ íŒŒì¼ ë§¤ì¹­ë¨: {original_filename} -> {new_filename}")
                break

        # ìƒí’ˆì„±ê³¼ íŒŒì¼ íŒ¨í„´ ë§¤ì¹­ (í™•ì¥ëœ íŒ¨í„´ ì§€ì›)
        if not date_str:  # Only check if order pattern wasn't matched
            perf_patterns = [
                r"ìƒí’ˆì„±ê³¼[_\s]*(\d{4}-\d{2}-\d{2}).*?\.xlsx$",  # Standard pattern with optional suffix
                r"ì„±ê³¼[_\s]*(\d{4}-\d{2}-\d{2}).*?\.xlsx$",  # Short pattern
            ]

            for pattern in perf_patterns:
                perf_match = re.search(pattern, original_filename, re.IGNORECASE)
                if perf_match:
                    date_str = perf_match.group(1)
                    file_type = 'ì„±ê³¼'
                    new_filename = f"{store_name} ìƒí’ˆì„±ê³¼_{date_str}.xlsx"
                    logging.info(f"[get_file_info] ğŸ“Š ìƒí’ˆì„±ê³¼ íŒŒì¼ ë§¤ì¹­ë¨: {original_filename} -> {new_filename}")
                    break

        # Context7 2025: Enhanced validation and logging
        if date_str and file_type and new_filename:
            # Validate date format
            try:
                datetime.datetime.strptime(date_str, "%Y-%m-%d")
                logging.info(f"[get_file_info] âœ… íŒŒì¼ ì •ë³´ ì¶”ì¶œ ì„±ê³µ: {original_filename}")
                logging.info(f"[get_file_info]    ğŸ“ ìŠ¤í† ì–´: {store_name}")
                logging.info(f"[get_file_info]    ğŸ“… ë‚ ì§œ: {date_str}")
                logging.info(f"[get_file_info]    ğŸ“‹ íƒ€ì…: {file_type}")
                return store_name, date_str, file_type, new_filename
            except ValueError:
                logging.error(f"[get_file_info] ì˜ëª»ëœ ë‚ ì§œ í˜•ì‹: {date_str}")
                return None, None, None, None
        else:
            logging.warning(f"[get_file_info] âŒ íŒŒì¼ íŒ¨í„´ ë§¤ì¹­ ì‹¤íŒ¨: {original_filename}")
            logging.warning(f"[get_file_info]    ì§€ì› íŒ¨í„´:")
            logging.warning(f"[get_file_info]    - ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´_ì£¼ë¬¸ì¡°íšŒ_YYYY-MM-DD.xlsx")
            logging.warning(f"[get_file_info]    - ìƒí’ˆì„±ê³¼_YYYY-MM-DD.xlsx")
            return None, None, None, None

    except (ValueError, FileNotFoundError, PermissionError) as e:
        logging.warning(f"[get_file_info] íŒŒì¼ ê²€ì¦ ì‹¤íŒ¨: {e}")
        return None, None, None, None
    except Exception as e:
        logging.error(f"[get_file_info] ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
        return None, None, None, None

def _check_and_process_data(store: str, date: str) -> bool:
    """
    ì£¼ë¬¸ì¡°íšŒ íŒŒì¼ì´ ì¤€ë¹„ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ê³  ë¦¬í¬íŠ¸ ìƒì„±ì„ íŠ¸ë¦¬ê±° - Context7 2025 ëª¨ë²” ì‚¬ë¡€

    Enhanced data processing trigger with pathlib integration, comprehensive
    file validation, and robust error handling.

    Args:
        store: Store name
        date: Date string in YYYY-MM-DD format

    Returns:
        bool: True if report generation was successful, False otherwise

    Features:
        - Secure path handling with pathlib
        - Comprehensive file existence checks
        - Detailed progress logging
        - Atomic report generation with success validation
    """
    logging.info(f"ğŸ” [{store}, {date}] ===== ë°ì´í„° ì²˜ë¦¬ ì‹œì‘ =====")

    # Context7 2025: Use pathlib for all path operations
    processing_dir = config.get_processing_dir()
    order_file = f"{store} ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´_ì£¼ë¬¸ì¡°íšŒ_{date}.xlsx"
    order_path = processing_dir / order_file

    logging.info(f"ğŸ“‚ [{store}, {date}] ì°¾ê³  ìˆëŠ” íŒŒì¼: {order_path}")
    logging.info(f"ğŸ“ [{store}, {date}] ì‘ì—…í´ë” ê²½ë¡œ: {processing_dir}")

    # Context7 2025: Enhanced directory validation and file listing
    try:
        if processing_dir.exists():
            # Use pathlib glob for safer file listing
            all_files = list(processing_dir.iterdir())
            xlsx_files = [f.name for f in all_files if f.suffix.lower() == '.xlsx' and f.is_file()]
            logging.info(f"ğŸ“„ [{store}, {date}] ì‘ì—…í´ë” ë‚´ Excel íŒŒì¼ë“¤ ({len(xlsx_files)}ê°œ): {xlsx_files}")
        else:
            logging.warning(f"âŒ [{store}, {date}] ì‘ì—…í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {processing_dir}")
            return False
    except (OSError, PermissionError) as e:
        logging.error(f"âŒ [{store}, {date}] ì‘ì—…í´ë” ì½ê¸° ì˜¤ë¥˜: {e}")
        return False

    # Context7 2025: Use pathlib for existence check
    if order_path.exists():
        logging.info(f"âœ… [{store}, {date}] ì£¼ë¬¸ì¡°íšŒ íŒŒì¼ ë°œê²¬! ë°ì´í„° ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")

        # Check if report already exists - pathlib based
        individual_report = f'{store}_í†µí•©_ë¦¬í¬íŠ¸_{date}.xlsx'
        individual_report_path = processing_dir / individual_report

        logging.info(f"ğŸ¯ [{store}, {date}] ìƒì„±í•  ë¦¬í¬íŠ¸ íŒŒì¼: {individual_report_path}")

        if individual_report_path.exists():
            # Context7 2025: Use pathlib stat for file info
            file_size = individual_report_path.stat().st_size
            logging.info(f"âœ… [{store}, {date}] ì´ë¯¸ ë¦¬í¬íŠ¸ê°€ ìƒì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤. (í¬ê¸°: {file_size:,} bytes)")
            return True  # ì´ë¯¸ ë¦¬í¬íŠ¸ê°€ ìˆìœ¼ë¯€ë¡œ ì„±ê³µìœ¼ë¡œ ê°„ì£¼
        else:
            logging.info(f"ğŸ”„ [{store}, {date}] ë¦¬í¬íŠ¸ ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
            # ê°œë³„ ë¦¬í¬íŠ¸ë§Œ ìƒì„± (íŒŒì¼ ì´ë™ì€ í•˜ì§€ ì•ŠìŒ)
            try:
                # í˜„ì¬ ì„¤ì •ëœ ì—”ì§„ì— ë”°ë¼ ì ì ˆí•œ generator ì‚¬ìš©
                current_engine = get_current_engine()
                logging.info(f"ğŸš€ [{store}, {date}] ë°ì´í„° ì²˜ë¦¬ ì—”ì§„: {current_engine}")

                if current_engine == "Polars":
                    # Polars ì—”ì§„ ì‚¬ìš©
                    try:
                        from . import report_generator_polars
                        processed_groups = report_generator_polars.generate_individual_reports_polars()
                        logging.info(f"âš¡ [{store}, {date}] Polars ì—”ì§„ìœ¼ë¡œ ê³ ì„±ëŠ¥ ì²˜ë¦¬ ì™„ë£Œ")
                    except ImportError as e:
                        logging.warning(f"âš ï¸ [{store}, {date}] Polars ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨, Pandasë¡œ í´ë°±: {e}")
                        processed_groups = report_generator.generate_individual_reports()
                    except Exception as e:
                        logging.warning(f"âš ï¸ [{store}, {date}] Polars ì²˜ë¦¬ ì‹¤íŒ¨, Pandasë¡œ í´ë°±: {e}")
                        processed_groups = report_generator.generate_individual_reports()
                else:
                    # Pandas ì—”ì§„ ì‚¬ìš©
                    processed_groups = report_generator.generate_individual_reports()
                    logging.info(f"ğŸ“Š [{store}, {date}] Pandas ì—”ì§„ìœ¼ë¡œ í‘œì¤€ ì²˜ë¦¬ ì™„ë£Œ")

                if processed_groups:
                    logging.info(f"âœ… [{store}, {date}] ë¦¬í¬íŠ¸ ìƒì„± ì„±ê³µ! ì²˜ë¦¬ëœ ê·¸ë£¹: {processed_groups}")

                    # Context7 2025: Use pathlib for file verification
                    if individual_report_path.exists():
                        file_size = individual_report_path.stat().st_size
                        logging.info(f"âœ… [{store}, {date}] ë¦¬í¬íŠ¸ íŒŒì¼ í™•ì¸ë¨: {individual_report_path.name} (í¬ê¸°: {file_size:,} bytes)")
                        logging.info(f"âœ… [{store}, {date}] ê°œë³„ ë¦¬í¬íŠ¸ ì²˜ë¦¬ ì™„ë£Œ.")
                        return True  # ë¦¬í¬íŠ¸ ìƒì„± ì„±ê³µ
                    else:
                        logging.error(f"âŒ [{store}, {date}] ë¦¬í¬íŠ¸ ìƒì„±í–ˆë‹¤ê³  í•˜ì§€ë§Œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {individual_report_path}")
                        return False  # ë¦¬í¬íŠ¸ íŒŒì¼ì´ ì‹¤ì œë¡œ ìƒì„±ë˜ì§€ ì•ŠìŒ
                else:
                    logging.error(f"âŒ [{store}, {date}] ë¦¬í¬íŠ¸ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. (processed_groupsê°€ ë¹„ì–´ìˆìŒ)")
                    return False  # ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨
            except Exception as e:
                logging.error(f"âŒ [{store}, {date}] ë¦¬í¬íŠ¸ ìƒì„± ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
                import traceback
                logging.error(f"ğŸ“š [{store}, {date}] ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤: {traceback.format_exc()}")
                return False  # ì˜ˆì™¸ ë°œìƒìœ¼ë¡œ ì‹¤íŒ¨
    else:
        logging.warning(f"âŒ [{store}, {date}] ì£¼ë¬¸ì¡°íšŒ íŒŒì¼ì´ ì•„ì§ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {order_path}")
        return False  # ì£¼ë¬¸ì¡°íšŒ íŒŒì¼ì´ ì—†ìœ¼ë¯€ë¡œ ì‹¤íŒ¨

def process_file(src_path: Union[str, Path]) -> None:
    """
    ê°ì§€ëœ íŒŒì¼ì„ ì²˜ë¦¬ í´ë”ë¡œ ì˜®ê¸°ê³ , ë°ì´í„° ì²˜ë¦¬ë¥¼ ì‹œì‘ - Context7 2025 ëª¨ë²” ì‚¬ë¡€

    Enhanced file processing with pathlib integration, improved error handling,
    and comprehensive logging for sales data automation.

    Args:
        src_path: Source file path (string or Path object)

    Security Features:
        - Path validation and sanitization
        - Safe file operations with atomic moves
        - Comprehensive error handling and rollback
    """
    # Context7 2025: Convert to Path object for secure operations
    src_path_obj = Path(src_path).resolve() if isinstance(src_path, str) else src_path.resolve()

    logging.info(f"[process_file] ğŸ“ íŒŒì¼ ì²˜ë¦¬ ì‹œì‘: {src_path_obj.name}")

    try:
        # Enhanced file info extraction with security validation
        store, date, file_type, new_filename = get_file_info(src_path_obj)
        if not all([store, date, file_type, new_filename]):
            logging.warning(f"[process_file] âŒ íŒŒì¼ ì •ë³´ê°€ ì˜¬ë°”ë¥´ì§€ ì•Šì•„ ë¬´ì‹œí•©ë‹ˆë‹¤: {src_path_obj}")
            return

        # Context7 2025: Use pathlib for all path operations
        processing_dir = config.get_processing_dir()
        dest_path = processing_dir / new_filename

        # Security: Ensure processing directory exists
        processing_dir.mkdir(parents=True, exist_ok=True)

        # Security: Prevent overwriting existing files
        if dest_path.exists():
            logging.warning(f"[process_file] ğŸ”„ ëª©ì ì§€ íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤: {dest_path.name}")
            # Create backup name with timestamp
            timestamp = datetime.datetime.now().strftime("%H%M%S")
            backup_name = f"{dest_path.stem}_backup_{timestamp}{dest_path.suffix}"
            dest_path = processing_dir / backup_name
            logging.info(f"[process_file] ğŸ“ ë°±ì—… íŒŒì¼ëª…ìœ¼ë¡œ ë³€ê²½: {backup_name}")

        # Context7 2025: Use pathlib for safe file operations
        logging.info(f"[process_file] ğŸšš íŒŒì¼ ì´ë™: '{src_path_obj.name}' -> '{dest_path.name}'")

        # Atomic file move operation
        src_path_obj.rename(dest_path)

        logging.info(f"[process_file] âœ… íŒŒì¼ ì´ë™ ì™„ë£Œ: {dest_path}")

        # íŒŒì¼ ì´ë™ë§Œ ìˆ˜í–‰ (ë¦¬í¬íŠ¸ ìƒì„±ì€ finalize_all_processingì—ì„œ ì¼ê´„ ì²˜ë¦¬)
        logging.info(f"[process_file] âœ… íŒŒì¼ ì´ë™ ì™„ë£Œ. ë¦¬í¬íŠ¸ ìƒì„±ì€ ë‚˜ì¤‘ì— ì¼ê´„ ì²˜ë¦¬ë©ë‹ˆë‹¤: {dest_path.name}")

    except (FileNotFoundError, PermissionError) as e:
        logging.error(f"[process_file] ğŸš« íŒŒì¼ ì‹œìŠ¤í…œ ì˜¤ë¥˜: {e}")
        # Attempt to restore original file if move failed
        if 'dest_path' in locals() and dest_path.exists() and not src_path_obj.exists():
            try:
                dest_path.rename(src_path_obj)
                logging.info(f"[process_file] ğŸ”„ íŒŒì¼ ë³µì› ì™„ë£Œ: {src_path_obj}")
            except Exception:
                logging.error(f"[process_file] âŒ íŒŒì¼ ë³µì› ì‹¤íŒ¨")
    except Exception as e:
        logging.error(f"[process_file] âŒ íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
        import traceback
        logging.debug(f"[process_file] ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤: {traceback.format_exc()}")

class FileProcessorHandler(FileSystemEventHandler):
    """íŒŒì¼ ì‹œìŠ¤í…œ ì´ë²¤íŠ¸ë¥¼ ê°ì§€í•˜ì—¬ íŒŒì¼ ì²˜ë¦¬ë¥¼ ì‹œì‘í•˜ëŠ” í•¸ë“¤ëŸ¬"""
    def on_created(self, event):
        if not event.is_directory and event.src_path.endswith('.xlsx') and not os.path.basename(event.src_path).startswith('~$'):
            logging.info(f"[on_created] ìƒˆ íŒŒì¼ ê°ì§€: {event.src_path}")
            time.sleep(1)
            process_file(event.src_path)

def process_existing_files() -> None:
    """
    í”„ë¡œê·¸ë¨ ì‹œì‘ ì‹œ ë‹¤ìš´ë¡œë“œ í´ë”ì— ì´ë¯¸ ìˆëŠ” íŒŒì¼ë“¤ì„ ì²˜ë¦¬ - Context7 2025 ëª¨ë²” ì‚¬ë¡€

    Enhanced existing file processing with pathlib integration, security validation,
    and comprehensive error handling for sales data automation.

    Features:
        - Secure directory traversal with pathlib
        - System folder exclusion with pattern matching
        - Atomic file processing with rollback capability
        - Comprehensive progress tracking and logging
    """
    logging.info("===== ê¸°ì¡´ íŒŒì¼ ìŠ¤ìº” ì‹œì‘ ====")

    # Context7 2025: Use pathlib for directory validation
    download_dir = config.validate_directory(config.DOWNLOAD_DIR)
    if not download_dir.exists():
        logging.warning(f"ğŸ” ê°ì‹œí•  ë‹¤ìš´ë¡œë“œ í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {download_dir}")
        return

    processed_files = 0
    total_files = 0

    # Context7 2025: Enhanced system folder exclusion with pathlib
    excluded_folders = set()
    try:
        excluded_folders.add(config.get_processing_dir().name)     # ì‘ì—…í´ë”
        excluded_folders.add(config.get_archive_dir().name)        # ì›ë³¸_ë³´ê´€í•¨
        excluded_folders.add(config.get_report_archive_dir().name) # ë¦¬í¬íŠ¸ë³´ê´€í•¨
    except Exception as e:
        logging.warning(f"ì‹œìŠ¤í…œ í´ë” ê²½ë¡œ í™•ì¸ ì‹¤íŒ¨: {e}")

    logging.info(f"ì œì™¸í•  ì‹œìŠ¤í…œ í´ë”ë“¤: {excluded_folders}")

    # Context7 2025: Use pathlib for safe directory traversal
    try:
        for store_folder_path in download_dir.iterdir():
            if store_folder_path.is_dir() and store_folder_path.name not in excluded_folders:
                logging.info(f"ğŸ“ ìŠ¤í† ì–´ í´ë” ìŠ¤ìº”: {store_folder_path.name}")

                # Context7 2025: Use pathlib glob for file discovery
                store_files = [
                    f for f in store_folder_path.iterdir()
                    if f.is_file() and f.suffix.lower() == '.xlsx' and not f.name.startswith('~')
                ]
                total_files += len(store_files)

                if not store_files:
                    logging.info(f"   â„¹ï¸ {store_folder_path.name}ì— ì²˜ë¦¬í•  Excel íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                    continue

                # Context7 2025: Use pathlib file names for logging
                file_names = [f.name for f in store_files]
                logging.info(f"   ğŸ“„ ë°œê²¬ëœ íŒŒì¼ë“¤ ({len(store_files)}ê°œ): {file_names}")

                for file_path in store_files:
                    logging.info(f"ğŸ”„ [ê¸°ì¡´ íŒŒì¼] ì²˜ë¦¬ ì‹œë„: '{file_path.name}'")
                    try:
                        # ê°œë³„ íŒŒì¼ ì²˜ë¦¬ (ìµœì¢… ì •ë¦¬ëŠ” ë‚˜ì¤‘ì— ì¼ê´„ ìˆ˜í–‰)
                        process_file(file_path)
                        processed_files += 1
                        logging.info(f"âœ… [ê¸°ì¡´ íŒŒì¼] ì²˜ë¦¬ ì™„ë£Œ: '{file_path.name}'")
                    except Exception as e:
                        logging.error(f"âŒ [ê¸°ì¡´ íŒŒì¼] ì²˜ë¦¬ ì‹¤íŒ¨: '{file_path.name}' - {e}")

    except (OSError, PermissionError) as e:
        logging.error(f"âŒ ë‹¤ìš´ë¡œë“œ í´ë” ìŠ¤ìº” ì¤‘ ì˜¤ë¥˜: {e}")
        return

    logging.info(f"ğŸ“Š íŒŒì¼ ì²˜ë¦¬ ê²°ê³¼: {processed_files}/{total_files}ê°œ ì„±ê³µ")

    # 2ë‹¨ê³„: ì‘ì—…í´ë”ì˜ ë¯¸ì™„ë£Œ ì²˜ë¦¬ íŒŒì¼ë“¤ ê²€ì‚¬ ë° ì²˜ë¦¬
    process_incomplete_files()

    # 3ë‹¨ê³„: ëª¨ë“  ê°œë³„ ì²˜ë¦¬ ì™„ë£Œ í›„ ì „ì²´ í†µí•© ë¦¬í¬íŠ¸ ìƒì„± ë° íŒŒì¼ ì •ë¦¬
    if processed_files > 0:
        logging.info("ğŸ”„ ê°œë³„ ì²˜ë¦¬ ì™„ë£Œ - ì „ì²´ í†µí•© ë¦¬í¬íŠ¸ ìƒì„± ì‹œì‘")
        finalize_all_processing()
    
    logging.info("===== ê¸°ì¡´ íŒŒì¼ ìŠ¤ìº” ì™„ë£Œ =====")

def process_incomplete_files():
    """ì‘ì—…í´ë”ì— ìˆëŠ” ë¯¸ì™„ë£Œ ì²˜ë¦¬ íŒŒì¼ë“¤ì„ ê²€ì‚¬í•˜ê³  ë¦¬í¬íŠ¸ ìƒì„±ì„ ì‹œë„í•©ë‹ˆë‹¤."""
    logging.info("--- ì‘ì—…í´ë” ë¯¸ì™„ë£Œ íŒŒì¼ ê²€ì‚¬ ì‹œì‘ ---")
    
    # Context7 2025: Use pathlib for stop flag check
    if STOP_FLAG_FILE.exists():
        logging.info("ì¤‘ì§€ ì‹ í˜¸ ê°ì§€. ì‘ì—…í´ë” ì²˜ë¦¬ë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
        return

    # Context7 2025: Use pathlib for directory check
    processing_dir = config.get_processing_dir()
    if not processing_dir.exists():
        return
    
    # Context7 2025: Use pathlib for file scanning
    all_files = [
        f.name for f in processing_dir.iterdir()
        if f.is_file() and f.suffix.lower() == '.xlsx' and not f.name.startswith('~')
    ]
    source_files = [f for f in all_files if 'í†µí•©_ë¦¬í¬íŠ¸' not in f and 'ë§ˆì§„ì •ë³´' not in f]
    
    if not source_files:
        logging.info("ì‘ì—…í´ë”ì— ë¯¸ì²˜ë¦¬ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ìŠ¤í† ì–´ë³„, ë‚ ì§œë³„ íŒŒì¼ ê·¸ë£¹ ìƒì„±
    file_groups = {}
    for f in source_files:
        store, date, file_type = None, None, None
        if 'ìƒí’ˆì„±ê³¼' in f:
            parts = f.split(' ìƒí’ˆì„±ê³¼_')
            if len(parts) == 2: 
                store, date, file_type = parts[0], parts[1].replace('.xlsx',''), 'ì„±ê³¼'
        elif 'ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´_ì£¼ë¬¸ì¡°íšŒ' in f:
            parts = f.split(' ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´_ì£¼ë¬¸ì¡°íšŒ_')
            if len(parts) == 2: 
                store, date, file_type = parts[0], parts[1].replace('.xlsx',''), 'ì£¼ë¬¸'
        
        if store and date and file_type:
            key = (store, date)
            if key not in file_groups: 
                file_groups[key] = {}
            file_groups[key][file_type] = f
    
    # ì™„ì „í•œ íŒŒì¼ ìŒì´ ìˆëŠ”ë° ë¦¬í¬íŠ¸ê°€ ì—†ëŠ” ê²½ìš° ì²˜ë¦¬
    processed_any = False
    for (store, date), files in file_groups.items():
        # ì¤‘ì§€ ì‹ í˜¸ í™•ì¸
        if os.path.exists(STOP_FLAG_FILE):
            logging.info("ì¤‘ì§€ ì‹ í˜¸ ê°ì§€. ë¯¸ì™„ë£Œ íŒŒì¼ ì²˜ë¦¬ë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
            return
            
        if 'ì„±ê³¼' in files and 'ì£¼ë¬¸' in files:
            individual_report = f'{store}_í†µí•©_ë¦¬í¬íŠ¸_{date}.xlsx'
            individual_report_path = os.path.join(config.get_processing_dir(), individual_report)
            
            if not os.path.exists(individual_report_path):
                logging.info(f"[ë¯¸ì™„ë£Œ ì²˜ë¦¬ ë°œê²¬] {store} ({date}) - ë¦¬í¬íŠ¸ ìƒì„±ì„ ì¬ì‹œë„í•©ë‹ˆë‹¤.")
                _check_and_process_data(store, date)
    
    logging.info("--- ì‘ì—…í´ë” ë¯¸ì™„ë£Œ íŒŒì¼ ê²€ì‚¬ ì™„ë£Œ ---")

def finalize_all_processing():
    """ëª¨ë“  ê°œë³„ ì²˜ë¦¬ ì™„ë£Œ í›„ ì „ì²´ í†µí•© ë¦¬í¬íŠ¸ ìƒì„± ë° íŒŒì¼ ì •ë¦¬ë¥¼ ì¼ê´„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
    logging.info("ğŸ ===== ìµœì¢… ì •ë¦¬ ì‘ì—… ì‹œì‘ =====")
    
    # ì¤‘ì§€ ì‹ í˜¸ í™•ì¸
    if os.path.exists(STOP_FLAG_FILE):
        logging.info("â›” ì¤‘ì§€ ì‹ í˜¸ ê°ì§€. ìµœì¢… ì •ë¦¬ ì‘ì—…ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
        return
    
    processing_dir = config.get_processing_dir()
    logging.info(f"ğŸ“ ì‘ì—…í´ë” ê²½ë¡œ: {processing_dir}")
    
    # ì²˜ë¦¬í•  ê²ƒì´ ìˆëŠ”ì§€ í™•ì¸
    if not os.path.exists(processing_dir):
        logging.warning(f"âŒ ì‘ì—…í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {processing_dir}")
        return
    
    # ì‘ì—…í´ë” ë‚´ ëª¨ë“  íŒŒì¼ í™•ì¸
    try:
        all_files = os.listdir(processing_dir)
        all_xlsx_files = [f for f in all_files if f.endswith('.xlsx') and not f.startswith('~')]
        logging.info(f"ğŸ“„ ì‘ì—…í´ë” ë‚´ ëª¨ë“  Excel íŒŒì¼ë“¤ ({len(all_xlsx_files)}ê°œ): {all_xlsx_files}")
    except Exception as e:
        logging.error(f"âŒ ì‘ì—…í´ë” ì½ê¸° ì˜¤ë¥˜: {e}")
        return
    
    # ì›ë³¸ íŒŒì¼ì´ë‚˜ ê°œë³„ ë¦¬í¬íŠ¸ê°€ ìˆëŠ”ì§€ í™•ì¸
    source_files = [f for f in all_xlsx_files if 'í†µí•©_ë¦¬í¬íŠ¸' not in f and 'ë§ˆì§„ì •ë³´' not in f]
    report_files = [f for f in all_xlsx_files if 'í†µí•©_ë¦¬í¬íŠ¸' in f]

    logging.info(f"ğŸ“‹ ì›ë³¸ íŒŒì¼ë“¤ ({len(source_files)}ê°œ): {source_files}")
    logging.info(f"ğŸ“Š ë¦¬í¬íŠ¸ íŒŒì¼ë“¤ ({len(report_files)}ê°œ): {report_files}")

    if not source_files and not report_files:
        logging.info("â„¹ï¸ ì •ë¦¬í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    # 0ë‹¨ê³„: ì›ë³¸ íŒŒì¼ë“¤ì´ ìˆëŠ”ë° ê°œë³„ ë¦¬í¬íŠ¸ê°€ ì—†ëŠ” ê²½ìš° ê°œë³„ ë¦¬í¬íŠ¸ ìƒì„± (ëˆ„ë½ë˜ì—ˆë˜ í•µì‹¬ ë‹¨ê³„!)
    if source_files and not report_files:
        logging.info(f"ğŸ”„ 0ë‹¨ê³„: ê°œë³„ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘... (ì›ë³¸ íŒŒì¼ {len(source_files)}ê°œ ì²˜ë¦¬)")
        try:
            # í˜„ì¬ ì„¤ì •ëœ ì—”ì§„ì— ë”°ë¼ ì ì ˆí•œ generator ì‚¬ìš© (ì‘ì—…í´ë” ê¸°ëŠ¥ê³¼ ë™ì¼í•œ ë¡œì§)
            current_engine = get_current_engine()
            logging.info(f"ğŸš€ ë°ì´í„° ì²˜ë¦¬ ì—”ì§„: {current_engine}")

            if current_engine == "Polars":
                try:
                    from . import report_generator_polars
                    processed_groups = report_generator_polars.generate_individual_reports_polars()
                    logging.info(f"âš¡ Polars ì—”ì§„ìœ¼ë¡œ ê³ ì„±ëŠ¥ ì²˜ë¦¬ ì™„ë£Œ")
                except ImportError as e:
                    logging.warning(f"âš ï¸ Polars ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨, Pandasë¡œ í´ë°±: {e}")
                    from . import report_generator
                    processed_groups = report_generator.generate_individual_reports()
                except Exception as e:
                    logging.warning(f"âš ï¸ Polars ì²˜ë¦¬ ì‹¤íŒ¨, Pandasë¡œ í´ë°±: {e}")
                    from . import report_generator
                    processed_groups = report_generator.generate_individual_reports()
            else:
                from . import report_generator
                processed_groups = report_generator.generate_individual_reports()
                logging.info(f"ğŸ“Š Pandas ì—”ì§„ìœ¼ë¡œ í‘œì¤€ ì²˜ë¦¬ ì™„ë£Œ")

            if processed_groups:
                logging.info(f"âœ… 0ë‹¨ê³„: ê°œë³„ ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ ({processed_groups}ê°œ ê·¸ë£¹ ì²˜ë¦¬)")

                # ìƒì„±ëœ ê°œë³„ ë¦¬í¬íŠ¸ë“¤ í™•ì¸í•˜ì—¬ report_files ì—…ë°ì´íŠ¸
                updated_files = os.listdir(processing_dir)
                report_files = [f for f in updated_files if f.endswith('.xlsx') and 'í†µí•©_ë¦¬í¬íŠ¸' in f and not f.startswith('~')]
                logging.info(f"ğŸ“Š ìƒˆë¡œ ìƒì„±ëœ ê°œë³„ ë¦¬í¬íŠ¸ë“¤ ({len(report_files)}ê°œ): {report_files}")
            else:
                logging.error("âŒ 0ë‹¨ê³„: ê°œë³„ ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨ - ì²˜ë¦¬ëœ ê·¸ë£¹ì´ ì—†ìŒ")

        except Exception as e:
            logging.error(f"âŒ 0ë‹¨ê³„ ì‹¤íŒ¨: ê°œë³„ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            import traceback
            logging.error(f"ğŸ“š ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤: {traceback.format_exc()}")
    else:
        if source_files and report_files:
            logging.info("â„¹ï¸ 0ë‹¨ê³„ ìŠ¤í‚µ: ê°œë³„ ë¦¬í¬íŠ¸ê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.")
        else:
            logging.info("â„¹ï¸ 0ë‹¨ê³„ ìŠ¤í‚µ: ì²˜ë¦¬í•  ì›ë³¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    # 1ë‹¨ê³„: ì „ì²´ í†µí•© ë¦¬í¬íŠ¸ ìƒì„± (ê°œë³„ ë¦¬í¬íŠ¸ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ)
    if report_files:
        logging.info(f"ğŸ”„ 1ë‹¨ê³„: ì „ì²´ í†µí•© ë¦¬í¬íŠ¸ ìƒì„± ì¤‘... (ê°œë³„ ë¦¬í¬íŠ¸ {len(report_files)}ê°œ í†µí•©)")
        try:
            from . import report_generator
            report_generator.consolidate_daily_reports()
            logging.info("âœ… 1ë‹¨ê³„: ì „ì²´ í†µí•© ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ")
            
            # ìƒì„±ëœ ì „ì²´ í†µí•© ë¦¬í¬íŠ¸ í™•ì¸
            updated_files = os.listdir(processing_dir)
            full_reports = [f for f in updated_files if f.startswith('ì „ì²´_í†µí•©_ë¦¬í¬íŠ¸_') and f.endswith('.xlsx')]
            logging.info(f"ğŸ“Š ìƒì„±ëœ ì „ì²´ í†µí•© ë¦¬í¬íŠ¸ë“¤ ({len(full_reports)}ê°œ): {full_reports}")
        except Exception as e:
            logging.error(f"âŒ 1ë‹¨ê³„ ì‹¤íŒ¨: ì „ì²´ í†µí•© ë¦¬í¬íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            import traceback
            logging.error(f"ğŸ“š ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤: {traceback.format_exc()}")
    else:
        logging.info("â„¹ï¸ 1ë‹¨ê³„ ìŠ¤í‚µ: ê°œë³„ ë¦¬í¬íŠ¸ê°€ ì—†ì–´ì„œ ì „ì²´ í†µí•© ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    
    # 2ë‹¨ê³„: ëª¨ë“  ì›ë³¸ íŒŒì¼ë“¤ì„ ì›ë³¸_ë³´ê´€í•¨ìœ¼ë¡œ ì´ë™
    if source_files:
        logging.info(f"ğŸ”„ 2ë‹¨ê³„: ì›ë³¸ íŒŒì¼ë“¤ì„ ì›ë³¸_ë³´ê´€í•¨ìœ¼ë¡œ ì´ë™ ì¤‘... ({len(source_files)}ê°œ)")
        try:
            move_source_files_to_archive()
            logging.info("âœ… 2ë‹¨ê³„: ì›ë³¸ íŒŒì¼ ì´ë™ ì™„ë£Œ")
        except Exception as e:
            logging.error(f"âŒ 2ë‹¨ê³„ ì‹¤íŒ¨: ì›ë³¸ íŒŒì¼ ì´ë™ ì¤‘ ì˜¤ë¥˜: {e}")
    else:
        logging.info("â„¹ï¸ 2ë‹¨ê³„ ìŠ¤í‚µ: ì´ë™í•  ì›ë³¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    # 3ë‹¨ê³„: ëª¨ë“  ë¦¬í¬íŠ¸ íŒŒì¼ë“¤ì„ ë¦¬í¬íŠ¸ë³´ê´€í•¨ìœ¼ë¡œ ì´ë™
    logging.info("ğŸ”„ 3ë‹¨ê³„: ë¦¬í¬íŠ¸ íŒŒì¼ë“¤ì„ ë¦¬í¬íŠ¸ë³´ê´€í•¨ìœ¼ë¡œ ì´ë™ ì¤‘...")
    try:
        # ì´ë™ ì „ ì‘ì—…í´ë” ë‚´ ë¦¬í¬íŠ¸ íŒŒì¼ë“¤ ë‹¤ì‹œ í™•ì¸
        current_files = os.listdir(processing_dir)
        current_reports = [f for f in current_files if f.endswith('.xlsx') and 'í†µí•©_ë¦¬í¬íŠ¸' in f and not f.startswith('~')]
        logging.info(f"ğŸ“Š ì´ë™í•  ë¦¬í¬íŠ¸ íŒŒì¼ë“¤ ({len(current_reports)}ê°œ): {current_reports}")
        
        move_reports_to_archive()
        logging.info("âœ… 3ë‹¨ê³„: ë¦¬í¬íŠ¸ íŒŒì¼ ì´ë™ ì™„ë£Œ")
        
        # ë¦¬í¬íŠ¸ë³´ê´€í•¨ í™•ì¸
        report_archive_dir = config.get_report_archive_dir()
        if os.path.exists(report_archive_dir):
            archived_reports = [f for f in os.listdir(report_archive_dir) if f.endswith('.xlsx')]
            logging.info(f"ğŸ“ ë¦¬í¬íŠ¸ë³´ê´€í•¨ ë‚´ íŒŒì¼ë“¤ ({len(archived_reports)}ê°œ): {archived_reports}")
        else:
            logging.warning(f"âŒ ë¦¬í¬íŠ¸ë³´ê´€í•¨ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {report_archive_dir}")
            
    except Exception as e:
        logging.error(f"âŒ 3ë‹¨ê³„ ì‹¤íŒ¨: ë¦¬í¬íŠ¸ íŒŒì¼ ì´ë™ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        logging.error(f"ğŸ“š ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤: {traceback.format_exc()}")
    
    logging.info("ğŸ ===== ìµœì¢… ì •ë¦¬ ì‘ì—… ì™„ë£Œ =====")

def move_source_files_to_archive():
    """ì‘ì—…í´ë”ì˜ ëª¨ë“  ì›ë³¸ íŒŒì¼ë“¤(ìƒí’ˆì„±ê³¼, ì£¼ë¬¸ì¡°íšŒ)ì„ ì›ë³¸_ë³´ê´€í•¨ìœ¼ë¡œ ì´ë™í•©ë‹ˆë‹¤."""
    processing_dir = config.get_processing_dir()
    archive_dir = config.get_archive_dir()
    
    if not os.path.exists(processing_dir):
        return
    
    # ì›ë³¸ íŒŒì¼ë“¤ ì°¾ê¸° (í†µí•©_ë¦¬í¬íŠ¸ê°€ ì•„ë‹Œ íŒŒì¼ë“¤)
    source_files = [f for f in os.listdir(processing_dir) 
                   if f.endswith('.xlsx') and 'í†µí•©_ë¦¬í¬íŠ¸' not in f and not f.startswith('~')]
    
    if not source_files:
        logging.info("ì´ë™í•  ì›ë³¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    logging.info(f"--- ì›ë³¸ íŒŒì¼ë“¤ì„ ì›ë³¸_ë³´ê´€í•¨ìœ¼ë¡œ ì´ë™ ì‹œì‘ ({len(source_files)}ê°œ íŒŒì¼) ---")
    
    for source_file in source_files:
        try:
            src_path = os.path.join(processing_dir, source_file)
            dst_path = os.path.join(archive_dir, source_file)
            shutil.move(src_path, dst_path)
            logging.info(f"ì›ë³¸ íŒŒì¼ ì´ë™ ì™„ë£Œ: {source_file}")
        except Exception as e:
            logging.error(f"ì›ë³¸ íŒŒì¼ ì´ë™ ì‹¤íŒ¨ ({source_file}): {e}")
    
    logging.info("--- ì›ë³¸ íŒŒì¼ ì´ë™ ì™„ë£Œ ---")

def move_reports_to_archive():
    """ì‘ì—…í´ë”ì˜ ë¦¬í¬íŠ¸ íŒŒì¼ë“¤ì„ ë¦¬í¬íŠ¸ë³´ê´€í•¨ìœ¼ë¡œ ì´ë™í•©ë‹ˆë‹¤."""
    logging.info("ğŸ“¦ ===== ë¦¬í¬íŠ¸ íŒŒì¼ ì´ë™ ì‹œì‘ =====")
    
    processing_dir = config.get_processing_dir()
    report_archive_dir = config.get_report_archive_dir()
    
    logging.info(f"ğŸ“ ì‘ì—…í´ë”: {processing_dir}")
    logging.info(f"ğŸ“ ë¦¬í¬íŠ¸ë³´ê´€í•¨: {report_archive_dir}")
    
    if not os.path.exists(processing_dir):
        logging.warning(f"âŒ ì‘ì—…í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {processing_dir}")
        return
    
    # ë¦¬í¬íŠ¸ë³´ê´€í•¨ í´ë” ìƒì„± í™•ì¸
    if not os.path.exists(report_archive_dir):
        try:
            os.makedirs(report_archive_dir, exist_ok=True)
            logging.info(f"âœ… ë¦¬í¬íŠ¸ë³´ê´€í•¨ í´ë” ìƒì„±: {report_archive_dir}")
        except Exception as e:
            logging.error(f"âŒ ë¦¬í¬íŠ¸ë³´ê´€í•¨ í´ë” ìƒì„± ì‹¤íŒ¨: {e}")
            return
    
    # ë¦¬í¬íŠ¸ íŒŒì¼ë“¤ ì°¾ê¸° (í†µí•©_ë¦¬í¬íŠ¸ë¡œ ì‹œì‘í•˜ëŠ” íŒŒì¼ë“¤)
    try:
        all_files = os.listdir(processing_dir)
        report_files = [f for f in all_files if f.endswith('.xlsx') and 'í†µí•©_ë¦¬í¬íŠ¸' in f and not f.startswith('~')]
        logging.info(f"ğŸ“Š ì°¾ì€ ë¦¬í¬íŠ¸ íŒŒì¼ë“¤ ({len(report_files)}ê°œ): {report_files}")
        
        # ì¶”ê°€ë¡œ ì¼ì¼_í†µí•©_ë¦¬í¬íŠ¸ì™€ ì£¼ê°„_í†µí•©_ë¦¬í¬íŠ¸ë„ í¬í•¨
        additional_reports = [f for f in all_files if f.endswith('.xlsx') and ('ì¼ì¼_í†µí•©_ë¦¬í¬íŠ¸' in f or 'ì£¼ê°„_í†µí•©_ë¦¬í¬íŠ¸' in f) and not f.startswith('~')]
        if additional_reports:
            logging.info(f"ğŸ“ˆ ì¶”ê°€ í†µí•© ë¦¬í¬íŠ¸ë“¤ ({len(additional_reports)}ê°œ): {additional_reports}")
            report_files.extend(additional_reports)
            # ì¤‘ë³µ ì œê±°
            report_files = list(set(report_files))
            
    except Exception as e:
        logging.error(f"âŒ ì‘ì—…í´ë” ì½ê¸° ì˜¤ë¥˜: {e}")
        return
    
    if not report_files:
        logging.info("â„¹ï¸ ì´ë™í•  ë¦¬í¬íŠ¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        # ì‘ì—…í´ë” ì „ì²´ íŒŒì¼ ëª©ë¡ ë¡œê·¸ë¡œ í™•ì¸
        try:
            all_files = os.listdir(processing_dir)
            xlsx_files = [f for f in all_files if f.endswith('.xlsx')]
            logging.info(f"ğŸ” ì‘ì—…í´ë” ë‚´ Excel íŒŒì¼ë“¤: {xlsx_files}")
        except:
            pass
        return
    
    logging.info(f"ğŸ”„ ë¦¬í¬íŠ¸ íŒŒì¼ë“¤ì„ ë¦¬í¬íŠ¸ë³´ê´€í•¨ìœ¼ë¡œ ì´ë™ ì‹œì‘ (ì´ {len(report_files)}ê°œ íŒŒì¼)")
    
    moved_count = 0
    failed_count = 0
    
    for report_file in report_files:
        try:
            src_path = os.path.join(processing_dir, report_file)
            
            # ë¦¬í¬íŠ¸ íƒ€ì… ê°ì§€ ë° ë¶„ë¥˜ëœ ê²½ë¡œ ìƒì„±
            report_type = config.detect_report_type(report_file)
            if report_type == 'unknown':
                # ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬
                dst_path = os.path.join(report_archive_dir, report_file)
                logging.info(f"ğŸ”„ ì•Œ ìˆ˜ ì—†ëŠ” ë¦¬í¬íŠ¸ íƒ€ì…, ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©: {report_file}")
            else:
                # ë‚ ì§œ ì¶”ì¶œ
                import re
                date_match = re.search(r'(\d{4}-\d{2}-\d{2})', report_file)
                if date_match:
                    from datetime import datetime
                    date_str = date_match.group(1)
                    date_obj = datetime.strptime(date_str, '%Y-%m-%d').date()
                    dst_path = config.get_categorized_report_path(report_type, date_obj, report_file)
                    logging.info(f"ğŸ“ ë¶„ë¥˜ëœ ê²½ë¡œë¡œ ì´ë™: {report_type} â†’ {dst_path}")
                else:
                    # ë‚ ì§œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´ ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©
                    dst_path = os.path.join(report_archive_dir, report_file)
                    logging.warning(f"âš ï¸ ë‚ ì§œë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©: {report_file}")
            
            # ì›ë³¸ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            if not os.path.exists(src_path):
                logging.error(f"âŒ ì›ë³¸ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {report_file}")
                failed_count += 1
                continue
                
            logging.info(f"ğŸ”„ ì´ë™ ì‹œì‘: {report_file}")
            
            # ì´ë¯¸ ê°™ì€ ì´ë¦„ì˜ íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ” ê²½ìš°ì—ë§Œ ë°±ì—… (ê³¼ë„í•œ ë°±ì—… ë°©ì§€)
            if os.path.exists(dst_path):
                # íŒŒì¼ í¬ê¸°ë‚˜ ìˆ˜ì • ì‹œê°„ì´ ë‹¤ë¥¸ ê²½ìš°ì—ë§Œ ë°±ì—…
                src_stat = os.path.getsize(src_path)
                dst_stat = os.path.getsize(dst_path)
                
                if src_stat != dst_stat:  # í¬ê¸°ê°€ ë‹¤ë¥´ë©´ ìƒˆë¡œìš´ ë°ì´í„°
                    timestamp = datetime.datetime.now().strftime("_%Y%m%d_%H%M%S")
                    name, ext = os.path.splitext(report_file)
                    backup_name = f"{name}_backup{timestamp}{ext}"
                    backup_path = os.path.join(report_archive_dir, backup_name)
                    shutil.move(dst_path, backup_path)
                    logging.info(f"ğŸ“‹ ê¸°ì¡´ ë¦¬í¬íŠ¸ ë°±ì—…: {backup_name}")
                else:
                    # ê°™ì€ í¬ê¸°ë©´ ë®ì–´ì“°ê¸° (ë°±ì—…í•˜ì§€ ì•ŠìŒ)
                    os.remove(dst_path)
                    logging.info(f"ğŸ”„ ë™ì¼í•œ ë¦¬í¬íŠ¸ ë®ì–´ì“°ê¸°: {report_file}")
            
            # íŒŒì¼ ì´ë™ ì‹¤í–‰
            shutil.move(src_path, dst_path)
            
            # ì´ë™ ê²€ì¦
            if os.path.exists(dst_path) and not os.path.exists(src_path):
                logging.info(f"âœ… ë¦¬í¬íŠ¸ ì´ë™ ì™„ë£Œ: {report_file}")
                moved_count += 1
            else:
                logging.error(f"âŒ ì´ë™ ê²€ì¦ ì‹¤íŒ¨: {report_file}")
                failed_count += 1
                
        except Exception as e:
            logging.error(f"âŒ ë¦¬í¬íŠ¸ ì´ë™ ì‹¤íŒ¨ ({report_file}): {e}")
            import traceback
            logging.error(f"ğŸ” ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            failed_count += 1
    
    # ì´ë™ ê²°ê³¼ ìš”ì•½
    logging.info(f"ğŸ“Š ë¦¬í¬íŠ¸ ì´ë™ ê²°ê³¼:")
    logging.info(f"   âœ… ì„±ê³µ: {moved_count}ê°œ")
    logging.info(f"   âŒ ì‹¤íŒ¨: {failed_count}ê°œ")
    
    # ë¦¬í¬íŠ¸ë³´ê´€í•¨ ìµœì¢… ìƒíƒœ í™•ì¸
    try:
        archived_files = [f for f in os.listdir(report_archive_dir) if f.endswith('.xlsx')]
        logging.info(f"ğŸ“ ë¦¬í¬íŠ¸ë³´ê´€í•¨ ìµœì¢… ìƒíƒœ: {len(archived_files)}ê°œ íŒŒì¼")
        if archived_files:
            logging.info(f"ğŸ“‹ ë³´ê´€ëœ íŒŒì¼ë“¤: {archived_files}")
    except Exception as e:
        logging.error(f"âŒ ë¦¬í¬íŠ¸ë³´ê´€í•¨ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}")
    
    logging.info("ğŸ¯ ===== ë¦¬í¬íŠ¸ íŒŒì¼ ì´ë™ ì™„ë£Œ =====")
    
    if moved_count == 0 and len(report_files) > 0:
        logging.warning("âš ï¸ ëª¨ë“  ë¦¬í¬íŠ¸ íŒŒì¼ ì´ë™ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ê¶Œí•œì´ë‚˜ ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")

def initialize_folders() -> None:
    """
    í•„ìš”í•œ ëª¨ë“  í´ë”ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ê³  ì—†ìœ¼ë©´ ìƒì„± - Context7 2025 ëª¨ë²” ì‚¬ë¡€

    Enhanced folder initialization with pathlib integration and secure permissions.
    """
    # Context7 2025: Use pathlib for safe directory creation
    config.get_processing_dir().mkdir(parents=True, exist_ok=True)
    config.get_archive_dir().mkdir(parents=True, exist_ok=True)
    config.get_report_archive_dir().mkdir(parents=True, exist_ok=True)

def start_monitoring() -> None:
    """
    íŒŒì¼ ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ì„ ì‹œì‘í•˜ê³ , stop.flag íŒŒì¼ì´ ìƒê¸°ë©´ ì¤‘ì§€ - Context7 2025 ëª¨ë²” ì‚¬ë¡€

    Enhanced monitoring system with pathlib integration, secure file operations,
    and comprehensive error handling.
    """
    initialize_folders()

    # Context7 2025: Use pathlib for stop flag operations
    if STOP_FLAG_FILE.exists():
        STOP_FLAG_FILE.unlink()

    process_existing_files()

    logging.info("\n===== ìŠ¤ë§ˆíŠ¸ í´ë” ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹œì‘ =====")
    logging.info(f"- ê°ì‹œ ëŒ€ìƒ: {config.DOWNLOAD_DIR} (í•˜ìœ„ í´ë” í¬í•¨)")
    logging.info("- íŒŒì¼ì„ ê° ìŠ¤í† ì–´ í´ë”ì— ë„£ìœ¼ë©´ ì²˜ë¦¬ê°€ ì‹œì‘ë©ë‹ˆë‹¤.")

    event_handler = FileProcessorHandler()
    observer = Observer()
    # Note: Observer still needs string path for compatibility
    observer.schedule(event_handler, str(config.DOWNLOAD_DIR), recursive=True)
    observer.start()

    try:
        while True:
            if STOP_FLAG_FILE.exists():
                logging.info("'stop.flag' íŒŒì¼ ê°ì§€. ëª¨ë‹ˆí„°ë§ì„ ì¤‘ì§€í•©ë‹ˆë‹¤.")
                break
            time.sleep(1)
    finally:
        observer.stop()
        observer.join() # ìŠ¤ë ˆë“œê°€ ì™„ì „íˆ ì¢…ë£Œë  ë•Œê¹Œì§€ ëŒ€ê¸°
        if os.path.exists(STOP_FLAG_FILE):
            os.remove(STOP_FLAG_FILE)
        logging.info("\n===== ëª¨ë‹ˆí„°ë§ì´ ì •ìƒì ìœ¼ë¡œ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. =====")
