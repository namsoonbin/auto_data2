# -*- coding: utf-8 -*-
import pandas as pd
import numpy as np
import os
import glob
import re
import logging
import io
import json
from datetime import datetime, timedelta
from . import config

def normalize_product_id(value):
    """ìƒí’ˆIDë¥¼ ì •ê·œí™” - ë¬¸ìì—´ê³¼ ìˆ«ì íƒ€ì… ëª¨ë‘ ì²˜ë¦¬ (ë¬¸ìì—´ ë‚´ .0 í¬í•¨)"""
    if pd.isna(value):
        return ''
    
    # ë¨¼ì € ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ê³  ê³µë°± ì œê±°
    value_str = str(value).strip()

    try:
        # ë¬¸ìì—´ì„ floatìœ¼ë¡œ ë³€í™˜ ì‹œë„ (ì˜ˆ: "12345.0" ì²˜ë¦¬)
        float_val = float(value_str)
        # floatì´ ì •ìˆ˜ì´ë©´ .0ì„ ì œê±°í•˜ê³  ë¬¸ìì—´ë¡œ ë³€í™˜
        if float_val.is_integer():
            return str(int(float_val))
        # ì†Œìˆ˜ì ì´ ìˆëŠ” floatì´ë©´ ê·¸ëŒ€ë¡œ ë¬¸ìì—´ë¡œ ë³€í™˜
        else:
            return str(float_val)
    except (ValueError, TypeError):
        # float ë³€í™˜ì— ì‹¤íŒ¨í•˜ë©´ (ìˆœìˆ˜ ë¬¸ìì—´ ID), ì›ë³¸ ë¬¸ìì—´ ë°˜í™˜
        return value_str

def read_protected_excel(file_path, password=None, **kwargs):
    """
    ì•”í˜¸ë¡œ ë³´í˜¸ëœ Excel íŒŒì¼ì„ ì½ëŠ” í•¨ìˆ˜
    msoffcrypto-toolì´ ì„¤ì¹˜ë˜ì–´ ìˆìœ¼ë©´ ì‚¬ìš©í•˜ê³ , ì—†ìœ¼ë©´ ê¸°ë³¸ pandas ì‚¬ìš©
    """
    try:
        # ë¨¼ì € ì•”í˜¸ ì—†ì´ ì‹œë„
        return pd.read_excel(file_path, engine='openpyxl', **kwargs)
    except Exception as e:
        if password is None:
            logging.error(f"ì•”í˜¸ ë³´í˜¸ëœ íŒŒì¼ì´ì§€ë§Œ ì•”í˜¸ê°€ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {file_path}")
            raise e
        
        # msoffcrypto-tool ì‚¬ìš© ì‹œë„
        try:
            import msoffcrypto
            
            with open(file_path, 'rb') as file:
                office_file = msoffcrypto.OfficeFile(file)
                office_file.load_key(password=password)
                
                # ë©”ëª¨ë¦¬ì—ì„œ í•´ë…ëœ íŒŒì¼ ì²˜ë¦¬ (ìµœì‹  ë²„ì „ í˜¸í™˜)
                decrypted = io.BytesIO()
                try:
                    # ìµœì‹  ë²„ì „: decrypt ë©”ì„œë“œ ì‚¬ìš©
                    office_file.decrypt(decrypted)
                except AttributeError:
                    # ì´ì „ ë²„ì „: save ë©”ì„œë“œ ì‚¬ìš©
                    office_file.save(decrypted)
                
                decrypted.seek(0)
                return pd.read_excel(decrypted, engine='openpyxl', **kwargs)
                
        except ImportError:
            logging.error("msoffcrypto-toolì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            logging.error("í•´ê²° ë°©ë²•: pip install msoffcrypto-tool")
            logging.error("ë˜ëŠ” Excelì—ì„œ íŒŒì¼ì„ ì—´ì–´ ì•”í˜¸ë¥¼ ì œê±°í•œ í›„ ì €ì¥í•˜ì„¸ìš”.")
            raise ImportError("msoffcrypto-tool ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤. 'pip install msoffcrypto-tool'ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”.")
        except Exception as decrypt_error:
            logging.error(f"ì•”í˜¸ í•´ë… ì‹¤íŒ¨: {decrypt_error}")
            logging.error("ì•”í˜¸ê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸í•˜ê±°ë‚˜ Excelì—ì„œ ìˆ˜ë™ìœ¼ë¡œ ì•”í˜¸ë¥¼ ì œê±°í•´ë³´ì„¸ìš”.")
            raise decrypt_error

# ì „ì—­ ë¦¬ì›Œë“œ ìºì‹œ
_reward_cache = None
_reward_cache_timestamp = None

def _load_reward_cache():
    """ë¦¬ì›Œë“œ ì„¤ì •ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë¡œë“œí•˜ì—¬ ìºì‹œ"""
    global _reward_cache, _reward_cache_timestamp
    
    try:
        reward_file = os.path.join(config.BASE_DIR, 'ë¦¬ì›Œë“œì„¤ì •.json')
        
        # íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not os.path.exists(reward_file):
            _reward_cache = {}
            return
        
        # íŒŒì¼ ìˆ˜ì • ì‹œê°„ í™•ì¸ (ìºì‹œ ë¬´íš¨í™”ìš©)
        file_timestamp = os.path.getmtime(reward_file)
        if _reward_cache is not None and _reward_cache_timestamp == file_timestamp:
            return  # ìºì‹œ ìœ íš¨í•¨
        
        # íŒŒì¼ í¬ê¸° í™•ì¸
        if os.path.getsize(reward_file) == 0:
            _reward_cache = {}
            return
        
        # JSON íŒŒì¼ ì½ê¸°
        with open(reward_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # ë°ì´í„° êµ¬ì¡° ê²€ì¦
        if not isinstance(data, dict) or 'rewards' not in data:
            _reward_cache = {}
            return
        
        rewards_list = data.get('rewards', [])
        if not isinstance(rewards_list, list):
            _reward_cache = {}
            return
        
        # íš¨ìœ¨ì ì¸ ì¡°íšŒë¥¼ ìœ„í•œ ë”•ì…”ë„ˆë¦¬ ìƒì„±
        reward_map = {}
        for reward_entry in rewards_list:
            try:
                # í•„ìˆ˜ í‚¤ ì¡´ì¬ í™•ì¸
                if not all(k in reward_entry for k in ['start_date', 'end_date', 'product_id', 'reward']):
                    continue
                
                start_date = datetime.strptime(reward_entry['start_date'], '%Y-%m-%d').date()
                end_date = datetime.strptime(reward_entry['end_date'], '%Y-%m-%d').date()
                product_id = normalize_product_id(reward_entry['product_id'])
                reward_value = reward_entry['reward']
                
                # ë¦¬ì›Œë“œ ê°’ì´ ìˆ«ìì¸ì§€ í™•ì¸
                if not isinstance(reward_value, (int, float)) or reward_value < 0:
                    continue
                
                # ë‚ ì§œ ë²”ìœ„ì˜ ê° ë‚ ì§œë³„ë¡œ ë”•ì…”ë„ˆë¦¬ì— ì €ì¥
                current_date = start_date
                while current_date <= end_date:
                    key = (current_date.strftime('%Y-%m-%d'), product_id)
                    reward_map[key] = int(reward_value)
                    current_date += timedelta(days=1)
                    
            except (ValueError, KeyError, TypeError):
                continue
        
        _reward_cache = reward_map
        _reward_cache_timestamp = file_timestamp
        logging.info(f"ë¦¬ì›Œë“œ ìºì‹œ ë¡œë“œ ì™„ë£Œ: {len(reward_map)}ê°œ ì—”íŠ¸ë¦¬")
        
    except Exception as e:
        logging.warning(f"ë¦¬ì›Œë“œ ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
        _reward_cache = {}

def get_reward_for_date_and_product(product_id, date_str):
    """ë‚ ì§œì™€ ìƒí’ˆIDì— í•´ë‹¹í•˜ëŠ” ë¦¬ì›Œë“œ ê°’ ì¡°íšŒ (ìºì‹œ ê¸°ë°˜ ê³ ì† ì¡°íšŒ)"""
    try:
        # ìºì‹œ ë¡œë“œ (í•„ìš”ì‹œ)
        _load_reward_cache()
        
        # ë‚ ì§œ ì •ê·œí™”
        target_date = None
        for date_format in ['%Y-%m-%d', '%Y-%m-%d %H:%M:%S', '%Y/%m/%d']:
            try:
                target_date = datetime.strptime(date_str, date_format).date().strftime('%Y-%m-%d')
                break
            except ValueError:
                continue
        
        if target_date is None:
            return 0
        
        # ìƒí’ˆID ì •ê·œí™”
        normalized_product_id = normalize_product_id(product_id)
        
        # O(1) ë”•ì…”ë„ˆë¦¬ ì¡°íšŒ
        key = (target_date, normalized_product_id)
        return _reward_cache.get(key, 0)
        
    except Exception as e:
        logging.warning(f"ë¦¬ì›Œë“œ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        return 0

def get_purchase_count_for_date_and_product(product_id, date_str):
    """ë‚ ì§œì™€ ìƒí’ˆIDì— í•´ë‹¹í•˜ëŠ” ê°€êµ¬ë§¤ ê°œìˆ˜ ì¡°íšŒ (ë¦¬ì›Œë“œ ë°©ì‹ê³¼ ë™ì¼)"""
    try:
        purchase_file = os.path.join(config.BASE_DIR, 'ê°€êµ¬ë§¤ì„¤ì •.json')
        
        # íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not os.path.exists(purchase_file):
            return 0
        
        # íŒŒì¼ í¬ê¸° í™•ì¸ (ë¹ˆ íŒŒì¼ ì²´í¬)
        if os.path.getsize(purchase_file) == 0:
            return 0
        
        # JSON íŒŒì¼ ì½ê¸°
        with open(purchase_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # ë°ì´í„° êµ¬ì¡° ê²€ì¦
        if not isinstance(data, dict) or 'purchases' not in data:
            return 0
        
        purchases_list = data.get('purchases', [])
        if not isinstance(purchases_list, list):
            return 0
        
        # ë‚ ì§œ íŒŒì‹± (ì—¬ëŸ¬ í˜•ì‹ ì§€ì›)
        target_date = None
        for date_format in ['%Y-%m-%d', '%Y-%m-%d %H:%M:%S', '%Y/%m/%d']:
            try:
                target_date = datetime.strptime(date_str, date_format).date()
                break
            except ValueError:
                continue
        
        if target_date is None:
            logging.warning(f"ê°€êµ¬ë§¤ ê°œìˆ˜ ì¡°íšŒ: ë‚ ì§œ í˜•ì‹ì„ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {date_str}")
            return 0
        
        # í•´ë‹¹ ìƒí’ˆê³¼ ë‚ ì§œì— ë§ëŠ” ê°€êµ¬ë§¤ ê°œìˆ˜ ì°¾ê¸°
        for purchase_entry in purchases_list:
            try:
                # í•„ìˆ˜ í‚¤ ì¡´ì¬ í™•ì¸
                if not all(k in purchase_entry for k in ['start_date', 'end_date', 'product_id', 'purchase_count']):
                    continue
                
                start_date = datetime.strptime(purchase_entry['start_date'], '%Y-%m-%d').date()
                end_date = datetime.strptime(purchase_entry['end_date'], '%Y-%m-%d').date()
                
                # ìƒí’ˆID ì •ê·œí™”í•˜ì—¬ ë¹„êµ
                normalized_entry_id = normalize_product_id(purchase_entry['product_id'])
                normalized_target_id = normalize_product_id(product_id)
                
                if (start_date <= target_date <= end_date and 
                    normalized_entry_id == normalized_target_id):
                    purchase_count = purchase_entry['purchase_count']
                    # ê°€êµ¬ë§¤ ê°œìˆ˜ê°€ ìˆ«ìì¸ì§€ í™•ì¸
                    if isinstance(purchase_count, (int, float)) and purchase_count >= 0:
                        return int(purchase_count)
            except (ValueError, KeyError, TypeError) as e:
                # ê°œë³„ ì—”íŠ¸ë¦¬ íŒŒì‹± ì‹¤íŒ¨ëŠ” ë¡œê·¸ë§Œ ë‚¨ê¸°ê³  ê³„ì† ì§„í–‰
                continue
        
        return 0  # ì„¤ì •ì´ ì—†ìœ¼ë©´ 0
        
    except FileNotFoundError:
        return 0
    except json.JSONDecodeError as e:
        logging.warning(f"ê°€êµ¬ë§¤ ì„¤ì • JSON íŒŒì¼ í˜•ì‹ ì˜¤ë¥˜: {e}")
        return 0
    except Exception as e:
        logging.warning(f"ê°€êµ¬ë§¤ ê°œìˆ˜ ì¡°íšŒ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
        return 0

def generate_individual_reports():
    """ê°œë³„ ìŠ¤í† ì–´ì˜ ì£¼ë¬¸ì¡°íšŒ íŒŒì¼ì„ ê¸°ë°˜ìœ¼ë¡œ ì˜µì…˜ë³„ í†µí•© ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    logging.info("ğŸ¯ ===== GENERATE_INDIVIDUAL_REPORTS í•¨ìˆ˜ í˜¸ì¶œë¨ =====")
    logging.info("--- 1ë‹¨ê³„: ì£¼ë¬¸ì¡°íšŒ ê¸°ë°˜ ê°œë³„ í†µí•© ë¦¬í¬íŠ¸ ìƒì„± ì‹œì‘ ---")
    
    # ë§ˆì§„ì •ë³´ íŒŒì¼ ë¡œë“œ ë° ê²€ì¦
    try:
        margin_df = pd.read_excel(config.MARGIN_FILE, engine='openpyxl')
        logging.info(f"'{os.path.basename(config.MARGIN_FILE)}' íŒŒì¼ì„ ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
        
        # í•„ìˆ˜ ì»¬ëŸ¼ ì¡´ì¬ í™•ì¸
        required_columns = ['ìƒí’ˆë²ˆí˜¸', 'ìƒí’ˆëª…', 'íŒë§¤ê°€', 'ë§ˆì§„ìœ¨']
        missing_columns = [col for col in required_columns if col not in margin_df.columns]
        if missing_columns:
            raise ValueError(f"ë§ˆì§„ì •ë³´ íŒŒì¼ì— í•„ìˆ˜ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {missing_columns}")
        
        # ì»¬ëŸ¼ëª… ì •ê·œí™”
        margin_df = margin_df.rename(columns={'ìƒí’ˆë²ˆí˜¸': 'ìƒí’ˆID'})
        
        # ìƒí’ˆID ë°ì´í„° íƒ€ì… ì •ê·œí™” (ë¬¸ìì—´/ìˆ«ì ëª¨ë‘ ì²˜ë¦¬)
        margin_df['ìƒí’ˆID'] = margin_df['ìƒí’ˆID'].apply(normalize_product_id)
        if margin_df['ìƒí’ˆID'].isna().any():
            logging.warning("ë§ˆì§„ì •ë³´ì— ë¹ˆ ìƒí’ˆIDê°€ ìˆìŠµë‹ˆë‹¤. í•´ë‹¹ í–‰ë“¤ì„ ì œê±°í•©ë‹ˆë‹¤.")
            margin_df = margin_df.dropna(subset=['ìƒí’ˆID'])
        
        # ë°ì´í„° íƒ€ì… ê²€ì¦ ë° ë³€í™˜
        if not pd.api.types.is_numeric_dtype(margin_df['íŒë§¤ê°€']):
            logging.warning("íŒë§¤ê°€ ì»¬ëŸ¼ì´ ìˆ«ì íƒ€ì…ì´ ì•„ë‹™ë‹ˆë‹¤. ë³€í™˜ì„ ì‹œë„í•©ë‹ˆë‹¤.")
            margin_df['íŒë§¤ê°€'] = pd.to_numeric(margin_df['íŒë§¤ê°€'], errors='coerce')
        
        if not pd.api.types.is_numeric_dtype(margin_df['ë§ˆì§„ìœ¨']):
            logging.warning("ë§ˆì§„ìœ¨ ì»¬ëŸ¼ì´ ìˆ«ì íƒ€ì…ì´ ì•„ë‹™ë‹ˆë‹¤. ë³€í™˜ì„ ì‹œë„í•©ë‹ˆë‹¤.")
            margin_df['ë§ˆì§„ìœ¨'] = pd.to_numeric(margin_df['ë§ˆì§„ìœ¨'], errors='coerce')
        
        # ëŒ€í‘œì˜µì…˜ ì •ë³´ ì²˜ë¦¬
        if 'ëŒ€í‘œì˜µì…˜' in margin_df.columns:
            margin_df['ëŒ€í‘œì˜µì…˜'] = margin_df['ëŒ€í‘œì˜µì…˜'].astype(str).str.upper().isin(['O', 'Y', 'TRUE'])
            rep_price_map = margin_df[margin_df['ëŒ€í‘œì˜µì…˜'] == True].set_index('ìƒí’ˆID')['íŒë§¤ê°€'].to_dict()
            logging.info("ëŒ€í‘œì˜µì…˜ íŒë§¤ê°€ ì •ë³´ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")
        else:
            logging.warning(f"ê²½ê³ : '{os.path.basename(config.MARGIN_FILE)}'ì— 'ëŒ€í‘œì˜µì…˜' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            margin_df['ëŒ€í‘œì˜µì…˜'] = False
            rep_price_map = {}
            
        # ì˜µì…˜ì •ë³´ ì •ê·œí™” (ë§ˆì§„ì •ë³´) - pandasì˜ nullable ë°ì´í„° ì²˜ë¦¬ ëª¨ë²”ì‚¬ë¡€ ì ìš©
        def normalize_option_info(value):
            """ì˜µì…˜ì •ë³´ ì •ê·œí™” - ë§ˆì§„ì •ë³´ ë§¤ì¹­ì„ ìœ„í•œ ì¼ê´€ëœ ì²˜ë¦¬"""
            if pd.isna(value):
                return ''
            
            value_str = str(value).strip()
            # ë‹¤ì–‘í•œ ë¹ˆê°’ í‘œí˜„ë“¤ì„ ë¹ˆ ë¬¸ìì—´ë¡œ í†µì¼ (ë§ˆì§„ì •ë³´ì™€ ë§¤ì¹­ë˜ë„ë¡)
            if value_str == '' or value_str.lower() in ['ë‹¨ì¼', 'ê¸°ë³¸ì˜µì…˜', 'ì„ íƒì•ˆí•¨', 'null', 'none', 'ì—†ìŒ', 'nan']:
                return ''
            
            return value_str
            
        if 'ì˜µì…˜ì •ë³´' not in margin_df.columns:
            margin_df['ì˜µì…˜ì •ë³´'] = ''
        else:
            margin_df['ì˜µì…˜ì •ë³´'] = margin_df['ì˜µì…˜ì •ë³´'].apply(normalize_option_info)
            
    except FileNotFoundError:
        logging.error(f"ë§ˆì§„ì •ë³´ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {config.MARGIN_FILE}")
        return []
    except PermissionError:
        logging.error(f"ë§ˆì§„ì •ë³´ íŒŒì¼ì— ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {config.MARGIN_FILE}")
        return []
    except ValueError as e:
        logging.error(f"ë§ˆì§„ì •ë³´ íŒŒì¼ ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨: {e}")
        return []
    except Exception as e:
        logging.error(f"ë§ˆì§„ì •ë³´ íŒŒì¼ ì½ê¸° ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
        return []

    # ì²˜ë¦¬ ê°€ëŠ¥í•œ íŒŒì¼ë“¤ ì°¾ê¸°
    logging.info(f"ğŸ” ì‘ì—…í´ë” ìŠ¤ìº”: {config.get_processing_dir()}")
    all_files = [f for f in os.listdir(config.get_processing_dir()) if f.endswith('.xlsx') and not f.startswith('~')]
    logging.info(f"ğŸ“„ ì „ì²´ Excel íŒŒì¼ë“¤ ({len(all_files)}ê°œ): {all_files}")
    
    source_files = [f for f in all_files if 'í†µí•©_ë¦¬í¬íŠ¸' not in f and 'ë§ˆì§„ì •ë³´' not in f]
    logging.info(f"ğŸ“Š ì›ë³¸ íŒŒì¼ë“¤ ({len(source_files)}ê°œ): {source_files}")

    # ì£¼ë¬¸ì¡°íšŒ íŒŒì¼ë§Œ í•„í„°ë§
    order_files = [f for f in source_files if 'ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´_ì£¼ë¬¸ì¡°íšŒ' in f]
    logging.info(f"ğŸ›’ ì£¼ë¬¸ì¡°íšŒ íŒŒì¼ë“¤ ({len(order_files)}ê°œ): {order_files}")
    
    if not order_files:
        logging.warning("âš ï¸ ì²˜ë¦¬í•  ì£¼ë¬¸ì¡°íšŒ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤!")
        logging.info("ğŸ“‹ íŒŒì¼ëª… íŒ¨í„´ì„ í™•ì¸í•´ì£¼ì„¸ìš”: íŒŒì¼ëª…ì— 'ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´_ì£¼ë¬¸ì¡°íšŒ'ê°€ í¬í•¨ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.")
        return []  # True ëŒ€ì‹  ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜

    logging.info(f"ì´ {len(order_files)}ê°œì˜ ì£¼ë¬¸ì¡°íšŒ íŒŒì¼ì— ëŒ€í•œ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
    processed_groups = []
    
    for order_file in order_files:
        # íŒŒì¼ëª…ì—ì„œ ìŠ¤í† ì–´ëª…ê³¼ ë‚ ì§œ ì¶”ì¶œ
        if 'ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´_ì£¼ë¬¸ì¡°íšŒ' in order_file:
            parts = order_file.split(' ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´_ì£¼ë¬¸ì¡°íšŒ_')
            if len(parts) == 2:
                store = parts[0]
                date = parts[1].replace('.xlsx', '')
            else:
                continue
        else:
            continue
            
        output_filename = f'{store}_í†µí•©_ë¦¬í¬íŠ¸_{date}.xlsx'
        output_path = os.path.join(config.get_processing_dir(), output_filename)
        
        # ì´ë¯¸ ë¦¬í¬íŠ¸ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        if os.path.exists(output_path):
            logging.info(f"- {store} ({date}) ì´ë¯¸ ë¦¬í¬íŠ¸ê°€ ìƒì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            processed_groups.append((store, date))
            continue
            
        logging.info(f"- {store} ({date}) ì£¼ë¬¸ì¡°íšŒ ê¸°ë°˜ ë°ì´í„° ì²˜ë¦¬ ì‹œì‘...")
        
        try:
            # ì´ íŒŒì¼ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¡œì»¬ ë³€ìˆ˜ë“¤ (ë‹¤ë¥¸ íŒŒì¼ê³¼ ì™„ì „íˆ ë…ë¦½)
            local_missing_products = []  # ì´ íŒŒì¼ì—ì„œë§Œ ì‚¬ìš©ë˜ëŠ” ëˆ„ë½ ìƒí’ˆ ë¦¬ìŠ¤íŠ¸
            
            # ì£¼ë¬¸ì¡°íšŒ íŒŒì¼ ì½ê¸° (ì•”í˜¸ ë³´í˜¸ë  ìˆ˜ ìˆìŒ)
            order_path = os.path.join(config.get_processing_dir(), order_file)
            order_df = read_protected_excel(order_path, password=config.ORDER_FILE_PASSWORD)
            
            # íŒŒì¼ì´ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸
            if order_df.empty:
                logging.error(f"-> {store}({date}) ì£¼ë¬¸ì¡°íšŒ íŒŒì¼ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤: {order_file}")
                continue
            
            logging.info(f"-> {store}({date}) ì£¼ë¬¸ì¡°íšŒ íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {len(order_df)}í–‰")
            logging.info(f"-> {store}({date}) ì£¼ë¬¸ì¡°íšŒ íŒŒì¼ ì»¬ëŸ¼: {list(order_df.columns)}")
            
            # ìƒí’ˆë²ˆí˜¸ -> ìƒí’ˆID ë³€í™˜ (ì»¬ëŸ¼ì´ ìˆëŠ” ê²½ìš°ì—ë§Œ)
            if 'ìƒí’ˆë²ˆí˜¸' in order_df.columns:
                order_df = order_df.rename(columns={'ìƒí’ˆë²ˆí˜¸': 'ìƒí’ˆID'})
            
            # í•„ìˆ˜ ì»¬ëŸ¼ ì¡´ì¬ í™•ì¸
            required_cols = ['ìƒí’ˆID']
            missing_cols = [col for col in required_cols if col not in order_df.columns]
            if missing_cols:
                logging.error(f"-> {store}({date}) í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {missing_cols}")
                continue
            
            # ìƒí’ˆID ë°ì´í„° íƒ€ì… ì •ê·œí™” (ë§ˆì§„ì •ë³´ì™€ ë™ì¼í•œ ë°©ì‹)
            order_df['ìƒí’ˆID'] = order_df['ìƒí’ˆID'].apply(normalize_product_id)
            
            # ì˜µì…˜ì •ë³´ ì •ê·œí™” 
            def normalize_option_info(value):
                """ì£¼ë¬¸ì¡°íšŒ ì˜µì…˜ì •ë³´ ì •ê·œí™” - ë§ˆì§„ì •ë³´ì™€ ë™ì¼í•œ ì²˜ë¦¬"""
                if pd.isna(value) or value == '' or str(value).strip() == '':
                    return ''
                value_str = str(value).strip()
                # ë‹¤ì–‘í•œ ë¹ˆê°’ í‘œí˜„ë“¤ì„ ë¹ˆ ë¬¸ìì—´ë¡œ í†µì¼ (ë§ˆì§„ì •ë³´ì™€ ë§¤ì¹­ë˜ë„ë¡)
                if value_str.lower() in ['ë‹¨ì¼', 'ê¸°ë³¸ì˜µì…˜', 'ì„ íƒì•ˆí•¨', 'null', 'none', 'ì—†ìŒ', 'nan']:
                    return ''
                return value_str
            
            if 'ì˜µì…˜ì •ë³´' not in order_df.columns:
                order_df['ì˜µì…˜ì •ë³´'] = ''
            else:
                order_df['ì˜µì…˜ì •ë³´'] = order_df['ì˜µì…˜ì •ë³´'].apply(normalize_option_info)
            
            logging.info(f"-> {store}({date}) ì˜µì…˜ì •ë³´ ì •ê·œí™” í›„ ìƒ˜í”Œ: {order_df['ì˜µì…˜ì •ë³´'].head(5).tolist()}")
            
            # í´ë ˆì„ìƒíƒœ ì»¬ëŸ¼ í™•ì¸ ë° í™˜ë¶ˆ ê´€ë ¨ ì²˜ë¦¬
            if 'í´ë ˆì„ìƒíƒœ' not in order_df.columns:
                # ë‹¤ë¥¸ ê°€ëŠ¥í•œ ì»¬ëŸ¼ëª…ë“¤ í™•ì¸
                possible_status_cols = ['ìƒíƒœ', 'ì£¼ë¬¸ìƒíƒœ', 'ì²˜ë¦¬ìƒíƒœ', 'ë°°ì†¡ìƒíƒœ', 'ì£¼ë¬¸ì²˜ë¦¬ìƒíƒœ', 'ê²°ì œìƒíƒœ']
                status_col = None
                for col in possible_status_cols:
                    if col in order_df.columns:
                        status_col = col
                        break
                
                if status_col:
                    logging.info(f"-> {store}({date}) '{status_col}' ì»¬ëŸ¼ì„ í´ë ˆì„ìƒíƒœë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                    order_df['í´ë ˆì„ìƒíƒœ'] = order_df[status_col]
                else:
                    logging.warning(f"-> {store}({date}) í´ë ˆì„ìƒíƒœ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    order_df['í´ë ˆì„ìƒíƒœ'] = 'ì •ìƒ'
            
            # ìˆ˜ëŸ‰ ì»¬ëŸ¼ í™•ì¸
            if 'ìˆ˜ëŸ‰' not in order_df.columns:
                possible_quantity_cols = ['ê²°ì œìˆ˜ëŸ‰', 'ì£¼ë¬¸ìˆ˜ëŸ‰', 'ìƒí’ˆìˆ˜ëŸ‰', 'ê²°ì œìƒí’ˆìˆ˜ëŸ‰']
                quantity_col = None
                for col in possible_quantity_cols:
                    if col in order_df.columns:
                        quantity_col = col
                        break
                
                if quantity_col:
                    logging.info(f"-> {store}({date}) '{quantity_col}' ì»¬ëŸ¼ì„ ìˆ˜ëŸ‰ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                    order_df['ìˆ˜ëŸ‰'] = order_df[quantity_col]
                else:
                    logging.warning(f"-> {store}({date}) ìˆ˜ëŸ‰ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ 1 ì‚¬ìš©")
                    order_df['ìˆ˜ëŸ‰'] = 1
            
            # ìˆ˜ëŸ‰ì„ ìˆ«ìí˜•ìœ¼ë¡œ ë³€í™˜
            order_df['ìˆ˜ëŸ‰'] = pd.to_numeric(order_df['ìˆ˜ëŸ‰'], errors='coerce').fillna(1)
            
            # í´ë ˆì„ìƒíƒœ ë¶„í¬ í™•ì¸
            status_counts = order_df['í´ë ˆì„ìƒíƒœ'].value_counts()
            logging.info(f"-> {store}({date}) í´ë ˆì„ìƒíƒœ ë¶„í¬: {status_counts.to_dict()}")
            
            # í™˜ë¶ˆìˆ˜ëŸ‰ ê³„ì‚°
            cancel_mask = order_df['í´ë ˆì„ìƒíƒœ'].isin(config.CANCEL_OR_REFUND_STATUSES)
            order_df['í™˜ë¶ˆìˆ˜ëŸ‰'] = order_df['ìˆ˜ëŸ‰'].where(cancel_mask, 0)
            
            # í™˜ë¶ˆìˆ˜ëŸ‰ ê³„ì‚° ê²°ê³¼
            total_refund_quantity = order_df['í™˜ë¶ˆìˆ˜ëŸ‰'].sum()
            refund_rows = (order_df['í™˜ë¶ˆìˆ˜ëŸ‰'] > 0).sum()
            logging.info(f"-> {store}({date}) ì´ í™˜ë¶ˆìˆ˜ëŸ‰: {total_refund_quantity}, í™˜ë¶ˆ í–‰ ìˆ˜: {refund_rows}")
            
            # ì˜µì…˜ë³„ ì§‘ê³„ (í•µì‹¬ ë¡œì§!) - ìƒí’ˆëª…ë„ í•¨ê»˜ ì§‘ê³„
            logging.info(f"-> {store}({date}) ì˜µì…˜ë³„ ë°ì´í„° ì§‘ê³„ ì‹œì‘...")
            
            # ìƒí’ˆëª… ì»¬ëŸ¼ í™•ì¸
            if 'ìƒí’ˆëª…' in order_df.columns:
                group_cols = ['ìƒí’ˆID', 'ìƒí’ˆëª…', 'ì˜µì…˜ì •ë³´']
                agg_dict = {
                    'ìˆ˜ëŸ‰': 'sum',           # ì˜µì…˜ë³„ ì´ íŒë§¤ìˆ˜ëŸ‰
                    'í™˜ë¶ˆìˆ˜ëŸ‰': 'sum'        # ì˜µì…˜ë³„ ì´ í™˜ë¶ˆìˆ˜ëŸ‰
                }
            else:
                group_cols = ['ìƒí’ˆID', 'ì˜µì…˜ì •ë³´'] 
                agg_dict = {
                    'ìˆ˜ëŸ‰': 'sum',           # ì˜µì…˜ë³„ ì´ íŒë§¤ìˆ˜ëŸ‰
                    'í™˜ë¶ˆìˆ˜ëŸ‰': 'sum'        # ì˜µì…˜ë³„ ì´ í™˜ë¶ˆìˆ˜ëŸ‰
                }
                logging.warning(f"-> {store}({date}) ì£¼ë¬¸ì¡°íšŒ íŒŒì¼ì— ìƒí’ˆëª… ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            
            # ì¤‘ë³µ ë°ì´í„° ê²€ì¦
            duplicates = order_df.duplicated(group_cols).sum()
            if duplicates > 0:
                logging.warning(f"-> {store}({date}) ì£¼ë¬¸ì¡°íšŒ ë°ì´í„°ì— ì¤‘ë³µëœ ìƒí’ˆID-ì˜µì…˜ì •ë³´ ì¡°í•©ì´ {duplicates}ê°œ ìˆìŠµë‹ˆë‹¤.")
            
            option_summary = order_df.groupby(group_cols, as_index=False).agg(agg_dict)
            
            logging.info(f"-> {store}({date}) ì˜µì…˜ë³„ ì§‘ê³„ ì™„ë£Œ: {len(option_summary)}ê°œ ì˜µì…˜")
            
            # ë¦¬ì›Œë“œê°€ ì„¤ì •ëœ ìƒí’ˆ ì¤‘ ëˆ„ë½ëœ ê²ƒë“¤ì„ 0 ë°ì´í„°ë¡œ ì¶”ê°€
            logging.info(f"-> {store}({date}) ë¦¬ì›Œë“œ ì„¤ì •ëœ ìƒí’ˆ ì¤‘ ëˆ„ë½ëœ ìƒí’ˆ ì²´í¬...")
            try:
                # ë¦¬ì›Œë“œ ì„¤ì • íŒŒì¼ì—ì„œ í•´ë‹¹ ë‚ ì§œì˜ ì„¤ì •ëœ ìƒí’ˆë“¤ ê°€ì ¸ì˜¤ê¸°
                reward_file = os.path.join(config.BASE_DIR, 'ë¦¬ì›Œë“œì„¤ì •.json')
                logging.info(f"-> {store}({date}) ë¦¬ì›Œë“œ íŒŒì¼ ê²½ë¡œ: {reward_file}")
                logging.info(f"-> {store}({date}) ë¦¬ì›Œë“œ íŒŒì¼ ì ˆëŒ€ê²½ë¡œ: {os.path.abspath(reward_file)}")
                
                if os.path.exists(reward_file):
                    logging.info(f"-> {store}({date}) ë¦¬ì›Œë“œ íŒŒì¼ ì¡´ì¬í•¨, íŒŒì¼ í¬ê¸°: {os.path.getsize(reward_file)} bytes")
                    logging.info(f"-> {store}({date}) íŒŒì¼ ì½ê¸° ê¶Œí•œ í™•ì¸: {os.access(reward_file, os.R_OK)}")
                    
                    try:
                        with open(reward_file, 'r', encoding='utf-8') as f:
                            file_content = f.read()
                            logging.info(f"-> {store}({date}) íŒŒì¼ ë‚´ìš© ê¸¸ì´: {len(file_content)} ë¬¸ì")
                            logging.info(f"-> {store}({date}) íŒŒì¼ ë‚´ìš© ì²« 100ì: {file_content[:100]}")
                            
                        # JSON íŒŒì‹±
                        with open(reward_file, 'r', encoding='utf-8') as f:
                            reward_data = json.load(f)
                            logging.info(f"-> {store}({date}) JSON íŒŒì‹± ì„±ê³µ, ë°ì´í„° íƒ€ì…: {type(reward_data)}")
                            if isinstance(reward_data, dict):
                                logging.info(f"-> {store}({date}) JSON í‚¤ë“¤: {list(reward_data.keys())}")
                                if 'rewards' in reward_data:
                                    logging.info(f"-> {store}({date}) rewards ë°°ì—´ ê¸¸ì´: {len(reward_data.get('rewards', []))}")
                    except json.JSONDecodeError as e:
                        logging.error(f"-> {store}({date}) JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
                        logging.error(f"-> {store}({date}) ì˜¤ë¥˜ ìœ„ì¹˜: line {e.lineno}, column {e.colno}")
                        raise
                    except Exception as e:
                        logging.error(f"-> {store}({date}) íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}")
                        raise
                    
                    # í•´ë‹¹ ë‚ ì§œì— ë¦¬ì›Œë“œê°€ ì„¤ì •ëœ ìƒí’ˆë“¤ ì°¾ê¸°
                    rewarded_products = set()
                    original_to_normalized = {}  # ì›ë³¸ ID -> ì •ê·œí™” ID ë§¤í•‘
                    for reward_entry in reward_data.get('rewards', []):
                        start_date = reward_entry.get('start_date', '')
                        end_date = reward_entry.get('end_date', '')
                        product_id = str(reward_entry.get('product_id', ''))
                        reward_amount = reward_entry.get('reward', 0)
                        
                        # ìœ íš¨í•œ ë¦¬ì›Œë“œì¸ì§€ ì²´í¬: ë‚ ì§œ ë²”ìœ„, ìƒí’ˆID, ë¦¬ì›Œë“œ ê¸ˆì•¡ ëª¨ë‘ í™•ì¸
                        if (start_date <= date <= end_date and 
                            product_id and 
                            product_id != 'nan' and 
                            product_id != '' and
                            reward_amount and 
                            reward_amount > 0):
                            # ìƒí’ˆID ì •ê·œí™”í•˜ì—¬ ì €ì¥ (.0 ì œê±°)
                            normalized_id = normalize_product_id(product_id)
                            rewarded_products.add(normalized_id)
                            original_to_normalized[product_id] = normalized_id
                            logging.debug(f"-> {store}({date}) ìœ íš¨í•œ ë¦¬ì›Œë“œ ìƒí’ˆ ì¶”ê°€: {product_id} -> {normalized_id} (ë¦¬ì›Œë“œ: {reward_amount}ì›)")
                    
                    logging.info(f"-> {store}({date}) ë¦¬ì›Œë“œ ì„¤ì •ëœ ìƒí’ˆë“¤: {list(rewarded_products)}")
                    
                    # ë§ˆì§„ì •ë³´ì— ìŠ¤í† ì–´ ì»¬ëŸ¼ì´ ìˆìœ¼ë©´ í•´ë‹¹ ìŠ¤í† ì–´ ìƒí’ˆë§Œ í•„í„°ë§
                    if 'ìŠ¤í† ì–´' in margin_df.columns:
                        # í˜„ì¬ ìŠ¤í† ì–´ì—ì„œ íŒë§¤í•˜ëŠ” ìƒí’ˆë“¤ë§Œ ì¶”ì¶œ (ì •ê·œí™”í•˜ì—¬ ì €ì¥)
                        store_products = set(margin_df[margin_df['ìŠ¤í† ì–´'] == store]['ìƒí’ˆID'].astype(str).apply(normalize_product_id))
                        logging.info(f"-> {store}({date}) {store} ìŠ¤í† ì–´ì—ì„œ íŒë§¤í•˜ëŠ” ìƒí’ˆ: {len(store_products)}ê°œ")
                        
                        # ë¦¬ì›Œë“œ ì„¤ì •ëœ ìƒí’ˆ ì¤‘ ì´ ìŠ¤í† ì–´ì—ì„œ íŒë§¤í•˜ëŠ” ìƒí’ˆë§Œ ì²´í¬
                        store_rewarded_products = rewarded_products & store_products
                        logging.info(f"-> {store}({date}) {store} ìŠ¤í† ì–´ì—ì„œ íŒë§¤í•˜ëŠ” ë¦¬ì›Œë“œ ì„¤ì • ìƒí’ˆ: {list(store_rewarded_products)}")
                    else:
                        # ìŠ¤í† ì–´ ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ê¸°ì¡´ ë°©ì‹ ì‚¬ìš© (ëª¨ë“  ìƒí’ˆ ì²´í¬)
                        store_rewarded_products = rewarded_products
                        logging.info(f"-> {store}({date}) ìŠ¤í† ì–´ ì»¬ëŸ¼ì´ ì—†ì–´ ëª¨ë“  ë¦¬ì›Œë“œ ìƒí’ˆ ì²´í¬")
                    
                    # ì£¼ë¬¸ì¡°íšŒì— ì—†ëŠ” ë¦¬ì›Œë“œ ì„¤ì • ìƒí’ˆë“¤ ì°¾ê¸° (ìŠ¤í† ì–´ë³„ë¡œ í•„í„°ë§ë¨)
                    # ê¸°ì¡´ ìƒí’ˆë“¤ë„ ì •ê·œí™”í•˜ì—¬ ë¹„êµ
                    existing_products = set(option_summary['ìƒí’ˆID'].astype(str).apply(normalize_product_id))
                    missing_rewarded_products = store_rewarded_products - existing_products
                    
                    if missing_rewarded_products:
                        logging.info(f"-> {store}({date}) {store} ìŠ¤í† ì–´ì—ì„œ ì£¼ë¬¸ì¡°íšŒì— ì—†ëŠ” ë¦¬ì›Œë“œ ì„¤ì • ìƒí’ˆ {len(missing_rewarded_products)}ê°œ: {list(missing_rewarded_products)}")
                        
                        # ëˆ„ë½ëœ ìƒí’ˆë“¤ì„ 0 ë°ì´í„°ë¡œ ì¶”ê°€
                        for normalized_product_id in missing_rewarded_products:
                            # ë§ˆì§„ì •ë³´ì—ì„œ ì •ê·œí™”ëœ IDë¡œ ë§¤ì¹­í•˜ì—¬ ìƒí’ˆ ì •ë³´ ì°¾ê¸°
                            margin_df_normalized = margin_df.copy()
                            margin_df_normalized['ì •ê·œí™”_ìƒí’ˆID'] = margin_df_normalized['ìƒí’ˆID'].astype(str).apply(normalize_product_id)
                            # ëŒ€í‘œì˜µì…˜ì„ booleanìœ¼ë¡œ ë³€í™˜ (ë©”ì¸ ë¡œì§ê³¼ ë™ì¼í•˜ê²Œ)
                            if 'ëŒ€í‘œì˜µì…˜' in margin_df_normalized.columns:
                                margin_df_normalized['ëŒ€í‘œì˜µì…˜'] = margin_df_normalized['ëŒ€í‘œì˜µì…˜'].astype(str).str.upper().isin(['O', 'Y', 'TRUE'])
                            
                            if 'ìŠ¤í† ì–´' in margin_df.columns:
                                # ë‹¨ê³„ë³„ ë””ë²„ê¹… ë¡œê·¸
                                id_matches = margin_df_normalized[margin_df_normalized['ì •ê·œí™”_ìƒí’ˆID'] == normalized_product_id]
                                logging.info(f"-> {store}({date}) ìƒí’ˆ {normalized_product_id}: ì •ê·œí™”ID ë§¤ì¹­ {len(id_matches)}ê°œ")
                                
                                store_matches = margin_df_normalized[
                                    (margin_df_normalized['ì •ê·œí™”_ìƒí’ˆID'] == normalized_product_id) & 
                                    (margin_df_normalized['ìŠ¤í† ì–´'] == store)
                                ]
                                logging.info(f"-> {store}({date}) ìƒí’ˆ {normalized_product_id}: ìŠ¤í† ì–´ ë§¤ì¹­ {len(store_matches)}ê°œ")
                                if len(store_matches) > 0:
                                    logging.info(f"-> {store}({date}) ìƒí’ˆ {normalized_product_id}: ëŒ€í‘œì˜µì…˜ ê°’ë“¤ {store_matches['ëŒ€í‘œì˜µì…˜'].tolist()}")
                                
                                product_margin = margin_df_normalized[
                                    (margin_df_normalized['ì •ê·œí™”_ìƒí’ˆID'] == normalized_product_id) & 
                                    (margin_df_normalized['ìŠ¤í† ì–´'] == store) &
                                    (margin_df_normalized['ëŒ€í‘œì˜µì…˜'] == True)
                                ]
                                logging.info(f"-> {store}({date}) ìƒí’ˆ {normalized_product_id}: ìµœì¢… ë§¤ì¹­ {len(product_margin)}ê°œ")
                            else:
                                # ìŠ¤í† ì–´ ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ê¸°ì¡´ ë°©ì‹
                                product_margin = margin_df_normalized[
                                    (margin_df_normalized['ì •ê·œí™”_ìƒí’ˆID'] == normalized_product_id) & 
                                    (margin_df_normalized['ëŒ€í‘œì˜µì…˜'] == True)
                                ]
                            
                            if len(product_margin) > 0:
                                product_info = product_margin.iloc[0]
                                # 0 ë°ì´í„° í–‰ ìƒì„± (ì •ê·œí™”ëœ ìƒí’ˆID ì‚¬ìš©)
                                zero_row = {
                                    'ìƒí’ˆID': normalized_product_id,
                                    'ì˜µì…˜ì •ë³´': product_info.get('ì˜µì…˜ì •ë³´', ''),
                                    'ìˆ˜ëŸ‰': 0,
                                    'í™˜ë¶ˆìˆ˜ëŸ‰': 0
                                }
                                
                                # ìƒí’ˆëª… ì¶”ê°€ (ë§ˆì§„ì •ë³´ì—ì„œ ê°€ì ¸ì˜¤ê¸°)
                                if 'ìƒí’ˆëª…' in product_info:
                                    zero_row['ìƒí’ˆëª…'] = product_info['ìƒí’ˆëª…']
                                else:
                                    zero_row['ìƒí’ˆëª…'] = f'ìƒí’ˆ{normalized_product_id}'
                                
                                local_missing_products.append(zero_row)
                                logging.info(f"-> {store}({date}) 0 ë°ì´í„° ì¶”ê°€: ìƒí’ˆ {normalized_product_id} (ìƒí’ˆëª…: {zero_row['ìƒí’ˆëª…']})")
                            else:
                                logging.info(f"-> {store}({date}) ìƒí’ˆ {normalized_product_id}ì˜ ëŒ€í‘œì˜µì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                    else:
                        logging.info(f"-> {store}({date}) ëª¨ë“  ë¦¬ì›Œë“œ ì„¤ì • ìƒí’ˆì´ ì£¼ë¬¸ì¡°íšŒì— ì¡´ì¬í•©ë‹ˆë‹¤.")
                
                # ëˆ„ë½ëœ ìƒí’ˆë“¤ì„ option_summaryì— ì¶”ê°€
                if local_missing_products:
                    missing_df = pd.DataFrame(local_missing_products)
                    option_summary = pd.concat([option_summary, missing_df], ignore_index=True)
                    logging.info(f"-> {store}({date}) {len(local_missing_products)}ê°œ ë¦¬ì›Œë“œ ìƒí’ˆì„ 0 ë°ì´í„°ë¡œ ì¶”ê°€ ì™„ë£Œ")
                    logging.info(f"-> {store}({date}) ìµœì¢… ì˜µì…˜ë³„ ì§‘ê³„: {len(option_summary)}ê°œ ì˜µì…˜")
                else:
                    logging.info(f"-> {store}({date}) ë¦¬ì›Œë“œ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {reward_file}")
                    logging.info(f"-> {store}({date}) 0 ë°ì´í„° ì¶”ê°€ ì—†ì´ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
                    
            except json.JSONDecodeError as e:
                logging.error(f"-> {store}({date}) ë¦¬ì›Œë“œ ì„¤ì • íŒŒì¼ JSON í˜•ì‹ ì˜¤ë¥˜: {e}")
                logging.error(f"-> {store}({date}) íŒŒì¼ ë‚´ìš©ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
            except FileNotFoundError:
                logging.info(f"-> {store}({date}) ë¦¬ì›Œë“œ ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. 0 ë°ì´í„° ì¶”ê°€ ì—†ì´ ì§„í–‰í•©ë‹ˆë‹¤.")
            except PermissionError:
                logging.error(f"-> {store}({date}) ë¦¬ì›Œë“œ ì„¤ì • íŒŒì¼ ì½ê¸° ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤.")
            except Exception as e:
                logging.error(f"-> {store}({date}) ë¦¬ì›Œë“œ ì„¤ì • ìƒí’ˆ ì¶”ê°€ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
                import traceback
                logging.error(f"-> {store}({date}) ìƒì„¸ ì˜¤ë¥˜ ì •ë³´: {traceback.format_exc()}")
            
            # íŒë§¤ê°€ëŠ” ë§ˆì§„ì •ë³´ íŒŒì¼ì—ì„œë§Œ ê°€ì ¸ì˜´ (ì£¼ë¬¸ì¡°íšŒ íŒŒì¼ì—ëŠ” íŒë§¤ê°€ ì»¬ëŸ¼ì´ ì—†ìŒ)
            logging.info(f"-> {store}({date}) íŒë§¤ê°€ëŠ” ë§ˆì§„ì •ë³´ íŒŒì¼ì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤.")
            
            # ë³‘í•© ì „ ë°ì´í„° í™•ì¸
            logging.info(f"-> {store}({date}) ë³‘í•© ì „ ì£¼ë¬¸ì¡°íšŒ ìƒí’ˆID ìƒ˜í”Œ: {option_summary['ìƒí’ˆID'].head(3).tolist()}")
            logging.info(f"-> {store}({date}) ë³‘í•© ì „ ì£¼ë¬¸ì¡°íšŒ ì˜µì…˜ì •ë³´ ìƒ˜í”Œ: {option_summary['ì˜µì…˜ì •ë³´'].head(3).tolist()}")
            logging.info(f"-> {store}({date}) ë³‘í•© ì „ ë§ˆì§„ì •ë³´ ìƒí’ˆID ìƒ˜í”Œ: {margin_df['ìƒí’ˆID'].head(3).tolist()}")
            logging.info(f"-> {store}({date}) ë³‘í•© ì „ ë§ˆì§„ì •ë³´ ì˜µì…˜ì •ë³´ ìƒ˜í”Œ: {margin_df['ì˜µì…˜ì •ë³´'].head(3).tolist()}")
            
            # ë§ˆì§„ì •ë³´ì™€ ì•ˆì „í•œ ë³‘í•© with ê²€ì¦
            logging.info(f"-> {store}({date}) ë§ˆì§„ì •ë³´ì™€ ë³‘í•© ì‹œì‘...")
            
            # ë³‘í•© ì „ ë§ˆì§„ì •ë³´ ì¤‘ë³µ ê²€ì¦
            margin_duplicates = margin_df.duplicated(['ìƒí’ˆID', 'ì˜µì…˜ì •ë³´']).sum()
            if margin_duplicates > 0:
                logging.warning(f"-> {store}({date}) ë§ˆì§„ì •ë³´ì— ì¤‘ë³µëœ ìƒí’ˆID-ì˜µì…˜ì •ë³´ ì¡°í•©ì´ {margin_duplicates}ê°œ ìˆìŠµë‹ˆë‹¤.")
                # ì²« ë²ˆì§¸ ê°’ë§Œ ìœ ì§€
                margin_df = margin_df.drop_duplicates(['ìƒí’ˆID', 'ì˜µì…˜ì •ë³´'], keep='first')
                logging.info(f"-> {store}({date}) ì¤‘ë³µ ì œê±° í›„ ë§ˆì§„ì •ë³´ í–‰ ìˆ˜: {len(margin_df)}")
            
            # ë§ˆì§„ì •ë³´ì—ì„œ ìƒí’ˆëª… ì»¬ëŸ¼ ì œê±° (ì£¼ë¬¸ì¡°íšŒì˜ ìƒí’ˆëª… ìœ ì§€)
            margin_cols_to_use = [col for col in margin_df.columns if col != 'ìƒí’ˆëª…']
            margin_df_clean = margin_df[margin_cols_to_use].copy()
            
            try:
                # ì•ˆì „í•œ ë³‘í•© with validation (ìƒí’ˆëª…ì€ ì£¼ë¬¸ì¡°íšŒì—ì„œë§Œ ì‚¬ìš©)
                final_df = pd.merge(
                    option_summary, 
                    margin_df_clean, 
                    on=['ìƒí’ˆID', 'ì˜µì…˜ì •ë³´'], 
                    how='left',
                    validate='many_to_one'  # ë§ˆì§„ì •ë³´ì˜ ê° ìƒí’ˆ-ì˜µì…˜ì€ ê³ ìœ í•´ì•¼ í•¨
                )
                
                # ë§¤ì¹­ ê²°ê³¼ í™•ì¸
                margin_matched = final_df['íŒë§¤ê°€'].notna().sum()
                total_products = len(final_df)
                match_rate = (margin_matched / total_products) * 100 if total_products > 0 else 0
                
                logging.info(f"-> {store}({date}) ë§ˆì§„ì •ë³´ ë§¤ì¹­ ê²°ê³¼: {margin_matched}/{total_products} ({match_rate:.1f}%)")
                
                # ë§¤ì¹­ ì‹¤íŒ¨í•œ ë¹ˆ ì˜µì…˜ì •ë³´ ìƒí’ˆë“¤ì— ëŒ€í•œ ëŒ€ì•ˆ ë§¤ì¹­ ì‹œë„
                unmatched_df = final_df[final_df['íŒë§¤ê°€'].isna()]
                empty_option_unmatched = unmatched_df[unmatched_df['ì˜µì…˜ì •ë³´'] == '']
                
                if len(empty_option_unmatched) > 0:
                    logging.info(f"-> {store}({date}) ë¹ˆ ì˜µì…˜ì •ë³´ë¡œ ë§¤ì¹­ ì‹¤íŒ¨í•œ ìƒí’ˆ {len(empty_option_unmatched)}ê°œ, ìƒí’ˆIDë§Œìœ¼ë¡œ ì¬ë§¤ì¹­ ì‹œë„")
                    
                    # ë¹ˆ ì˜µì…˜ì •ë³´ë¥¼ ê°€ì§„ ë§ˆì§„ì •ë³´ë¡œ ë§¤ì¹­ ì‹œë„
                    margin_empty_options = margin_df_clean[margin_df_clean['ì˜µì…˜ì •ë³´'] == ''].copy()
                    
                    if len(margin_empty_options) > 0:
                        # ìƒí’ˆIDë§Œìœ¼ë¡œ ë§¤ì¹­ (ì˜µì…˜ì •ë³´ëŠ” ë¹ˆê°’ë¼ë¦¬)
                        for idx, row in empty_option_unmatched.iterrows():
                            product_id = row['ìƒí’ˆID']
                            margin_match = margin_empty_options[margin_empty_options['ìƒí’ˆID'] == product_id]
                            
                            if len(margin_match) > 0:
                                # ë§¤ì¹­ëœ ë§ˆì§„ì •ë³´ë¡œ ì—…ë°ì´íŠ¸
                                margin_info = margin_match.iloc[0]
                                for col in margin_info.index:
                                    if col not in ['ìƒí’ˆID', 'ì˜µì…˜ì •ë³´']:  # í‚¤ ì»¬ëŸ¼ ì œì™¸
                                        final_df.at[idx, col] = margin_info[col]
                        
                        # ì¬ë§¤ì¹­ í›„ ê²°ê³¼ í™•ì¸
                        margin_matched_after = final_df['íŒë§¤ê°€'].notna().sum()
                        additional_matches = margin_matched_after - margin_matched
                        if additional_matches > 0:
                            logging.info(f"-> {store}({date}) ë¹ˆ ì˜µì…˜ì •ë³´ ì¬ë§¤ì¹­ìœ¼ë¡œ {additional_matches}ê°œ ì¶”ê°€ ë§¤ì¹­ ì„±ê³µ")
                
                final_match_rate = (final_df['íŒë§¤ê°€'].notna().sum() / len(final_df)) * 100 if len(final_df) > 0 else 0
                logging.info(f"-> {store}({date}) ìµœì¢… ë§ˆì§„ì •ë³´ ë§¤ì¹­ë¥ : {final_match_rate:.1f}%")
            except pd.errors.MergeError as e:
                logging.error(f"-> {store}({date}) ë³‘í•© ê²€ì¦ ì‹¤íŒ¨: {e}")
                # validation ì—†ì´ ì¬ì‹œë„
                final_df = pd.merge(option_summary, margin_df_clean, on=['ìƒí’ˆID', 'ì˜µì…˜ì •ë³´'], how='left')
            
            # ë³‘í•© ê²°ê³¼ í™•ì¸
            merged_count = len(final_df)
            margin_matched = final_df['ë§ˆì§„ìœ¨'].notna().sum()
            logging.info(f"-> {store}({date}) ë³‘í•© ì™„ë£Œ: {merged_count}í–‰, ë§ˆì§„ ë§¤ì¹­ {margin_matched}í–‰")
            
            # ë§¤ì¹­ ì‹¤íŒ¨í•œ ê²½ìš° ë””ë²„ê¹… ì •ë³´ ë° ë³€ë“œì„ í†µí•œ ëŒ€ì•ˆ ë§¤ì¹­ ì‹œë„
            if margin_matched == 0:
                logging.warning(f"-> {store}({date}) ë§ˆì§„ì •ë³´ ë§¤ì¹­ ì‹¤íŒ¨! ë””ë²„ê¹… ì •ë³´:")
                logging.warning(f"   ì£¼ë¬¸ì¡°íšŒ ê³ ìœ  ìƒí’ˆID: {option_summary['ìƒí’ˆID'].unique()[:5]}")
                logging.warning(f"   ë§ˆì§„ì •ë³´ ê³ ìœ  ìƒí’ˆID: {margin_df['ìƒí’ˆID'].unique()[:5]}")
                logging.warning(f"   ì£¼ë¬¸ì¡°íšŒ ê³ ìœ  ì˜µì…˜ì •ë³´: {option_summary['ì˜µì…˜ì •ë³´'].unique()[:5]}")
                logging.warning(f"   ë§ˆì§„ì •ë³´ ê³ ìœ  ì˜µì…˜ì •ë³´: {margin_df['ì˜µì…˜ì •ë³´'].unique()[:5]}")
                
                # ìƒí’ˆIDë§Œìœ¼ë¡œ ëŒ€ì•ˆ ë§¤ì¹­ ì‹œë„ (ì˜µì…˜ ë¬´ì‹œ)
                logging.info(f"-> {store}({date}) ì˜µì…˜ì •ë³´ ì—†ì´ ìƒí’ˆIDë§Œìœ¼ë¡œ ëŒ€ì•ˆ ë§¤ì¹­ ì‹œë„...")
                
                # ë¹ˆ ì˜µì…˜ì •ë³´ë§Œ í•„í„°ë§í•˜ì—¬ ëŒ€ì•ˆ ë§¤ì¹­ (ìƒí’ˆëª…ë„ ì œì™¸)
                margin_df_no_option = margin_df[margin_df['ì˜µì…˜ì •ë³´'] == ''].copy()
                if len(margin_df_no_option) > 0:
                    # ì˜µì…˜ì •ë³´ì™€ ìƒí’ˆëª… ëª¨ë‘ ì œì™¸
                    alt_cols = margin_df_no_option.columns.difference(['ì˜µì…˜ì •ë³´', 'ìƒí’ˆëª…'])
                    final_df_alt = pd.merge(
                        option_summary, 
                        margin_df_no_option[alt_cols], 
                        on='ìƒí’ˆID', 
                        how='left'
                    )
                    alt_matched = final_df_alt['ë§ˆì§„ìœ¨'].notna().sum()
                    if alt_matched > 0:
                        logging.info(f"-> {store}({date}) ëŒ€ì•ˆ ë§¤ì¹­ ì„±ê³µ: {alt_matched}ê°œ ìƒí’ˆ ë§¤ì¹­")
                        # ì˜µì…˜ì •ë³´ ì»¬ëŸ¼ ë‹¤ì‹œ ì¶”ê°€
                        final_df_alt['ì˜µì…˜ì •ë³´'] = option_summary['ì˜µì…˜ì •ë³´']
                        final_df = final_df_alt
                        margin_matched = alt_matched
            
            # ê¸°ë³¸ê°’ ì„¤ì • ë° ë°ì´í„° íƒ€ì… ê²€ì¦
            numeric_columns = ['ë§ˆì§„ìœ¨', 'íŒë§¤ê°€', 'ê°œë‹¹ ê°€êµ¬ë§¤ ë¹„ìš©']
            for col in numeric_columns:
                if col in final_df.columns:
                    # ìˆ«ì íƒ€ì…ì„ ê°•ì œë¡œ ë³€í™˜
                    final_df[col] = pd.to_numeric(final_df[col], errors='coerce')
            
            final_df.fillna({
                'ë§ˆì§„ìœ¨': 0.0, 
                'íŒë§¤ê°€': 0.0,  # ë§ˆì§„ì •ë³´ì˜ íŒë§¤ê°€
                'ê°œë‹¹ ê°€êµ¬ë§¤ ë¹„ìš©': 0.0, 
                'ëŒ€í‘œì˜µì…˜': False
            }, inplace=True)
            
            # ìƒí’ˆëª… í™•ì¸ (ë§ˆì§„ì •ë³´ì—ì„œ ìƒí’ˆëª…ì„ ì œì™¸í–ˆìœ¼ë¯€ë¡œ ì£¼ë¬¸ì¡°íšŒì˜ ìƒí’ˆëª…ì´ ìœ ì§€ë¨)
            logging.info(f"-> {store}({date}) ìƒí’ˆëª… í™•ì¸ - í˜„ì¬ ì»¬ëŸ¼: {list(final_df.columns)}")
            
            if 'ìƒí’ˆëª…' not in final_df.columns:
                logging.error(f"-> {store}({date}) ìƒí’ˆëª… ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
                # ì‘ê¸‰ ì²˜ì¹˜: ìƒí’ˆIDë¥¼ ìƒí’ˆëª…ìœ¼ë¡œ ì‚¬ìš©
                final_df['ìƒí’ˆëª…'] = final_df['ìƒí’ˆID']
                logging.warning(f"-> {store}({date}) ì„ì‹œë¡œ ìƒí’ˆIDë¥¼ ìƒí’ˆëª…ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            else:
                logging.info(f"-> {store}({date}) ìƒí’ˆëª… ìœ ì§€ ì™„ë£Œ - ìƒ˜í”Œ: {final_df['ìƒí’ˆëª…'].head(2).tolist()}")
            
            # ê¸°ë³¸ ê³„ì‚° í•„ë“œë“¤
            final_df['ê²°ì œê¸ˆì•¡'] = final_df['ìˆ˜ëŸ‰'] * final_df['íŒë§¤ê°€']
            final_df['í™˜ë¶ˆê¸ˆì•¡'] = final_df['í™˜ë¶ˆìˆ˜ëŸ‰'] * final_df['íŒë§¤ê°€'] 
            final_df['ë§¤ì¶œ'] = final_df['ê²°ì œê¸ˆì•¡'] - final_df['í™˜ë¶ˆê¸ˆì•¡']
            
            # ëŒ€í‘œíŒë§¤ê°€ (ê°€êµ¬ë§¤ ê¸ˆì•¡ ê³„ì‚°ìš©)
            final_df['ëŒ€í‘œíŒë§¤ê°€'] = final_df['ìƒí’ˆID'].map(rep_price_map).fillna(0)
            
            # ê°€êµ¬ë§¤ ê°œìˆ˜ ì ìš© (ëŒ€í‘œì˜µì…˜ì—ë§Œ, GUIì—ì„œ ì„¤ì •í•œ ê°’)
            final_df['ê°€êµ¬ë§¤ ê°œìˆ˜'] = 0  # ê¸°ë³¸ê°’
            rep_option_mask = final_df['ëŒ€í‘œì˜µì…˜'] == True
            
            if rep_option_mask.sum() > 0:
                for product_id in final_df.loc[rep_option_mask, 'ìƒí’ˆID'].unique():
                    purchase_count = get_purchase_count_for_date_and_product(product_id, date)
                    final_df.loc[(final_df['ìƒí’ˆID'] == product_id) & rep_option_mask, 'ê°€êµ¬ë§¤ ê°œìˆ˜'] = purchase_count
                    if purchase_count > 0:
                        logging.info(f"-> {store}({date}) ìƒí’ˆ {product_id} ê°€êµ¬ë§¤ ê°œìˆ˜: {purchase_count}")
            
            # ì¶”ê°€ ê³„ì‚° í•„ë“œë“¤
            final_df['ê°€êµ¬ë§¤ ìˆ˜ëŸ‰'] = final_df['ê°€êµ¬ë§¤ ê°œìˆ˜']
            final_df['ê°œë‹¹ ê°€êµ¬ë§¤ ê¸ˆì•¡'] = final_df['ëŒ€í‘œíŒë§¤ê°€']
            final_df['ê°€êµ¬ë§¤ ê¸ˆì•¡'] = final_df['ê°œë‹¹ ê°€êµ¬ë§¤ ê¸ˆì•¡'] * final_df['ê°€êµ¬ë§¤ ìˆ˜ëŸ‰']
            final_df['ìˆœë§¤ì¶œ'] = final_df['ë§¤ì¶œ'] - final_df['ê°€êµ¬ë§¤ ê¸ˆì•¡']
            final_df['ê°€êµ¬ë§¤ ë¹„ìš©'] = final_df['ê°œë‹¹ ê°€êµ¬ë§¤ ë¹„ìš©'] * final_df['ê°€êµ¬ë§¤ ìˆ˜ëŸ‰']
            
            # ë¦¬ì›Œë“œ ì ìš© (ëŒ€í‘œì˜µì…˜ì—ë§Œ)
            final_df['ë¦¬ì›Œë“œ'] = 0
            if rep_option_mask.sum() > 0:
                for product_id in final_df.loc[rep_option_mask, 'ìƒí’ˆID'].unique():
                    reward_value = get_reward_for_date_and_product(product_id, date)
                    final_df.loc[(final_df['ìƒí’ˆID'] == product_id) & rep_option_mask, 'ë¦¬ì›Œë“œ'] = reward_value
                    if reward_value > 0:
                        logging.info(f"-> {store}({date}) ìƒí’ˆ {product_id} ë¦¬ì›Œë“œ: {reward_value}ì›")
            
            # ì•ˆì „í•œ ë‚˜ëˆ„ê¸° í•¨ìˆ˜ ì •ì˜
            def safe_divide(numerator, denominator, fill_value=0.0):
                """ì•ˆì „í•œ ë‚˜ëˆ„ê¸° - 0 ë‚˜ëˆ„ê¸°ì™€ NaN ì²˜ë¦¬"""
                with np.errstate(divide='ignore', invalid='ignore'):
                    result = np.where(
                        (denominator == 0) | pd.isna(denominator),
                        fill_value,
                        numerator / denominator
                    )
                return result
            
            # íŒë§¤ë§ˆì§„ ë° ë¹„ìœ¨ ê³„ì‚° (ì•ˆì „í•œ ë°©ì‹)
            final_df['íŒë§¤ë§ˆì§„'] = final_df['ìˆœë§¤ì¶œ'] * final_df['ë§ˆì§„ìœ¨']
            
            # ê´‘ê³ ë¹„ìœ¨ = (ë¦¬ì›Œë“œ + ê°€êµ¬ë§¤ ë¹„ìš©) / ìˆœë§¤ì¶œ
            final_df['ê´‘ê³ ë¹„ìœ¨'] = safe_divide(
                final_df['ë¦¬ì›Œë“œ'] + final_df['ê°€êµ¬ë§¤ ë¹„ìš©'],
                final_df['ìˆœë§¤ì¶œ'],
                fill_value=0.0  # ìˆœë§¤ì¶œì´ 0ì´ë©´ ê´‘ê³ ë¹„ìœ¨ì€ 0%
            )
            
            final_df['ì´ìœ¤ìœ¨'] = final_df['ë§ˆì§„ìœ¨'] - final_df['ê´‘ê³ ë¹„ìœ¨']
            final_df['ìˆœì´ìµ'] = final_df['íŒë§¤ë§ˆì§„'] - final_df['ê°€êµ¬ë§¤ ë¹„ìš©'] - final_df['ë¦¬ì›Œë“œ']
            
            # í¼ì„¼íŠ¸ ê°’ ë³€í™˜
            final_df['ë§ˆì§„ìœ¨'] = (final_df['ë§ˆì§„ìœ¨'] * 100).round(1)
            final_df['ê´‘ê³ ë¹„ìœ¨'] = (final_df['ê´‘ê³ ë¹„ìœ¨'] * 100).round(1)
            final_df['ì´ìœ¤ìœ¨'] = (final_df['ì´ìœ¤ìœ¨'] * 100).round(1)
            
            # ê²°ì œìˆ˜, í™˜ë¶ˆê±´ìˆ˜ ê³„ì‚° (ì£¼ë¬¸ì¡°íšŒ ê¸°ë°˜)
            if 'ìƒí’ˆì£¼ë¬¸ë²ˆí˜¸' in order_df.columns:
                # ê²°ì œìˆ˜ (ìƒí’ˆì£¼ë¬¸ë²ˆí˜¸ ê°œìˆ˜)
                order_count = order_df.groupby(['ìƒí’ˆID', 'ì˜µì…˜ì •ë³´'])['ìƒí’ˆì£¼ë¬¸ë²ˆí˜¸'].nunique().reset_index()
                order_count.rename(columns={'ìƒí’ˆì£¼ë¬¸ë²ˆí˜¸': 'ê²°ì œìˆ˜'}, inplace=True)
                final_df = pd.merge(final_df, order_count, on=['ìƒí’ˆID', 'ì˜µì…˜ì •ë³´'], how='left')
                final_df['ê²°ì œìˆ˜'] = final_df['ê²°ì œìˆ˜'].fillna(0)
                
                # í™˜ë¶ˆê±´ìˆ˜ (í™˜ë¶ˆ ìƒíƒœì¸ ì£¼ë¬¸ë²ˆí˜¸ ê°œìˆ˜)  
                cancel_orders = order_df[order_df['í´ë ˆì„ìƒíƒœ'].isin(config.CANCEL_OR_REFUND_STATUSES)]
                if not cancel_orders.empty:
                    refund_count = cancel_orders.groupby(['ìƒí’ˆID', 'ì˜µì…˜ì •ë³´'])['ìƒí’ˆì£¼ë¬¸ë²ˆí˜¸'].nunique().reset_index()
                    refund_count.rename(columns={'ìƒí’ˆì£¼ë¬¸ë²ˆí˜¸': 'í™˜ë¶ˆê±´ìˆ˜'}, inplace=True)
                    final_df = pd.merge(final_df, refund_count, on=['ìƒí’ˆID', 'ì˜µì…˜ì •ë³´'], how='left')
                    final_df['í™˜ë¶ˆê±´ìˆ˜'] = final_df['í™˜ë¶ˆê±´ìˆ˜'].fillna(0)
                else:
                    final_df['í™˜ë¶ˆê±´ìˆ˜'] = 0
            else:
                final_df['ê²°ì œìˆ˜'] = 0
                final_df['í™˜ë¶ˆê±´ìˆ˜'] = 0
                
            # ìµœì¢… ì»¬ëŸ¼ ì •ë¦¬
            final_columns = [col for col in config.COLUMNS_TO_KEEP if col in final_df.columns]
            sorted_df = final_df[final_columns].sort_values(by=['ìƒí’ˆëª…', 'ì˜µì…˜ì •ë³´'])
            
            # ë°ì´í„° ìš”ì•½ ë¡œê¹…
            logging.info(f"-> {store}({date}) ìµœì¢… ë°ì´í„° ìš”ì•½:")
            logging.info(f"   - ì´ ì˜µì…˜ ìˆ˜: {len(sorted_df)}")
            logging.info(f"   - ì´ íŒë§¤ìˆ˜ëŸ‰: {sorted_df['ìˆ˜ëŸ‰'].sum()}")
            logging.info(f"   - ì´ í™˜ë¶ˆìˆ˜ëŸ‰: {sorted_df['í™˜ë¶ˆìˆ˜ëŸ‰'].sum()}")
            logging.info(f"   - ì´ ë§¤ì¶œ: {sorted_df['ë§¤ì¶œ'].sum():,.0f}ì›")
            logging.info(f"   - ì´ íŒë§¤ë§ˆì§„: {sorted_df['íŒë§¤ë§ˆì§„'].sum():,.0f}ì›")
            
            # ì—‘ì…€ íŒŒì¼ ìƒì„±
            pivot_quantity = pd.pivot_table(sorted_df, index='ìƒí’ˆëª…', columns='ì˜µì…˜ì •ë³´', values='ìˆ˜ëŸ‰', aggfunc='sum', fill_value=0)
            pivot_margin = pd.pivot_table(sorted_df, index='ìƒí’ˆëª…', columns='ì˜µì…˜ì •ë³´', values='íŒë§¤ë§ˆì§„', aggfunc='sum', fill_value=0)
            
            with pd.ExcelWriter(output_path, engine='xlsxwriter') as writer:
                sorted_df.to_excel(writer, sheet_name='ì •ë¦¬ëœ ë°ì´í„°', index=False)
                pivot_quantity.to_excel(writer, sheet_name='ì˜µì…˜ë³„ íŒë§¤ìˆ˜ëŸ‰')
                pivot_margin.to_excel(writer, sheet_name='ì˜µì…˜ë³„ íŒë§¤ë§ˆì§„')
                
                # í‘œ ì„œì‹ ì ìš©
                worksheet = writer.sheets['ì •ë¦¬ëœ ë°ì´í„°']
                (max_row, max_col) = sorted_df.shape
                worksheet.add_table(0, 0, max_row, max_col - 1, {'columns': [{'header': col} for col in sorted_df.columns]})
                for i, col in enumerate(sorted_df.columns):
                    col_len = max(sorted_df[col].astype(str).map(len).max(), len(col)) + 2
                    worksheet.set_column(i, i, col_len)
            
            # ìƒì„± ì™„ë£Œ í™•ì¸
            if os.path.exists(output_path):
                file_size = os.path.getsize(output_path)
                logging.info(f"-> '{output_filename}' ìƒì„± ì™„ë£Œ: (íŒŒì¼ í¬ê¸°: {file_size:,} bytes)")
                processed_groups.append((store, date))
            else:
                logging.error(f"-> íŒŒì¼ ìƒì„± ì‹¤íŒ¨: {output_path}")
                
        except Exception as e:
            logging.error(f"-> {store}({date}) ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            logging.error(f"-> {store}({date}) ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
        finally:
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            try:
                if 'order_df' in locals():
                    del order_df
                if 'final_df' in locals():
                    del final_df
                if 'sorted_df' in locals():
                    del sorted_df
            except:
                pass
    
    logging.info("--- 1ë‹¨ê³„: ì£¼ë¬¸ì¡°íšŒ ê¸°ë°˜ ê°œë³„ í†µí•© ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ ---")
    logging.info(f"ğŸ¯ ===== GENERATE_INDIVIDUAL_REPORTS í•¨ìˆ˜ ì¢…ë£Œ: {len(processed_groups)}ê°œ ê·¸ë£¹ ì²˜ë¦¬ë¨ =====")
    logging.info(f"ğŸ“‹ ì²˜ë¦¬ëœ ê·¸ë£¹ë“¤: {processed_groups}")
    return processed_groups



def consolidate_daily_reports():
    """ë‚ ì§œë³„ë¡œ ìƒì„±ëœ ëª¨ë“  ê°œë³„ ë¦¬í¬íŠ¸ë¥¼ ì·¨í•©í•˜ì—¬ ì „ì²´ í†µí•© ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    logging.info("--- 2ë‹¨ê³„: ì „ì²´ í†µí•© ë¦¬í¬íŠ¸ ìƒì„± ì‹œì‘ ---")
    all_report_files = [f for f in glob.glob(os.path.join(config.get_processing_dir(), '*_í†µí•©_ë¦¬í¬íŠ¸_*.xlsx')) if not os.path.basename(f).startswith('~') and not os.path.basename(f).startswith('ì „ì²´_')]
    if not all_report_files:
        logging.info("ì·¨í•©í•  ê°œë³„ í†µí•© ë¦¬í¬íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    date_pattern = re.compile(r'_(\d{4}-\d{2}-\d{2})\.xlsx$')
    unique_dates = set()
    for f in all_report_files:
        match = date_pattern.search(os.path.basename(f))
        if match:
            unique_dates.add(match.group(1))
    
    if not unique_dates:
        logging.info("íŒŒì¼ì—ì„œ ë‚ ì§œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    logging.info(f"ì´ {len(sorted(list(unique_dates)))}ê°œì˜ ë‚ ì§œì— ëŒ€í•œ ì „ì²´ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤: {sorted(list(unique_dates))}")
    logging.info(f"ì²˜ë¦¬í•  ê°œë³„ ë¦¬í¬íŠ¸ íŒŒì¼ ìˆ˜: {len(all_report_files)}")
    for date in sorted(list(unique_dates)):
        logging.info(f"- {date} ë°ì´í„° í†µí•© ì¤‘...")
        output_file = os.path.join(config.get_processing_dir(), f'ì „ì²´_í†µí•©_ë¦¬í¬íŠ¸_{date}.xlsx')
        # ì •í™•í•œ ë‚ ì§œ ë§¤ì¹­ìœ¼ë¡œ íŒŒì¼ í•„í„°ë§ (ë¶€ë¶„ ë§¤ì¹­ ë°©ì§€)
        daily_files = [f for f in all_report_files if f'_í†µí•©_ë¦¬í¬íŠ¸_{date}.xlsx' in f]
        logging.info(f"-> {date} ë‚ ì§œì— ëŒ€í•œ ê°œë³„ íŒŒì¼ ìˆ˜: {len(daily_files)}")
        
        # ë””ë²„ê¹…: ì°¾ì€ íŒŒì¼ë“¤ ì¶œë ¥
        for file_path in daily_files:
            logging.info(f"-> ë°œê²¬ëœ íŒŒì¼: {os.path.basename(file_path)}")
        
        daily_dfs = []
        for file_path in daily_files:
            try:
                store_name = os.path.basename(file_path).split('_í†µí•©_ë¦¬í¬íŠ¸_')[0]
                df = pd.read_excel(file_path, sheet_name='ì •ë¦¬ëœ ë°ì´í„°', engine='openpyxl')
                df['ìŠ¤í† ì–´ëª…'] = store_name
                daily_dfs.append(df)
                logging.info(f"-> '{os.path.basename(file_path)}' í†µí•© ì™„ë£Œ: {len(df)}í–‰ ë°ì´í„° ì¶”ê°€")
            except Exception as e:
                logging.error(f"-> '{os.path.basename(file_path)}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        
        if daily_dfs:
            total_rows_before = sum(len(df) for df in daily_dfs)
            logging.info(f"-> {date} ë‚ ì§œ ë³‘í•© ì „ ì´ ë°ì´í„° í–‰ ìˆ˜: {total_rows_before}")
            
            master_df = pd.concat(daily_dfs, ignore_index=True)
            logging.info(f"-> {date} ë‚ ì§œ ë³‘í•© í›„ ë°ì´í„° í–‰ ìˆ˜: {len(master_df)}")
            
            # ë””ë²„ê¹…: ë³‘í•© í›„ ìŠ¤í† ì–´ë³„ ë°ì´í„° ê°œìˆ˜ í™•ì¸
            if 'ìŠ¤í† ì–´ëª…' in master_df.columns:
                store_counts = master_df['ìŠ¤í† ì–´ëª…'].value_counts()
                logging.info(f"-> ë³‘í•© í›„ ìŠ¤í† ì–´ë³„ ë°ì´í„° ê°œìˆ˜: {dict(store_counts)}")
            
            master_df = master_df[['ìŠ¤í† ì–´ëª…'] + [col for col in master_df.columns if col != 'ìŠ¤í† ì–´ëª…']]
            
            # ì§‘ê³„ í‚¤ì˜ NULL ê°’ ì²˜ë¦¬
            grouping_keys = ['ìŠ¤í† ì–´ëª…', 'ìƒí’ˆID', 'ìƒí’ˆëª…', 'ì˜µì…˜ì •ë³´']
            
            for key in grouping_keys:
                if key in master_df.columns:
                    before_null_count = master_df[key].isna().sum()
                    # ëª¨ë“  ì§‘ê³„ í‚¤ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ê³  NULLì„ ë¹ˆ ë¬¸ìì—´ë¡œ ëŒ€ì²´
                    master_df[key] = master_df[key].fillna('').astype(str)
                    after_empty_count = (master_df[key] == '').sum()
                    logging.info(f"-> '{key}' ì»¬ëŸ¼: NULL ê°’ {before_null_count}ê°œë¥¼ ë¹ˆ ë¬¸ìì—´ë¡œ ëŒ€ì²´ (ë¹ˆê°’ ì´ {after_empty_count}ê°œ)")
            
            # ì§‘ê³„ë¥¼ ìœ„í•œ ì„ì‹œ ì˜µì…˜ì •ë³´ ì»¬ëŸ¼ ìƒì„± (í‘œì‹œìš©)
            master_df['ì˜µì…˜ì •ë³´_í‘œì‹œ'] = master_df['ì˜µì…˜ì •ë³´'].apply(
                lambda x: 'ì˜µì…˜ì—†ìŒ' if x == '' else x
            )
            
            # ì§‘ê³„ í‚¤ë¥¼ í‘œì‹œìš© ì˜µì…˜ì •ë³´ë¡œ ë³€ê²½
            grouping_keys = ['ìŠ¤í† ì–´ëª…', 'ìƒí’ˆID', 'ìƒí’ˆëª…', 'ì˜µì…˜ì •ë³´_í‘œì‹œ']
            
            empty_option_count = (master_df['ì˜µì…˜ì •ë³´'] == '').sum()
            logging.info(f"-> ì§‘ê³„ìš© ì˜µì…˜ì •ë³´ ì²˜ë¦¬: ë¹ˆê°’ {empty_option_count}ê°œë¥¼ 'ì˜µì…˜ì—†ìŒ'ìœ¼ë¡œ í‘œì‹œ")
            
            # ë””ë²„ê¹…: NULL ê°’ ì²˜ë¦¬ í›„ ë°ì´í„° í™•ì¸
            before_nulls = len(master_df)
            master_df_clean = master_df.dropna(subset=grouping_keys)
            after_nulls = len(master_df_clean)
            if before_nulls != after_nulls:
                logging.warning(f"-> ì§‘ê³„ í‚¤ì— NULLì´ ë‚¨ì•„ìˆëŠ” {before_nulls - after_nulls}ê°œ í–‰ ë°œê²¬, ì œê±°ë¨")
                master_df = master_df_clean
            agg_methods = {
                'ìˆ˜ëŸ‰': 'sum', 'íŒë§¤ë§ˆì§„': 'sum', 'ê²°ì œìˆ˜': 'sum', 'ê²°ì œê¸ˆì•¡': 'sum',
                'í™˜ë¶ˆê±´ìˆ˜': 'sum', 'í™˜ë¶ˆê¸ˆì•¡': 'sum', 'í™˜ë¶ˆìˆ˜ëŸ‰': 'sum',
                'ê°€êµ¬ë§¤ ê°œìˆ˜': 'sum', 'íŒë§¤ê°€': 'mean', 'ë§ˆì§„ìœ¨': 'mean',
                'ê°€êµ¬ë§¤ ë¹„ìš©': 'sum', 'ìˆœë§¤ì¶œ': 'sum', 'ë§¤ì¶œ': 'sum', 'ê°€êµ¬ë§¤ ê¸ˆì•¡': 'sum',
                'ì´ìœ¤ìœ¨': 'mean', 'ê´‘ê³ ë¹„ìœ¨': 'mean', 'ìˆœì´ìµ': 'sum', 'ë¦¬ì›Œë“œ': 'sum'
            }
            actual_agg_methods = {k: v for k, v in agg_methods.items() if k in master_df.columns}
            logging.info(f"-> {date} ë‚ ì§œ ì§‘ê³„ ì „ ë°ì´í„° í–‰ ìˆ˜: {len(master_df)}, ì‚¬ìš© ê°€ëŠ¥í•œ ì§‘ê³„ ì»¬ëŸ¼: {list(actual_agg_methods.keys())}")
            
            # ë””ë²„ê¹…: NULL ê°’ ì²˜ë¦¬ í›„ ìŠ¤í† ì–´ë³„ ë°ì´í„° ê°œìˆ˜ í™•ì¸
            if 'ìŠ¤í† ì–´ëª…' in master_df.columns:
                clean_store_counts = master_df['ìŠ¤í† ì–´ëª…'].value_counts()
                logging.info(f"-> NULL ì²˜ë¦¬ í›„ ìŠ¤í† ì–´ë³„ ë°ì´í„° ê°œìˆ˜: {dict(clean_store_counts)}")
            
            aggregated_df = master_df.groupby(grouping_keys, as_index=False).agg(actual_agg_methods)
            logging.info(f"-> {date} ë‚ ì§œ ì§‘ê³„ í›„ ë°ì´í„° í–‰ ìˆ˜: {len(aggregated_df)}")
            
            # ë””ë²„ê¹…: ì§‘ê³„ í›„ ìŠ¤í† ì–´ë³„ ë°ì´í„° ê°œìˆ˜ í™•ì¸
            if 'ìŠ¤í† ì–´ëª…' in aggregated_df.columns:
                agg_store_counts = aggregated_df['ìŠ¤í† ì–´ëª…'].value_counts()
                logging.info(f"-> ì§‘ê³„ í›„ ìŠ¤í† ì–´ë³„ ë°ì´í„° ê°œìˆ˜: {dict(agg_store_counts)}")
            
            # ì˜µì…˜ì •ë³´_í‘œì‹œë¥¼ ì˜µì…˜ì •ë³´ë¡œ ë³€ê²½ (ìµœì¢… ë¦¬í¬íŠ¸ìš©)
            if 'ì˜µì…˜ì •ë³´_í‘œì‹œ' in aggregated_df.columns:
                aggregated_df['ì˜µì…˜ì •ë³´'] = aggregated_df['ì˜µì…˜ì •ë³´_í‘œì‹œ']
                aggregated_df = aggregated_df.drop(columns=['ì˜µì…˜ì •ë³´_í‘œì‹œ'])
                logging.info(f"-> ì§‘ê³„ í›„ ì˜µì…˜ì •ë³´ ì»¬ëŸ¼ ì •ë¦¬: 'ì˜µì…˜ì •ë³´_í‘œì‹œ' â†’ 'ì˜µì…˜ì •ë³´'")
            
            # í¼ì„¼íŠ¸ í•„ë“œë“¤ì„ ì†Œìˆ˜ì  ì²« ìë¦¬ê¹Œì§€ ë°˜ì˜¬ë¦¼
            for col in ['ë§ˆì§„ìœ¨', 'ê´‘ê³ ë¹„ìœ¨', 'ì´ìœ¤ìœ¨']:
                if col in aggregated_df.columns:
                    aggregated_df[col] = aggregated_df[col].round(1)
            
            final_columns = ['ìŠ¤í† ì–´ëª…'] + [col for col in config.COLUMNS_TO_KEEP if col in aggregated_df.columns]
            logging.info(f"-> {date} ë‚ ì§œ ìµœì¢… ì»¬ëŸ¼ ìˆ˜: {len(final_columns)}, ì»¬ëŸ¼: {final_columns[:10]}...")  # ì²˜ìŒ 10ê°œë§Œ
            
            aggregated_df = aggregated_df[final_columns]
            logging.info(f"-> {date} ë‚ ì§œ ìµœì¢… ë°ì´í„°: {len(aggregated_df)}í–‰")
            
            # ë””ë²„ê¹…: ìµœì¢… ì €ì¥ ì „ ìŠ¤í† ì–´ë³„ ë°ì´í„° ê°œìˆ˜ í™•ì¸
            if 'ìŠ¤í† ì–´ëª…' in aggregated_df.columns:
                final_store_counts = aggregated_df['ìŠ¤í† ì–´ëª…'].value_counts()
                logging.info(f"-> ìµœì¢… ì €ì¥ ì „ ìŠ¤í† ì–´ë³„ ë°ì´í„° ê°œìˆ˜: {dict(final_store_counts)}")
            try:
                with pd.ExcelWriter(output_file, engine='xlsxwriter') as writer:
                    aggregated_df.to_excel(writer, sheet_name='ì „ì²´ í†µí•© ë°ì´í„°', index=False)
                    worksheet = writer.sheets['ì „ì²´ í†µí•© ë°ì´í„°']
                    (max_row, max_col) = aggregated_df.shape
                    worksheet.add_table(0, 0, max_row, max_col - 1, {'columns': [{'header': col} for col in aggregated_df.columns]})
                    for i, col in enumerate(aggregated_df.columns):
                        col_len = max(aggregated_df[col].astype(str).map(len).max(), len(col)) + 2
                        worksheet.set_column(i, i, col_len)
                if os.path.exists(output_file):
                    file_size = os.path.getsize(output_file)
                    logging.info(f"-> '{os.path.basename(output_file)}' ìƒì„± ì™„ë£Œ: {output_file} (íŒŒì¼ í¬ê¸°: {file_size:,} bytes)")
                    
                    # ìƒì„±ëœ íŒŒì¼ ë‚´ìš© ê²€ì¦
                    try:
                        verify_df = pd.read_excel(output_file, sheet_name='ì „ì²´ í†µí•© ë°ì´í„°')
                        logging.info(f"-> ê²€ì¦: ì „ì²´ í†µí•© ë¦¬í¬íŠ¸ì— {len(verify_df)}í–‰ ë°ì´í„° ì €ì¥ë¨")
                    except Exception as verify_e:
                        logging.error(f"-> ì „ì²´ ë¦¬í¬íŠ¸ ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {verify_e}")
                else:
                    logging.error(f"-> ì „ì²´ ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {output_file} íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•ŠìŒ")
            except Exception as e:
                logging.error(f"-> ìµœì¢… íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
            finally:
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                try:
                    del master_df, aggregated_df
                except:
                    pass
        
        # daily_dfs ë©”ëª¨ë¦¬ ì •ë¦¬
        try:
            del daily_dfs
        except:
            pass
        else:
            logging.warning(f"-> {date} ë‚ ì§œì— ëŒ€í•œ ê°œë³„ ë¦¬í¬íŠ¸ê°€ ì—†ì–´ ì „ì²´ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
    logging.info("--- 2ë‹¨ê³„: ì „ì²´ í†µí•© ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ ---")