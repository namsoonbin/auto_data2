# -*- coding: utf-8 -*-
"""
Polars ê¸°ë°˜ ë¦¬í¬íŠ¸ ìƒì„± ëª¨ë“ˆ
ê¸°ì¡´ report_generator.pyì˜ Polars ìµœì í™” ë²„ì „
ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•´ pandas ëŒ€ì‹  polars ì‚¬ìš©
"""
import polars as pl
import pandas as pd  # í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€
import numpy as np
import os
import glob
import re
import logging
import io
import json
from datetime import datetime, timedelta
from . import config
from .polars_utils import (
    normalize_product_id_polars,
    normalize_option_info_polars,
    read_protected_excel_polars,
    safe_divide_polars,
    polars_groupby_agg,
    log_dataframe_info_polars,
    PolarsPerformanceMonitor
)
from .compatibility import DataFrameEngine, USE_POLARS


def create_purchase_dataframe_for_date(target_date: str) -> pl.DataFrame:
    """íŠ¹ì • ë‚ ì§œì˜ ê°€êµ¬ë§¤ ì„¤ì •ì„ Polars DataFrameìœ¼ë¡œ ë³€í™˜ - Context7 íƒ€ì… ì•ˆì „ì„± ê°•í™”"""
    # Context7 ëª¨ë²” ì‚¬ë¡€: ëª…í™•í•œ ìŠ¤í‚¤ë§ˆ ì •ì˜
    PURCHASE_SCHEMA = {
        'ìƒí’ˆID': pl.Utf8,
        'ì˜µì…˜ì •ë³´': pl.Utf8,
        'ê°€êµ¬ë§¤_ê°œìˆ˜': pl.Int32
    }

    try:
        purchase_file = os.path.join(config.BASE_DIR, 'ê°€êµ¬ë§¤ì„¤ì •.json')
        if not os.path.exists(purchase_file):
            return pl.DataFrame(schema=PURCHASE_SCHEMA)

        # íŒŒì¼ í¬ê¸° ê²€ì¦ (Context7 ëª¨ë²” ì‚¬ë¡€)
        if os.path.getsize(purchase_file) == 0:
            logging.warning(f"ë¹ˆ ê°€êµ¬ë§¤ ì„¤ì • íŒŒì¼: {purchase_file}")
            return pl.DataFrame(schema=PURCHASE_SCHEMA)

        with open(purchase_file, 'r', encoding='utf-8') as f:
            data = json.load(f)

        # ë°ì´í„° êµ¬ì¡° ê²€ì¦
        if not isinstance(data, dict) or 'purchases' not in data:
            logging.warning("ê°€êµ¬ë§¤ ì„¤ì • íŒŒì¼ êµ¬ì¡°ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤")
            return pl.DataFrame(schema=PURCHASE_SCHEMA)

        purchase_records = []
        for entry in data.get('purchases', []):
            try:
                # í•„ìˆ˜ í•„ë“œ ê²€ì¦
                if not all(key in entry for key in ['start_date', 'product_id', 'purchase_count']):
                    continue

                if entry.get('start_date') == target_date:
                    # íƒ€ì… ì•ˆì „ì„± í™•ë³´
                    product_id = normalize_product_id_polars(entry['product_id'])
                    option_info = str(entry.get('option_info', ''))
                    purchase_count = int(entry['purchase_count'])

                    if purchase_count < 0:  # ìŒìˆ˜ ê°€êµ¬ë§¤ ë°©ì§€
                        continue

                    purchase_records.append({
                        'ìƒí’ˆID': product_id,
                        'ì˜µì…˜ì •ë³´': option_info,
                        'ê°€êµ¬ë§¤_ê°œìˆ˜': purchase_count
                    })
            except (KeyError, ValueError, TypeError) as e:
                logging.debug(f"ê°€êµ¬ë§¤ ì—”íŠ¸ë¦¬ íŒŒì‹± ì‹¤íŒ¨: {entry}, ì˜¤ë¥˜: {e}")
                continue

        # Context7 ëª¨ë²” ì‚¬ë¡€: ìŠ¤í‚¤ë§ˆ ê°•ì œ ì ìš©
        if purchase_records:
            try:
                return pl.DataFrame(purchase_records, schema=PURCHASE_SCHEMA)
            except Exception as schema_error:
                logging.error(f"ê°€êµ¬ë§¤ DataFrame ìŠ¤í‚¤ë§ˆ ì ìš© ì‹¤íŒ¨: {schema_error}")
                return pl.DataFrame(schema=PURCHASE_SCHEMA)
        else:
            return pl.DataFrame(schema=PURCHASE_SCHEMA)

    except json.JSONDecodeError as e:
        logging.error(f"ê°€êµ¬ë§¤ ì„¤ì • JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
        return pl.DataFrame(schema=PURCHASE_SCHEMA)
    except Exception as e:
        logging.warning(f"ê°€êµ¬ë§¤ DataFrame ìƒì„± ì‹¤íŒ¨: {e}")
        return pl.DataFrame(schema=PURCHASE_SCHEMA)


def create_reward_dataframe_for_date(target_date: str) -> pl.DataFrame:
    """íŠ¹ì • ë‚ ì§œì˜ ë¦¬ì›Œë“œ ì„¤ì •ì„ Polars DataFrameìœ¼ë¡œ ë³€í™˜ - Context7 íƒ€ì… ì•ˆì „ì„± ê°•í™”"""
    # Context7 ëª¨ë²” ì‚¬ë¡€: ëª…í™•í•œ ìŠ¤í‚¤ë§ˆ ì •ì˜
    REWARD_SCHEMA = {
        'ìƒí’ˆID': pl.Utf8,
        'ì˜µì…˜ì •ë³´': pl.Utf8,
        'ë¦¬ì›Œë“œ_ê¸ˆì•¡': pl.Int32
    }

    try:
        reward_file = os.path.join(config.BASE_DIR, 'ë¦¬ì›Œë“œì„¤ì •.json')
        if not os.path.exists(reward_file):
            return pl.DataFrame(schema=REWARD_SCHEMA)

        # íŒŒì¼ í¬ê¸° ê²€ì¦
        if os.path.getsize(reward_file) == 0:
            logging.warning(f"ë¹ˆ ë¦¬ì›Œë“œ ì„¤ì • íŒŒì¼: {reward_file}")
            return pl.DataFrame(schema=REWARD_SCHEMA)

        with open(reward_file, 'r', encoding='utf-8') as f:
            data = json.load(f)

        # ë°ì´í„° êµ¬ì¡° ê²€ì¦
        if not isinstance(data, dict) or 'rewards' not in data:
            logging.warning("ë¦¬ì›Œë“œ ì„¤ì • íŒŒì¼ êµ¬ì¡°ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤")
            return pl.DataFrame(schema=REWARD_SCHEMA)

        # ëª©í‘œ ë‚ ì§œ íŒŒì‹± (íƒ€ì… ì•ˆì „ì„±)
        try:
            target_date_obj = datetime.strptime(target_date, '%Y-%m-%d').date()
        except ValueError as e:
            logging.error(f"ì˜ëª»ëœ ë‚ ì§œ í˜•ì‹: {target_date}, ì˜¤ë¥˜: {e}")
            return pl.DataFrame(schema=REWARD_SCHEMA)

        reward_records = []
        for entry in data.get('rewards', []):
            try:
                # í•„ìˆ˜ í•„ë“œ ê²€ì¦
                required_fields = ['start_date', 'end_date', 'product_id', 'reward']
                if not all(key in entry for key in required_fields):
                    continue

                # ë‚ ì§œ ë²”ìœ„ í™•ì¸ (íƒ€ì… ì•ˆì „ì„±)
                start_date = datetime.strptime(entry['start_date'], '%Y-%m-%d').date()
                end_date = datetime.strptime(entry['end_date'], '%Y-%m-%d').date()

                if start_date <= target_date_obj <= end_date:
                    # íƒ€ì… ì•ˆì „ì„± í™•ë³´
                    product_id = normalize_product_id_polars(entry['product_id'])
                    option_info = str(entry.get('option_info', ''))
                    reward_amount = int(entry['reward'])

                    if reward_amount < 0:  # ìŒìˆ˜ ë¦¬ì›Œë“œ ë°©ì§€
                        continue

                    reward_records.append({
                        'ìƒí’ˆID': product_id,
                        'ì˜µì…˜ì •ë³´': option_info,
                        'ë¦¬ì›Œë“œ_ê¸ˆì•¡': reward_amount
                    })
            except (KeyError, ValueError, TypeError) as e:
                logging.debug(f"ë¦¬ì›Œë“œ ì—”íŠ¸ë¦¬ íŒŒì‹± ì‹¤íŒ¨: {entry}, ì˜¤ë¥˜: {e}")
                continue

        # Context7 ëª¨ë²” ì‚¬ë¡€: ìŠ¤í‚¤ë§ˆ ê°•ì œ ì ìš©
        if reward_records:
            try:
                return pl.DataFrame(reward_records, schema=REWARD_SCHEMA)
            except Exception as schema_error:
                logging.error(f"ë¦¬ì›Œë“œ DataFrame ìŠ¤í‚¤ë§ˆ ì ìš© ì‹¤íŒ¨: {schema_error}")
                return pl.DataFrame(schema=REWARD_SCHEMA)
        else:
            return pl.DataFrame(schema=REWARD_SCHEMA)

    except json.JSONDecodeError as e:
        logging.error(f"ë¦¬ì›Œë“œ ì„¤ì • JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
        return pl.DataFrame(schema=REWARD_SCHEMA)
    except Exception as e:
        logging.warning(f"ë¦¬ì›Œë“œ DataFrame ìƒì„± ì‹¤íŒ¨: {e}")
        return pl.DataFrame(schema=REWARD_SCHEMA)


def normalize_product_id(value):
    """ìƒí’ˆIDë¥¼ ì •ê·œí™” - Polars ìµœì í™” ë²„ì „ìœ¼ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸"""
    return normalize_product_id_polars(value)


# ì „ì—­ ë¦¬ì›Œë“œ ìºì‹œ (ê¸°ì¡´ê³¼ ë™ì¼í•œ êµ¬ì¡° ìœ ì§€)
_reward_cache = None
_reward_cache_timestamp = None


def _migrate_legacy_rewards_polars(data):
    """ê¸°ì¡´ ë¦¬ì›Œë“œ ì„¤ì •ì„ ì˜µì…˜ë³„ ì„¤ì •ìœ¼ë¡œ ìë™ ë§ˆì´ê·¸ë ˆì´ì…˜ - Polars ë²„ì „"""
    try:
        # ë§ˆì§„ì •ë³´ íŒŒì¼ ë¡œë“œ (Polars ì‚¬ìš©)
        margin_file = config.MARGIN_FILE
        if not os.path.exists(margin_file):
            return data

        # Polarsë¡œ Excel ì½ê¸°
        margin_df = pl.read_excel(margin_file)

        # ì»¬ëŸ¼ëª… ì •ê·œí™”
        if 'ìƒí’ˆë²ˆí˜¸' in margin_df.columns:
            margin_df = margin_df.rename({'ìƒí’ˆë²ˆí˜¸': 'ìƒí’ˆID'})

        # ìƒí’ˆID ì •ê·œí™” (Polars ë°©ì‹)
        margin_df = margin_df.with_columns([
            pl.col('ìƒí’ˆID').map_elements(normalize_product_id_polars).alias('ìƒí’ˆID')
        ])

        # ë¹ˆ ìƒí’ˆID ì œê±°
        margin_df = margin_df.filter(pl.col('ìƒí’ˆID') != '')

        # ëŒ€í‘œì˜µì…˜ì„ booleanìœ¼ë¡œ ë³€í™˜ (Polars ë°©ì‹)
        if 'ëŒ€í‘œì˜µì…˜' in margin_df.columns:
            margin_df = margin_df.with_columns([
                pl.col('ëŒ€í‘œì˜µì…˜').cast(pl.Utf8).str.to_uppercase().is_in(['O', 'Y', 'TRUE']).alias('ëŒ€í‘œì˜µì…˜')
            ])

        migrated_rewards = []
        migration_count = 0

        for reward_entry in data.get('rewards', []):
            # ì´ë¯¸ option_infoê°€ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ìœ ì§€
            if 'option_info' in reward_entry:
                migrated_rewards.append(reward_entry)
                continue

            # ê¸°ì¡´ í˜•ì‹ì´ë©´ ë§ˆì´ê·¸ë ˆì´ì…˜ ìˆ˜í–‰
            product_id = normalize_product_id_polars(reward_entry.get('product_id', ''))
            if not product_id:
                continue

            # í•´ë‹¹ ìƒí’ˆì˜ ëª¨ë“  ëŒ€í‘œì˜µì…˜ ì°¾ê¸° (Polars ë°©ì‹)
            rep_options = margin_df.filter(
                (pl.col('ìƒí’ˆID') == product_id) &
                (pl.col('ëŒ€í‘œì˜µì…˜') == True)
            )

            if rep_options.height == 0:
                # ëŒ€í‘œì˜µì…˜ì´ ì—†ìœ¼ë©´ ë¹ˆ ì˜µì…˜ì •ë³´ë¡œ ì €ì¥
                new_entry = reward_entry.copy()
                new_entry['option_info'] = ''
                migrated_rewards.append(new_entry)
                migration_count += 1
            else:
                # ê° ëŒ€í‘œì˜µì…˜ë³„ë¡œ ê°œë³„ ì—”íŠ¸ë¦¬ ìƒì„±
                for row in rep_options.iter_rows(named=True):
                    new_entry = reward_entry.copy()
                    option_info = row.get('ì˜µì…˜ì •ë³´', '')
                    if option_info is None:
                        option_info = ''
                    new_entry['option_info'] = str(option_info)
                    migrated_rewards.append(new_entry)
                    migration_count += 1

        if migration_count > 0:
            logging.info(f"ë¦¬ì›Œë“œ ì„¤ì • ë§ˆì´ê·¸ë ˆì´ì…˜ (Polars): {migration_count}ê°œ ì—”íŠ¸ë¦¬ ìƒì„±")
            # ë§ˆì´ê·¸ë ˆì´ì…˜ëœ ë°ì´í„°ë¥¼ íŒŒì¼ì— ì €ì¥
            migrated_data = {'rewards': migrated_rewards}
            reward_file = os.path.join(config.BASE_DIR, 'ë¦¬ì›Œë“œì„¤ì •.json')
            with open(reward_file, 'w', encoding='utf-8') as f:
                json.dump(migrated_data, f, ensure_ascii=False, indent=2)
            return migrated_data

        return data

    except Exception as e:
        logging.warning(f"ë¦¬ì›Œë“œ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤íŒ¨ (Polars): {e}")
        return data


def _load_reward_cache():
    """ë¦¬ì›Œë“œ ì„¤ì •ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë¡œë“œí•˜ì—¬ ìºì‹œ (Polars ìµœì í™”)"""
    global _reward_cache, _reward_cache_timestamp

    try:
        reward_file = os.path.join(config.BASE_DIR, 'ë¦¬ì›Œë“œì„¤ì •.json')

        # íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not os.path.exists(reward_file):
            _reward_cache = {}
            return

        # íŒŒì¼ ìˆ˜ì • ì‹œê°„ í™•ì¸ (ìºì‹œ ë¬´íš¨í™”ìš©)
        file_timestamp = os.path.getmtime(reward_file)
        if _reward_cache is not None and _reward_cache_timestamp == file_timestamp:
            return  # ìºì‹œ ìœ íš¨í•¨

        # íŒŒì¼ í¬ê¸° í™•ì¸
        if os.path.getsize(reward_file) == 0:
            _reward_cache = {}
            return

        # JSON íŒŒì¼ ì½ê¸°
        with open(reward_file, 'r', encoding='utf-8') as f:
            data = json.load(f)

        # ë°ì´í„° êµ¬ì¡° ê²€ì¦
        if not isinstance(data, dict) or 'rewards' not in data:
            _reward_cache = {}
            return

        # Polars ê¸°ë°˜ ìë™ ë§ˆì´ê·¸ë ˆì´ì…˜ ìˆ˜í–‰
        data = _migrate_legacy_rewards_polars(data)

        # ë§ˆì´ê·¸ë ˆì´ì…˜ í›„ íŒŒì¼ íƒ€ì„ìŠ¤íƒ¬í”„ ë‹¤ì‹œ í™•ì¸
        new_file_timestamp = os.path.getmtime(reward_file)

        rewards_list = data.get('rewards', [])
        if not isinstance(rewards_list, list):
            _reward_cache = {}
            return

        # íš¨ìœ¨ì ì¸ ì¡°íšŒë¥¼ ìœ„í•œ ë”•ì…”ë„ˆë¦¬ ìƒì„± (ì˜µì…˜ í¬í•¨)
        reward_map = {}
        for reward_entry in rewards_list:
            try:
                # í•„ìˆ˜ í‚¤ ì¡´ì¬ í™•ì¸
                if not all(k in reward_entry for k in ['start_date', 'end_date', 'product_id', 'reward']):
                    continue

                start_date = datetime.strptime(reward_entry['start_date'], '%Y-%m-%d').date()
                end_date = datetime.strptime(reward_entry['end_date'], '%Y-%m-%d').date()
                product_id = normalize_product_id_polars(reward_entry['product_id'])
                option_info = reward_entry.get('option_info', '')
                reward_value = reward_entry['reward']

                # ë¦¬ì›Œë“œ ê°’ ê²€ì¦
                if not isinstance(reward_value, (int, float)) or reward_value < 0:
                    continue

                # ë‚ ì§œ ë²”ìœ„ë³„ë¡œ ë”•ì…”ë„ˆë¦¬ì— ì €ì¥ (ì„±ëŠ¥ ìµœì í™”)
                current_date = start_date
                while current_date <= end_date:
                    # 3-tuple í‚¤: (date, product_id, option_info)
                    key = (current_date.strftime('%Y-%m-%d'), product_id, option_info)
                    reward_map[key] = int(reward_value)
                    current_date += timedelta(days=1)

            except (ValueError, KeyError, TypeError) as e:
                # ê°œë³„ ì—”íŠ¸ë¦¬ íŒŒì‹± ì‹¤íŒ¨ëŠ” ë¡œê·¸ë§Œ ë‚¨ê¸°ê³  ê³„ì† ì§„í–‰
                logging.debug(f"ë¦¬ì›Œë“œ ì—”íŠ¸ë¦¬ íŒŒì‹± ì‹¤íŒ¨: {reward_entry}, ì˜¤ë¥˜: {e}")
                continue

        _reward_cache = reward_map
        _reward_cache_timestamp = new_file_timestamp

        logging.info(f"ë¦¬ì›Œë“œ ìºì‹œ ë¡œë“œ ì™„ë£Œ (Polars): {len(reward_map)}ê°œ ì—”íŠ¸ë¦¬")

    except FileNotFoundError:
        _reward_cache = {}
    except json.JSONDecodeError as e:
        logging.warning(f"ë¦¬ì›Œë“œ ì„¤ì • JSON íŒŒì¼ í˜•ì‹ ì˜¤ë¥˜ (Polars): {e}")
        _reward_cache = {}
    except Exception as e:
        logging.warning(f"ë¦¬ì›Œë“œ ìºì‹œ ë¡œë“œ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ (Polars): {e}")
        _reward_cache = {}


def get_reward_for_date_and_product(product_id, date_str, option_info=''):
    """íŠ¹ì • ë‚ ì§œì™€ ìƒí’ˆì˜ ë¦¬ì›Œë“œ ê¸ˆì•¡ ì¡°íšŒ (ì˜µì…˜ë³„) - Polars ìµœì í™”"""
    # ìºì‹œê°€ ì—†ìœ¼ë©´ ë¡œë“œ
    if _reward_cache is None:
        _load_reward_cache()

    # ìƒí’ˆID ì •ê·œí™”
    normalized_product_id = normalize_product_id_polars(product_id)

    # ë‚ ì§œ ë¬¸ìì—´ ê²€ì¦
    try:
        target_date = date_str
        if not isinstance(target_date, str):
            target_date = str(target_date)
    except:
        return 0

    # ì˜µì…˜ì •ë³´ ì •ê·œí™”
    normalized_option_info = normalize_option_info_polars(option_info)

    # O(1) ë”•ì…”ë„ˆë¦¬ ì¡°íšŒ
    key = (target_date, normalized_product_id, normalized_option_info)
    return _reward_cache.get(key, 0)


def get_purchase_count_for_date_and_product(product_id, date_str, option_info=''):
    """íŠ¹ì • ë‚ ì§œì™€ ìƒí’ˆì˜ ê°€êµ¬ë§¤ ê°œìˆ˜ ì¡°íšŒ (ì˜µì…˜ë³„) - Polars ìµœì í™”"""
    try:
        purchase_file = os.path.join(config.BASE_DIR, 'ê°€êµ¬ë§¤ì„¤ì •.json')

        if not os.path.exists(purchase_file):
            return 0

        # ìƒí’ˆIDì™€ ì˜µì…˜ì •ë³´ ì •ê·œí™”
        normalized_product_id = normalize_product_id_polars(product_id)
        normalized_option_info = normalize_option_info_polars(option_info)

        with open(purchase_file, 'r', encoding='utf-8') as f:
            purchase_data = json.load(f)

        # í•´ë‹¹ ë‚ ì§œì˜ ì„¤ì • ì°¾ê¸°
        for purchase_entry in purchase_data.get('purchases', []):
            try:
                if (purchase_entry.get('start_date') == date_str and
                    normalize_product_id_polars(purchase_entry.get('product_id', '')) == normalized_product_id and
                    normalize_option_info_polars(purchase_entry.get('option_info', '')) == normalized_option_info):

                    purchase_count = purchase_entry['purchase_count']
                    # ê°€êµ¬ë§¤ ê°œìˆ˜ê°€ ìˆ«ìì¸ì§€ í™•ì¸
                    if isinstance(purchase_count, (int, float)) and purchase_count >= 0:
                        return int(purchase_count)
            except (ValueError, KeyError, TypeError):
                continue

        return 0  # ì„¤ì •ì´ ì—†ìœ¼ë©´ 0

    except FileNotFoundError:
        return 0
    except json.JSONDecodeError as e:
        logging.warning(f"ê°€êµ¬ë§¤ ì„¤ì • JSON íŒŒì¼ í˜•ì‹ ì˜¤ë¥˜ (Polars): {e}")
        return 0
    except Exception as e:
        logging.warning(f"ê°€êµ¬ë§¤ ê°œìˆ˜ ì¡°íšŒ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ (Polars): {e}")
        return 0


def read_protected_excel(file_path, password=None, **kwargs):
    """ì•”í˜¸ë¡œ ë³´í˜¸ëœ Excel íŒŒì¼ì„ ì½ëŠ” í•¨ìˆ˜ - Polars ë²„ì „"""
    return read_protected_excel_polars(file_path, password, **kwargs)


def load_and_validate_margin_data_polars():
    """ë§ˆì§„ì •ë³´ íŒŒì¼ì„ Polarsë¡œ ë¡œë“œí•˜ê³  ê²€ì¦"""
    try:
        # Polarsë¡œ Excel ì½ê¸°
        margin_df = pl.read_excel(config.MARGIN_FILE)
        logging.info(f"'{os.path.basename(config.MARGIN_FILE)}' íŒŒì¼ì„ ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤ (Polars).")

        # í•„ìˆ˜ ì»¬ëŸ¼ ì¡´ì¬ í™•ì¸
        required_columns = ['ìƒí’ˆë²ˆí˜¸', 'ìƒí’ˆëª…', 'íŒë§¤ê°€', 'ë§ˆì§„ìœ¨']
        missing_columns = [col for col in required_columns if col not in margin_df.columns]
        if missing_columns:
            raise ValueError(f"ë§ˆì§„ì •ë³´ íŒŒì¼ì— í•„ìˆ˜ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {missing_columns}")

        # ì»¬ëŸ¼ëª… ì •ê·œí™”
        if 'ìƒí’ˆë²ˆí˜¸' in margin_df.columns:
            margin_df = margin_df.rename({'ìƒí’ˆë²ˆí˜¸': 'ìƒí’ˆID'})

        # ìƒí’ˆID ë°ì´í„° íƒ€ì… ì •ê·œí™” (Polars ë°©ì‹)
        margin_df = margin_df.with_columns([
            pl.col('ìƒí’ˆID').map_elements(normalize_product_id_polars).alias('ìƒí’ˆID')
        ])

        # ë¹ˆ ìƒí’ˆID ì œê±°
        original_count = margin_df.height
        margin_df = margin_df.filter(pl.col('ìƒí’ˆID') != '')
        filtered_count = margin_df.height

        if original_count != filtered_count:
            logging.warning(f"ë§ˆì§„ì •ë³´ì— ë¹ˆ ìƒí’ˆIDê°€ ìˆìŠµë‹ˆë‹¤. {original_count - filtered_count}ê°œ í–‰ ì œê±°ë¨.")

        # ë°ì´í„° íƒ€ì… ê²€ì¦ ë° ë³€í™˜ (Polars ë°©ì‹)
        margin_df = margin_df.with_columns([
            pl.col('íŒë§¤ê°€').cast(pl.Float64, strict=False),
            pl.col('ë§ˆì§„ìœ¨').cast(pl.Float64, strict=False)
        ])

        # ëŒ€í‘œì˜µì…˜ ì •ë³´ ì²˜ë¦¬
        if 'ëŒ€í‘œì˜µì…˜' in margin_df.columns:
            margin_df = margin_df.with_columns([
                pl.col('ëŒ€í‘œì˜µì…˜').cast(pl.Utf8).str.to_uppercase().is_in(['O', 'Y', 'TRUE']).alias('ëŒ€í‘œì˜µì…˜')
            ])

            # ëŒ€í‘œì˜µì…˜ íŒë§¤ê°€ ì •ë³´ ìƒì„± (pandas í˜¸í™˜ì„±ì„ ìœ„í•´)
            rep_options = margin_df.filter(pl.col('ëŒ€í‘œì˜µì…˜') == True)
            rep_price_map = dict(zip(
                rep_options.select(pl.col('ìƒí’ˆID')).to_series().to_list(),
                rep_options.select(pl.col('íŒë§¤ê°€')).to_series().to_list()
            ))
            logging.info("ëŒ€í‘œì˜µì…˜ íŒë§¤ê°€ ì •ë³´ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤ (Polars).")
        else:
            logging.warning(f"ê²½ê³ : '{os.path.basename(config.MARGIN_FILE)}'ì— 'ëŒ€í‘œì˜µì…˜' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            margin_df = margin_df.with_columns([pl.lit(False).alias('ëŒ€í‘œì˜µì…˜')])
            rep_price_map = {}

        # ì˜µì…˜ì •ë³´ ì •ê·œí™”
        if 'ì˜µì…˜ì •ë³´' not in margin_df.columns:
            margin_df = margin_df.with_columns([pl.lit('').alias('ì˜µì…˜ì •ë³´')])
        else:
            margin_df = margin_df.with_columns([
                pl.col('ì˜µì…˜ì •ë³´').map_elements(normalize_option_info_polars).alias('ì˜µì…˜ì •ë³´')
            ])

        return margin_df, rep_price_map

    except FileNotFoundError:
        logging.error(f"ë§ˆì§„ì •ë³´ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {config.MARGIN_FILE}")
        raise
    except PermissionError:
        logging.error(f"ë§ˆì§„ì •ë³´ íŒŒì¼ì— ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {config.MARGIN_FILE}")
        raise
    except ValueError as e:
        logging.error(f"ë§ˆì§„ì •ë³´ íŒŒì¼ ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨: {e}")
        raise
    except Exception as e:
        logging.error(f"ë§ˆì§„ì •ë³´ íŒŒì¼ ì½ê¸° ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
        raise


def process_order_file_polars(order_file, store, date, margin_df):
    """ê°œë³„ ì£¼ë¬¸ì¡°íšŒ íŒŒì¼ì„ Polarsë¡œ ì²˜ë¦¬"""
    monitor = PolarsPerformanceMonitor()
    monitor.start()

    try:
        # ì£¼ë¬¸ì¡°íšŒ íŒŒì¼ ì½ê¸° (ì•”í˜¸ ë³´í˜¸ë  ìˆ˜ ìˆìŒ)
        order_path = os.path.join(config.get_processing_dir(), order_file)
        order_df = read_protected_excel_polars(order_path, password=config.ORDER_FILE_PASSWORD)

        # íŒŒì¼ì´ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸
        if order_df.height == 0:
            logging.error(f"-> {store}({date}) ì£¼ë¬¸ì¡°íšŒ íŒŒì¼ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤: {order_file}")
            return None

        log_dataframe_info_polars(order_df, "ì£¼ë¬¸ì¡°íšŒ íŒŒì¼ ë¡œë“œ", store, date)

        # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸ ë° ì¶”ê°€
        if 'ìƒí’ˆID' not in order_df.columns:
            possible_id_cols = ['ìƒí’ˆë²ˆí˜¸', 'ìƒí’ˆì½”ë“œ', 'ProductID']
            id_col = None
            for col in possible_id_cols:
                if col in order_df.columns:
                    id_col = col
                    break

            if id_col:
                logging.info(f"-> {store}({date}) '{id_col}' ì»¬ëŸ¼ì„ ìƒí’ˆIDë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                order_df = order_df.rename({id_col: 'ìƒí’ˆID'})
            else:
                logging.error(f"-> {store}({date}) ìƒí’ˆID ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None

        # ë°ì´í„° ì •ê·œí™” ë° ì»¬ëŸ¼ ì¶”ê°€ (Context7 ë³‘ë ¬ Expression íŒ¨í„´)
        normalization_exprs = [
            # ìƒí’ˆID ì •ê·œí™” - ë²¡í„°í™”ëœ Expression ì‚¬ìš©
            pl.col('ìƒí’ˆID').cast(pl.Utf8).str.strip_chars()
              .str.replace_all(r"\.0+$", "", literal=False)
              .str.replace_all(r"^\\.+|\\.+$", "", literal=False)
              .fill_null("").alias('ìƒí’ˆID')
        ]

        # ì˜µì…˜ì •ë³´ ì²˜ë¦¬ - ì¡°ê±´ë¶€ Expression ì¶”ê°€
        if 'ì˜µì…˜ì •ë³´' not in order_df.columns:
            normalization_exprs.append(pl.lit('').alias('ì˜µì…˜ì •ë³´'))
        else:
            normalization_exprs.append(
                pl.col('ì˜µì…˜ì •ë³´').cast(pl.Utf8).str.strip_chars().fill_null("")
                  .str.replace_all("^ë‹¨ì¼$", "", literal=True)
                  .str.replace_all("^ê¸°ë³¸ì˜µì…˜$", "", literal=True)
                  .str.replace_all("^ì„ íƒì•ˆí•¨$", "", literal=True)
                  .str.replace_all("^null$", "", literal=True)
                  .str.replace_all("^none$", "", literal=True)
                  .str.replace_all("^ì—†ìŒ$", "", literal=True)
                  .str.replace_all("^nan$", "", literal=True)
                  .alias('ì˜µì…˜ì •ë³´')
            )

        # ëª¨ë“  ì •ê·œí™”ë¥¼ í•œ ë²ˆì— ë³‘ë ¬ ì‹¤í–‰ (Context7 ëª¨ë²” ì‚¬ë¡€)
        order_df = order_df.with_columns(normalization_exprs)

        # ìˆ˜ëŸ‰ ì»¬ëŸ¼ í™•ì¸ ë° ì„¤ì •
        if 'ìˆ˜ëŸ‰' not in order_df.columns:
            possible_quantity_cols = ['ê²°ì œìˆ˜ëŸ‰', 'ì£¼ë¬¸ìˆ˜ëŸ‰', 'ìƒí’ˆìˆ˜ëŸ‰', 'ê²°ì œìƒí’ˆìˆ˜ëŸ‰']
            quantity_col = None
            for col in possible_quantity_cols:
                if col in order_df.columns:
                    quantity_col = col
                    break

            if quantity_col:
                logging.info(f"-> {store}({date}) '{quantity_col}' ì»¬ëŸ¼ì„ ìˆ˜ëŸ‰ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                order_df = order_df.rename({quantity_col: 'ìˆ˜ëŸ‰'})
            else:
                logging.warning(f"-> {store}({date}) ìˆ˜ëŸ‰ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ 1 ì‚¬ìš©")
                order_df = order_df.with_columns([pl.lit(1).alias('ìˆ˜ëŸ‰')])

        # ì»¬ëŸ¼ ì²˜ë¦¬ ë° ê³„ì‚°ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰ (Context7 ëª¨ë²” ì‚¬ë¡€)
        column_processing_exprs = [
            # í´ë ˆì„ìƒíƒœ ì²˜ë¦¬
            pl.col('í´ë ˆì„ìƒíƒœ').fill_null('ì •ìƒ').alias('í´ë ˆì„ìƒíƒœ') if 'í´ë ˆì„ìƒíƒœ' in order_df.columns
            else pl.lit('ì •ìƒ').alias('í´ë ˆì„ìƒíƒœ'),

            # ìˆ˜ëŸ‰ì„ ìˆ«ìí˜•ìœ¼ë¡œ ë³€í™˜
            pl.col('ìˆ˜ëŸ‰').cast(pl.Float64, strict=False).fill_null(1).alias('ìˆ˜ëŸ‰'),

            # í™˜ë¶ˆìˆ˜ëŸ‰ ê³„ì‚° (í•œ ë²ˆì— ì²˜ë¦¬)
            pl.when(pl.col('í´ë ˆì„ìƒíƒœ').is_in(config.CANCEL_OR_REFUND_STATUSES))
              .then(pl.col('ìˆ˜ëŸ‰'))
              .otherwise(0)
              .alias('í™˜ë¶ˆìˆ˜ëŸ‰')
        ]

        # í´ë ˆì„ìƒíƒœê°€ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ê°’ ì²˜ë¦¬
        if 'í´ë ˆì„ìƒíƒœ' not in order_df.columns:
            logging.warning(f"-> {store}({date}) í´ë ˆì„ìƒíƒœ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ 'ì •ìƒ' ì‚¬ìš©")
            column_processing_exprs[0] = pl.lit('ì •ìƒ').alias('í´ë ˆì„ìƒíƒœ')

        order_df = order_df.with_columns(column_processing_exprs)

        # í´ë ˆì„ìƒíƒœ ë¶„í¬ í™•ì¸
        status_counts = order_df.select(pl.col('í´ë ˆì„ìƒíƒœ').value_counts()).to_pandas()
        logging.info(f"-> {store}({date}) í´ë ˆì„ìƒíƒœ ë¶„í¬: {status_counts}")

        # í™˜ë¶ˆìˆ˜ëŸ‰ ê³„ì‚° ê²°ê³¼ (ì´ë¯¸ ìœ„ì—ì„œ ê³„ì‚°ë¨)
        total_refund_quantity = order_df.select(pl.col('í™˜ë¶ˆìˆ˜ëŸ‰').sum()).item()
        refund_rows = order_df.filter(pl.col('í™˜ë¶ˆìˆ˜ëŸ‰') > 0).height
        logging.info(f"-> {store}({date}) ì´ í™˜ë¶ˆìˆ˜ëŸ‰: {total_refund_quantity}, í™˜ë¶ˆ í–‰ ìˆ˜: {refund_rows}")

        # ì„±ëŠ¥ ì¸¡ì • ê²°ê³¼
        perf_result = monitor.end()
        logging.info(f"-> {store}({date}) ì£¼ë¬¸ì¡°íšŒ íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ: "
                    f"{perf_result['execution_time']:.2f}ì´ˆ, {perf_result['memory_used_mb']:.1f}MB")

        return order_df

    except Exception as e:
        logging.error(f"-> {store}({date}) ì£¼ë¬¸ì¡°íšŒ íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        return None


def aggregate_options_polars(order_df, store, date):
    """ì˜µì…˜ë³„ ë°ì´í„° ì§‘ê³„ - Polars ìµœì í™”"""
    logging.info(f"-> {store}({date}) ì˜µì…˜ë³„ ë°ì´í„° ì§‘ê³„ ì‹œì‘ (Polars)...")

    # ìƒí’ˆëª… ì»¬ëŸ¼ í™•ì¸
    group_cols = ['ìƒí’ˆID', 'ì˜µì…˜ì •ë³´']
    if 'ìƒí’ˆëª…' in order_df.columns:
        group_cols = ['ìƒí’ˆID', 'ìƒí’ˆëª…', 'ì˜µì…˜ì •ë³´']
    else:
        logging.warning(f"-> {store}({date}) ì£¼ë¬¸ì¡°íšŒ íŒŒì¼ì— ìƒí’ˆëª… ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")

    # ì¤‘ë³µ ë°ì´í„° ê²€ì¦
    duplicates = order_df.select(
        pl.struct(group_cols).is_duplicated().sum().alias('duplicates')
    ).item()

    if duplicates > 0:
        logging.warning(f"-> {store}({date}) ì£¼ë¬¸ì¡°íšŒ ë°ì´í„°ì— ì¤‘ë³µëœ ìƒí’ˆID-ì˜µì…˜ì •ë³´ ì¡°í•©ì´ {duplicates}ê°œ ìˆìŠµë‹ˆë‹¤.")

    # ì˜µì…˜ë³„ ì§‘ê³„ (Polars ë°©ì‹)
    option_summary = order_df.group_by(group_cols).agg([
        pl.col('ìˆ˜ëŸ‰').sum().alias('ìˆ˜ëŸ‰'),
        pl.col('í™˜ë¶ˆìˆ˜ëŸ‰').sum().alias('í™˜ë¶ˆìˆ˜ëŸ‰')
    ])

    logging.info(f"-> {store}({date}) ì˜µì…˜ë³„ ì§‘ê³„ ì™„ë£Œ (Polars): {option_summary.height}ê°œ ì˜µì…˜")

    return option_summary


def generate_individual_reports_polars():
    """ê°œë³„ ìŠ¤í† ì–´ì˜ ì£¼ë¬¸ì¡°íšŒ íŒŒì¼ì„ ê¸°ë°˜ìœ¼ë¡œ ì˜µì…˜ë³„ í†µí•© ë¦¬í¬íŠ¸ë¥¼ ìƒì„± - Polars ë²„ì „"""
    logging.info("ğŸ¡ ===== GENERATE_INDIVIDUAL_REPORTS_POLARS í•¨ìˆ˜ í˜¸ì¶œë¨ =====")
    logging.info("--- 1ë‹¨ê³„: ì£¼ë¬¸ì¡°íšŒ ê¸°ë°˜ ê°œë³„ í†µí•© ë¦¬í¬íŠ¸ ìƒì„± ì‹œì‘ (Polars) ---")

    # ë§ˆì§„ì •ë³´ íŒŒì¼ ë¡œë“œ ë° ê²€ì¦
    try:
        margin_df, rep_price_map = load_and_validate_margin_data_polars()
    except Exception as e:
        logging.error(f"ë§ˆì§„ì •ë³´ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return []

    # ì²˜ë¦¬ ê°€ëŠ¥í•œ íŒŒì¼ë“¤ ì°¾ê¸°
    logging.info(f"ğŸ” ì‘ì—…í´ë” ìŠ¤ìº”: {config.get_processing_dir()}")
    all_files = [f for f in os.listdir(config.get_processing_dir()) if f.endswith('.xlsx') and not f.startswith('~')]
    logging.info(f"ğŸ“„ ì „ì²´ Excel íŒŒì¼ë“¤ ({len(all_files)}ê°œ): {all_files}")

    source_files = [f for f in all_files if 'í†µí•©_ë¦¬í¬íŠ¸' not in f and 'ë§ˆì§„ì •ë³´' not in f]
    logging.info(f"ğŸ“Š ì›ë³¸ íŒŒì¼ë“¤ ({len(source_files)}ê°œ): {source_files}")

    # ì£¼ë¬¸ì¡°íšŒ íŒŒì¼ë§Œ í•„í„°ë§
    order_files = [f for f in source_files if 'ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´_ì£¼ë¬¸ì¡°íšŒ' in f]
    logging.info(f"ğŸ› ì£¼ë¬¸ì¡°íšŒ íŒŒì¼ë“¤ ({len(order_files)}ê°œ): {order_files}")

    if not order_files:
        logging.warning("âš ï¸ ì²˜ë¦¬í•  ì£¼ë¬¸ì¡°íšŒ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤!")
        logging.info("ğŸ“‹ íŒŒì¼ëª… íŒ¨í„´ì„ í™•ì¸í•´ì£¼ì„¸ìš”: íŒŒì¼ëª…ì— 'ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´_ì£¼ë¬¸ì¡°íšŒ'ê°€ í¬í•¨ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.")
        return []

    logging.info(f"ì´ {len(order_files)}ê°œì˜ ì£¼ë¬¸ì¡°íšŒ íŒŒì¼ì— ëŒ€í•œ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. (Polars ì—”ì§„)")
    processed_groups = []

    for order_file in order_files:
        # íŒŒì¼ëª…ì—ì„œ ìŠ¤í† ì–´ëª…ê³¼ ë‚ ì§œ ì¶”ì¶œ
        if 'ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´_ì£¼ë¬¸ì¡°íšŒ' in order_file:
            parts = order_file.split(' ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´_ì£¼ë¬¸ì¡°íšŒ_')
            if len(parts) == 2:
                store = parts[0]
                date = parts[1].replace('.xlsx', '')
            else:
                continue
        else:
            continue

        output_filename = f'{store}_í†µí•©_ë¦¬í¬íŠ¸_{date}.xlsx'
        output_path = os.path.join(config.get_processing_dir(), output_filename)

        # ì´ë¯¸ ë¦¬í¬íŠ¸ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        if os.path.exists(output_path):
            logging.info(f"- {store} ({date}) ì´ë¯¸ ë¦¬í¬íŠ¸ê°€ ìƒì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            processed_groups.append((store, date))
            continue

        logging.info(f"- {store} ({date}) ì£¼ë¬¸ì¡°íšŒ ê¸°ë°˜ ë°ì´í„° ì²˜ë¦¬ ì‹œì‘ (Polars)...")

        try:
            # ì£¼ë¬¸ì¡°íšŒ íŒŒì¼ ì²˜ë¦¬
            order_df = process_order_file_polars(order_file, store, date, margin_df)
            if order_df is None:
                continue

            # ì˜µì…˜ë³„ ì§‘ê³„
            option_summary = aggregate_options_polars(order_df, store, date)

            # ë¦¬ì›Œë“œ ëˆ„ë½ ìƒí’ˆ ì¶”ê°€ ë¡œì§
            option_summary = add_missing_reward_products_polars(option_summary, store, date, margin_df)

            # ë§ˆì§„ì •ë³´ì™€ ë³‘í•©
            final_df = merge_with_margin_data_polars(option_summary, margin_df, store, date)

            # ê³„ì‚° í•„ë“œ ì¶”ê°€
            final_df = add_calculated_fields_polars(final_df, store, date, rep_price_map)

            # ì£¼ë¬¸ì¡°íšŒ ê¸°ë°˜ ì¶”ê°€ ì§€í‘œ ê³„ì‚°
            final_df = add_order_based_metrics_polars(final_df, order_df, store, date)

            # ìµœì¢… ë°ì´í„° ì •ë¦¬ ë° ì €ì¥
            success = save_report_polars(final_df, output_path, store, date)

            if success:
                processed_groups.append((store, date))

        except Exception as e:
            logging.error(f"-> {store}({date}) ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ (Polars): {e}")
            continue

    logging.info(f"âœ… ê°œë³„ ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ (Polars): {len(processed_groups)}ê°œ ê·¸ë£¹ ì²˜ë¦¬ë¨")
    return processed_groups


def add_missing_reward_products_polars(option_summary, store, date, margin_df):
    """ë¦¬ì›Œë“œ ì„¤ì •ëœ ìƒí’ˆ ì¤‘ ëˆ„ë½ëœ ê²ƒë“¤ì„ 0 ë°ì´í„°ë¡œ ì¶”ê°€ - Polars ë²„ì „"""
    logging.info(f"-> {store}({date}) ë¦¬ì›Œë“œ ì„¤ì •ëœ ìƒí’ˆ ì¤‘ ëˆ„ë½ëœ ìƒí’ˆ ì²´í¬ (Polars)...")

    try:
        reward_file = os.path.join(config.BASE_DIR, 'ë¦¬ì›Œë“œì„¤ì •.json')

        if not os.path.exists(reward_file):
            logging.info(f"-> {store}({date}) ë¦¬ì›Œë“œ ì„¤ì • íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return option_summary

        with open(reward_file, 'r', encoding='utf-8') as f:
            reward_data = json.load(f)

        # í•´ë‹¹ ë‚ ì§œì— ë¦¬ì›Œë“œê°€ ì„¤ì •ëœ ìƒí’ˆë“¤ ì°¾ê¸°
        rewarded_products = set()
        for reward_entry in reward_data.get('rewards', []):
            start_date = reward_entry.get('start_date', '')
            end_date = reward_entry.get('end_date', '')
            product_id = str(reward_entry.get('product_id', ''))
            reward_amount = reward_entry.get('reward', 0)

            if (start_date <= date <= end_date and
                product_id and product_id != 'nan' and product_id != '' and
                reward_amount and reward_amount > 0):
                normalized_id = normalize_product_id_polars(product_id)
                rewarded_products.add(normalized_id)

        logging.info(f"-> {store}({date}) ë¦¬ì›Œë“œ ì„¤ì •ëœ ìƒí’ˆë“¤: {list(rewarded_products)}")

        # ìŠ¤í† ì–´ë³„ í•„í„°ë§
        if 'ìŠ¤í† ì–´' in margin_df.columns:
            store_products = set(
                margin_df.filter(pl.col('ìŠ¤í† ì–´') == store)
                         .select(pl.col('ìƒí’ˆID'))
                         .to_series()
                         .to_list()
            )
            store_rewarded_products = rewarded_products & store_products
        else:
            store_rewarded_products = rewarded_products

        # ê¸°ì¡´ ìƒí’ˆë“¤ (Polarsì—ì„œ ì§‘í•©ìœ¼ë¡œ ë³€í™˜)
        existing_products = set(
            option_summary.select(pl.col('ìƒí’ˆID'))
                          .to_series()
                          .to_list()
        )

        missing_rewarded_products = store_rewarded_products - existing_products

        if missing_rewarded_products:
            logging.info(f"-> {store}({date}) ì£¼ë¬¸ì¡°íšŒì— ì—†ëŠ” ë¦¬ì›Œë“œ ì„¤ì • ìƒí’ˆ {len(missing_rewarded_products)}ê°œ: {list(missing_rewarded_products)}")

            # ëˆ„ë½ëœ ìƒí’ˆë“¤ì„ 0 ë°ì´í„°ë¡œ ì¶”ê°€
            missing_rows = []
            for normalized_product_id in missing_rewarded_products:
                # ë§ˆì§„ì •ë³´ì—ì„œ ëŒ€í‘œì˜µì…˜ ì°¾ê¸°
                product_margin = margin_df.filter(
                    (pl.col('ìƒí’ˆID') == normalized_product_id) &
                    (pl.col('ëŒ€í‘œì˜µì…˜') == True)
                )

                if 'ìŠ¤í† ì–´' in margin_df.columns:
                    product_margin = product_margin.filter(pl.col('ìŠ¤í† ì–´') == store)

                if product_margin.height > 0:
                    product_info = product_margin.row(0, named=True)

                    # 0 ë°ì´í„° í–‰ ìƒì„±
                    zero_row = {
                        'ìƒí’ˆID': normalized_product_id,
                        'ì˜µì…˜ì •ë³´': product_info.get('ì˜µì…˜ì •ë³´', ''),
                        'ìˆ˜ëŸ‰': 0,
                        'í™˜ë¶ˆìˆ˜ëŸ‰': 0
                    }

                    # ìƒí’ˆëª… ì¶”ê°€
                    if 'ìƒí’ˆëª…' in product_info:
                        zero_row['ìƒí’ˆëª…'] = product_info['ìƒí’ˆëª…']
                    else:
                        zero_row['ìƒí’ˆëª…'] = f'ìƒí’ˆ{normalized_product_id}'

                    missing_rows.append(zero_row)
                    logging.info(f"-> {store}({date}) 0 ë°ì´í„° ì¶”ê°€: ìƒí’ˆ {normalized_product_id} (ìƒí’ˆë‹¨: {zero_row['ìƒí’ˆëª…']})")

            # ëˆ„ë½ëœ ìƒí’ˆë“¤ì„ option_summaryì— ì¶”ê°€
            if missing_rows:
                missing_df = pl.DataFrame(missing_rows)
                option_summary = pl.concat([option_summary, missing_df], how="diagonal")
                logging.info(f"-> {store}({date}) {len(missing_rows)}ê°œ ëˆ„ë½ ìƒí’ˆ ì¶”ê°€ ì™„ë£Œ")
        else:
            logging.info(f"-> {store}({date}) ëª¨ë“  ë¦¬ì›Œë“œ ì„¤ì • ìƒí’ˆì´ ì£¼ë¬¸ì¡°íšŒì— ì¡´ì¬í•©ë‹ˆë‹¤.")

        return option_summary

    except Exception as e:
        logging.warning(f"-> {store}({date}) ë¦¬ì›Œë“œ ëˆ„ë½ ìƒí’ˆ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return option_summary


def merge_with_margin_data_polars(option_summary, margin_df, store, date):
    """ì˜µì…˜ ìš”ì•½ê³¼ ë§ˆì§„ì •ë³´ë¥¼ ë³‘í•© - Polars ìµœì í™”"""
    logging.info(f"-> {store}({date}) ë§ˆì§„ì •ë³´ì™€ ë³‘í•© ì‹œì‘ (Polars)...")

    # 1ì°¨: ì •í™•í•œ ë§¤ì¹­ (ìƒí’ˆID + ì˜µì…˜ì •ë³´)
    exact_match = option_summary.join(
        margin_df,
        on=['ìƒí’ˆID', 'ì˜µì…˜ì •ë³´'],
        how='inner'
    )

    logging.info(f"-> {store}({date}) ì •í™•í•œ ë§¤ì¹­: {exact_match.height}/{option_summary.height} ({exact_match.height/option_summary.height*100:.1f}%)")

    # ë§¤ì¹­ë¥ ì´ ë‚®ìœ¼ë©´ ëŒ€ì•ˆ ë§¤ì¹­ ì‹œë„
    if exact_match.height < option_summary.height * 0.8:  # 80% ë¯¸ë§Œ ë§¤ì¹­ ì‹œ
        logging.warning(f"-> {store}({date}) ì •í™•í•œ ë§¤ì¹­ ë¹„ìœ¨ì´ ë‚®ì•„ ëŒ€ì•ˆ ë§¤ì¹­ ì‹œë„")

        # ìƒí’ˆIDë§Œìœ¼ë¡œ ë§¤ì¹­ (ì˜µì…˜ ë¬´ì‹œ)
        margin_id_only = margin_df.group_by('ìƒí’ˆID').first()
        fallback_match = option_summary.join(
            margin_id_only,
            on='ìƒí’ˆID',
            how='left'
        )

        logging.info(f"-> {store}({date}) ëŒ€ì•ˆ ë§¤ì¹­ (ìƒí’ˆIDë§Œ): {fallback_match.height - fallback_match.filter(pl.col('íŒë§¤ê°€').is_null()).height}/{option_summary.height}")

        final_df = fallback_match
    else:
        # ì •í™•í•œ ë§¤ì¹­ + ë§¤ì¹­ ì‹¤íŒ¨ ë°ì´í„° ì²˜ë¦¬
        unmatched = option_summary.join(
            margin_df.select(['ìƒí’ˆID', 'ì˜µì…˜ì •ë³´']),
            on=['ìƒí’ˆID', 'ì˜µì…˜ì •ë³´'],
            how='anti'
        )

        if unmatched.height > 0:
            logging.warning(f"-> {store}({date}) ë§¤ì¹­ ì‹¤íŒ¨ ë°ì´í„° {unmatched.height}ê°œë¥¼ ìƒí’ˆIDë¡œë§Œ ë§¤ì¹­ ì‹œë„")

            margin_id_only = margin_df.group_by('ìƒí’ˆID').first()
            unmatched_fixed = unmatched.join(
                margin_id_only,
                on='ìƒí’ˆID',
                how='left'
            )

            final_df = pl.concat([exact_match, unmatched_fixed], how="diagonal")
        else:
            final_df = exact_match

    # ë§¤ì¹­ ê²°ê³¼ í™•ì¸
    matched_count = final_df.filter(pl.col('íŒë§¤ê°€').is_not_null()).height
    logging.info(f"-> {store}({date}) ìµœì¢… ë§¤ì¹­ ê²°ê³¼: {matched_count}/{option_summary.height} ({matched_count/option_summary.height*100:.1f}%)")

    return final_df


def add_calculated_fields_polars(final_df, store, date, rep_price_map):
    """ê³„ì‚° í•„ë“œ ì¶”ê°€ - Polars ìµœì í™”"""
    logging.info(f"-> {store}({date}) ê³„ì‚° í•„ë“œ ì¶”ê°€ ì‹œì‘ (Polars)...")

    # ìƒí’ˆëª… ì²˜ë¦¬
    if 'ìƒí’ˆëª…' not in final_df.columns:
        logging.error(f"-> {store}({date}) ìƒí’ˆëª… ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
        final_df = final_df.with_columns([pl.col('ìƒí’ˆID').alias('ìƒí’ˆëª…')])
        logging.warning(f"-> {store}({date}) ì„ì‹œë¡œ ìƒí’ˆIDë¥¼ ìƒí’ˆëª…ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.")

    # ê¸°ë³¸ ê³„ì‚° í•„ë“œë“¤ì„ ë³‘ë ¬ë¡œ ì²˜ë¦¬ (Context7 ëª¨ë²” ì‚¬ë¡€)
    basic_calculation_exprs = [
        (pl.col('ìˆ˜ëŸ‰') * pl.col('íŒë§¤ê°€')).alias('ê²°ì œê¸ˆì•¡'),
        (pl.col('í™˜ë¶ˆìˆ˜ëŸ‰') * pl.col('íŒë§¤ê°€')).alias('í™˜ë¶ˆê¸ˆì•¡'),
    ]

    final_df = final_df.with_columns(basic_calculation_exprs)

    # ë§¤ì¶œ ê³„ì‚° (í™˜ë¶ˆê¸ˆì•¡ ì˜¤íƒ€ ìˆ˜ì •)
    final_df = final_df.with_columns([
        (pl.col('ê²°ì œê¸ˆì•¡') - pl.col('í™˜ë¶ˆê¸ˆì•¡')).alias('ë§¤ì¶œ')
    ])

    # ëŒ€í‘œíŒë§¤ê°€ ë§¤í•‘ (rep_price_map ì‚¬ìš©)
    if rep_price_map:
        # Polarsì—ì„œ map_dict ì‚¬ìš©
        final_df = final_df.with_columns([
            pl.col('ìƒí’ˆID').map_dict(rep_price_map, default=0.0).alias('ëŒ€í‘œíŒë§¤ê°€')
        ])
    else:
        final_df = final_df.with_columns([pl.lit(0.0).alias('ëŒ€í‘œíŒë§¤ê°€')])

    # ê°€êµ¬ë§¤ ê°œìˆ˜ ì ìš© - Context7 join íŒ¨í„´ìœ¼ë¡œ ì™„ì „ ë²¡í„°í™”
    logging.info(f"-> {store}({date}) ê°€êµ¬ë§¤ ê°œìˆ˜ ì ìš© ì‹œì‘ (ë²¡í„°í™”)...")

    # ê°€êµ¬ë§¤ ì„¤ì •ì„ DataFrameìœ¼ë¡œ ë³€í™˜
    purchase_df = create_purchase_dataframe_for_date(date)

    if purchase_df.height > 0:
        # joinìœ¼ë¡œ ê°€êµ¬ë§¤ ê°œìˆ˜ ë§¤í•‘ (Context7 ëª¨ë²” ì‚¬ë¡€)
        final_df = final_df.join(
            purchase_df,
            on=['ìƒí’ˆID', 'ì˜µì…˜ì •ë³´'],
            how='left'
        ).with_columns([
            # ëŒ€í‘œì˜µì…˜ì´ë©´ì„œ ê°€êµ¬ë§¤ ì„¤ì •ì´ ìˆëŠ” ê²½ìš°ì—ë§Œ ì ìš©
            pl.when(
                (pl.col('ëŒ€í‘œì˜µì…˜') == True) &
                (pl.col('ê°€êµ¬ë§¤_ê°œìˆ˜').is_not_null())
            )
            .then(pl.col('ê°€êµ¬ë§¤_ê°œìˆ˜'))
            .otherwise(0)
            .alias('ê°€êµ¬ë§¤ ê°œìˆ˜')
        ]).drop('ê°€êµ¬ë§¤_ê°œìˆ˜')  # ì„ì‹œ ì»¬ëŸ¼ ì œê±°
    else:
        # ê°€êµ¬ë§¤ ì„¤ì •ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ 0
        final_df = final_df.with_columns([pl.lit(0).alias('ê°€êµ¬ë§¤ ê°œìˆ˜')])

    logging.info(f"-> {store}({date}) ê°€êµ¬ë§¤ ê°œìˆ˜ ì ìš© ì™„ë£Œ")

    # ëª¨ë“  ê³„ì‚° í•„ë“œë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬ (Context7 ëª¨ë²” ì‚¬ë¡€)
    calculation_exprs = [
        # ê¸°ë³¸ ê³„ì‚°
        (pl.col('ìˆ˜ëŸ‰') - pl.col('í™˜ë¶ˆìˆ˜ëŸ‰') - pl.col('ê°€êµ¬ë§¤ ê°œìˆ˜')).alias('ì‹¤íŒë§¤ê°œìˆ˜'),
        pl.col('ê°€êµ¬ë§¤ ê°œìˆ˜').alias('ê°€êµ¬ë§¤ ìˆ˜ëŸ‰'),
        pl.col('ëŒ€í‘œíŒë§¤ê°€').alias('ê°œë‹¹ ê°€êµ¬ë§¤ ê¸ˆì•¡'),
    ]

    final_df = final_df.with_columns(calculation_exprs)

    # ì˜ì¡´ì„±ì´ ìˆëŠ” ê³„ì‚°ë“¤ì„ ë‘ ë²ˆì§¸ ë‹¨ê³„ë¡œ ì²˜ë¦¬
    dependent_calculation_exprs = [
        (pl.col('ê°œë‹¹ ê°€êµ¬ë§¤ ê¸ˆì•¡') * pl.col('ê°€êµ¬ë§¤ ìˆ˜ëŸ‰')).alias('ê°€êµ¬ë§¤ ê¸ˆì•¡'),
    ]

    final_df = final_df.with_columns(dependent_calculation_exprs)

    # ìµœì¢… ê³„ì‚°ë“¤
    final_calculation_exprs = [
        (pl.col('ë§¤ì¶œ') - pl.col('ê°€êµ¬ë§¤ ê¸ˆì•¡')).alias('ìˆœë§¤ì¶œ'),
        (pl.col('ê°œë‹¹ ê°€êµ¬ë§¤ ë¹„ìš©') * pl.col('ê°€êµ¬ë§¤ ìˆ˜ëŸ‰')).alias('ê°€êµ¬ë§¤ ë¹„ìš©')
    ]

    final_df = final_df.with_columns(final_calculation_exprs)

    # ë¦¬ì›Œë“œ ì ìš© - Context7 join íŒ¨í„´ìœ¼ë¡œ ì™„ì „ ë²¡í„°í™”
    logging.info(f"-> {store}({date}) ë¦¬ì›Œë“œ ì ìš© ì‹œì‘ (ë²¡í„°í™”)...")

    # ë¦¬ì›Œë“œ ì„¤ì •ì„ DataFrameìœ¼ë¡œ ë³€í™˜
    reward_df = create_reward_dataframe_for_date(date)

    if reward_df.height > 0:
        # joinìœ¼ë¡œ ë¦¬ì›Œë“œ ë§¤í•‘ (Context7 ëª¨ë²” ì‚¬ë¡€)
        final_df = final_df.join(
            reward_df,
            on=['ìƒí’ˆID', 'ì˜µì…˜ì •ë³´'],
            how='left'
        ).with_columns([
            # ëŒ€í‘œì˜µì…˜ì´ë©´ì„œ ë¦¬ì›Œë“œ ì„¤ì •ì´ ìˆëŠ” ê²½ìš°ì—ë§Œ ì ìš©
            pl.when(
                (pl.col('ëŒ€í‘œì˜µì…˜') == True) &
                (pl.col('ë¦¬ì›Œë“œ_ê¸ˆì•¡').is_not_null())
            )
            .then(pl.col('ë¦¬ì›Œë“œ_ê¸ˆì•¡'))
            .otherwise(0)
            .alias('ë¦¬ì›Œë“œ')
        ]).drop('ë¦¬ì›Œë“œ_ê¸ˆì•¡')  # ì„ì‹œ ì»¬ëŸ¼ ì œê±°
    else:
        # ë¦¬ì›Œë“œ ì„¤ì •ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ 0
        final_df = final_df.with_columns([pl.lit(0).alias('ë¦¬ì›Œë“œ')])

    logging.info(f"-> {store}({date}) ë¦¬ì›Œë“œ ì ìš© ì™„ë£Œ")

    # ëª¨ë“  ë¹„ìœ¨ ê³„ì‚°ì„ ë³‘ë ¬ë¡œ ì²˜ë¦¬ (Context7 ëª¨ë²” ì‚¬ë¡€)
    ratio_calculation_exprs = [
        # ê¸°ë³¸ ê³„ì‚°
        (pl.col('ìˆœë§¤ì¶œ') * pl.col('ë§ˆì§„ìœ¨')).alias('íŒë§¤ë§ˆì§„'),
        # ê´‘ê³ ë¹„ìœ¨ = (ë¦¬ì›Œë“œ + ê°€êµ¬ë§¤ ë¹„ìš©) / ìˆœë§¤ì¶œ
        safe_divide_polars(
            pl.col('ë¦¬ì›Œë“œ') + pl.col('ê°€êµ¬ë§¤ ë¹„ìš©'),
            pl.col('ìˆœë§¤ì¶œ'),
            0.0
        ).alias('ê´‘ê³ ë¹„ìœ¨')
    ]

    final_df = final_df.with_columns(ratio_calculation_exprs)

    # ì˜ì¡´ì„± ìˆëŠ” ê³„ì‚°ê³¼ í¼ì„¼íŠ¸ ë³€í™˜ì„ ë³‘ë ¬ë¡œ ì²˜ë¦¬
    final_ratio_exprs = [
        # ì´ìœ¤ìœ¨ê³¼ ìˆœì´ìµ ê³„ì‚°
        (pl.col('ë§ˆì§„ìœ¨') - pl.col('ê´‘ê³ ë¹„ìœ¨')).alias('ì´ìœ¤ìœ¨'),
        (pl.col('íŒë§¤ë§ˆì§„') - pl.col('ê°€êµ¬ë§¤ ë¹„ìš©') - pl.col('ë¦¬ì›Œë“œ')).alias('ìˆœì´ìµ'),
        # í¼ì„¼íŠ¸ ë³€í™˜ (ë™ì‹œ ì²˜ë¦¬)
        (pl.col('ë§ˆì§„ìœ¨') * 100).round(1).alias('ë§ˆì§„ìœ¨_percent'),
        (pl.col('ê´‘ê³ ë¹„ìœ¨') * 100).round(1).alias('ê´‘ê³ ë¹„ìœ¨_percent'),
    ]

    final_df = final_df.with_columns(final_ratio_exprs)

    # ì´ìœ¤ìœ¨ í¼ì„¼íŠ¸ ë³€í™˜ (ì˜ì¡´ì„± ë•Œë¬¸ì— ë³„ë„ ì²˜ë¦¬)
    final_df = final_df.with_columns([
        (pl.col('ì´ìœ¤ìœ¨') * 100).round(1).alias('ì´ìœ¤ìœ¨_percent')
    ])

    # ì›ë˜ ì»¬ëŸ¼ëª…ìœ¼ë¡œ ë³µì›
    final_df = final_df.with_columns([
        pl.col('ë§ˆì§„ìœ¨_percent').alias('ë§ˆì§„ìœ¨'),
        pl.col('ê´‘ê³ ë¹„ìœ¨_percent').alias('ê´‘ê³ ë¹„ìœ¨'),
        pl.col('ì´ìœ¤ìœ¨_percent').alias('ì´ìœ¤ìœ¨')
    ]).drop(['ë§ˆì§„ìœ¨_percent', 'ê´‘ê³ ë¹„ìœ¨_percent', 'ì´ìœ¤ìœ¨_percent'])

    logging.info(f"-> {store}({date}) ê³„ì‚° í•„ë“œ ì¶”ê°€ ì™„ë£Œ (Polars)")
    return final_df


def add_order_based_metrics_polars(final_df, order_df, store, date):
    """ì£¼ë¬¸ì¡°íšŒ ê¸°ë°˜ ì¶”ê°€ ì§€í‘œ ê³„ì‚° - Polars ë²„ì „"""
    logging.info(f"-> {store}({date}) ì£¼ë¬¸ì¡°íšŒ ê¸°ë°˜ ì§€í‘œ ê³„ì‚° (Polars)...")

    # ìƒí’ˆì£¼ë¬¸ë²ˆí˜¸ ì»¬ëŸ¼ì´ ìˆëŠ” ê²½ìš°
    if 'ìƒí’ˆì£¼ë¬¸ë²ˆí˜¸' in order_df.columns:
        # ê²°ì œìˆ˜ (ìƒí’ˆì£¼ë¬¸ë²ˆí˜¸ ê°œìˆ˜)
        order_count = order_df.group_by(['ìƒí’ˆID', 'ì˜µì…˜ì •ë³´']).agg([
            pl.col('ìƒí’ˆì£¼ë¬¸ë²ˆí˜¸').n_unique().alias('ê²°ì œìˆ˜')
        ])

        final_df = final_df.join(order_count, on=['ìƒí’ˆID', 'ì˜µì…˜ì •ë³´'], how='left')
        final_df = final_df.with_columns([pl.col('ê²°ì œìˆ˜').fill_null(0)])

        # í™˜ë¶ˆê±´ìˆ˜ (í™˜ë¶ˆ ìƒíƒœì¸ ì£¼ë¬¸ë²ˆí˜¸ ê°œìˆ˜)
        cancel_orders = order_df.filter(pl.col('í´ë ˆì„ìƒíƒœ').is_in(config.CANCEL_OR_REFUND_STATUSES))
        if cancel_orders.height > 0:
            refund_count = cancel_orders.group_by(['ìƒí’ˆID', 'ì˜µì…˜ì •ë³´']).agg([
                pl.col('ìƒí’ˆì£¼ë¬¸ë²ˆí˜¸').n_unique().alias('í™˜ë¶€ê±´ìˆ˜')
            ])

            final_df = final_df.join(refund_count, on=['ìƒí’ˆID', 'ì˜µì…˜ì •ë³´'], how='left')
            final_df = final_df.with_columns([pl.col('í™˜ë¶€ê±´ìˆ˜').fill_null(0)])
        else:
            final_df = final_df.with_columns([pl.lit(0).alias('í™˜ë¶€ê±´ìˆ˜')])
    else:
        final_df = final_df.with_columns([
            pl.lit(0).alias('ê²°ì œìˆ˜'),
            pl.lit(0).alias('í™˜ë¶€ê±´ìˆ˜')
        ])

    return final_df


def save_report_polars(final_df, output_path, store, date):
    """ìµœì¢… ë¦¬í¬íŠ¸ ì €ì¥ - Polars ë²„ì „"""
    try:
        # ìµœì¢… ì»¬ëŸ¼ ì •ë¦¬
        final_columns = [col for col in config.COLUMNS_TO_KEEP if col in final_df.columns]
        sorted_df = final_df.select(final_columns).sort(['ìƒí’ˆëª…', 'ì˜µì…˜ì •ë³´'])

        # ë°ì´í„° ìš”ì•½ ë¡œê¹…
        logging.info(f"-> {store}({date}) ìµœì¢… ë°ì´í„° ìš”ì•½ (Polars):")
        logging.info(f"   - ì´ ì˜µì…˜ ìˆ˜: {sorted_df.height}")
        logging.info(f"   - ì´ íŒë§¤ìˆ˜ëŸ‰: {sorted_df.select(pl.col('ìˆ˜ëŸ‰').sum()).item()}")
        logging.info(f"   - ì´ í™˜ë¶€ìˆ˜ëŸ‰: {sorted_df.select(pl.col('í™˜ë¶€ìˆ˜ëŸ‰').sum()).item()}")
        logging.info(f"   - ì´ ë§¤ì¶œ: {sorted_df.select(pl.col('ë§¤ì¶œ').sum()).item():,.0f}ì›")
        logging.info(f"   - ì´ íŒë§¤ë§ˆì§„: {sorted_df.select(pl.col('íŒë§¤ë§ˆì§„').sum()).item():,.0f}ì›")

        # pandasë¡œ ë³€í™˜í•˜ì—¬ Excel ì €ì¥ (ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€)
        sorted_df_pandas = sorted_df.to_pandas()

        # í”¼ë²— í…Œì´ë¸” ìƒì„±
        pivot_quantity = sorted_df_pandas.pivot_table(
            index='ìƒí’ˆëª…',
            columns='ì˜µì…˜ì •ë³´',
            values='ìˆ˜ëŸ‰',
            aggfunc='sum',
            fill_value=0
        )

        pivot_margin = sorted_df_pandas.pivot_table(
            index='ìƒí’ˆëª…',
            columns='ì˜µì…˜ì •ë³´',
            values='íŒë§¤ë§ˆì§„',
            aggfunc='sum',
            fill_value=0
        )

        # Excel íŒŒì¼ ìƒì„±
        with pd.ExcelWriter(output_path, engine='xlsxwriter') as writer:
            sorted_df_pandas.to_excel(writer, sheet_name='ì •ë¦¬ëœ ë°ì´í„°', index=False)
            pivot_quantity.to_excel(writer, sheet_name='ì˜µì…˜ë³„ íŒë§¤ìˆ˜ëŸ‰')
            pivot_margin.to_excel(writer, sheet_name='ì˜µì…˜ë³„ íŒë§¤ë§ˆì§„')

            # í‘œ ì„œì‹ ì ìš©
            worksheet = writer.sheets['ì •ë¦¬ëœ ë°ì´í„°']
            (max_row, max_col) = sorted_df_pandas.shape
            worksheet.add_table(0, 0, max_row, max_col - 1, {
                'columns': [{'header': col} for col in sorted_df_pandas.columns]
            })

            # ì»¬ëŸ¼ í­ ìë™ ì¡°ì •
            for i, col in enumerate(sorted_df_pandas.columns):
                col_len = max(sorted_df_pandas[col].astype(str).map(len).max(), len(col)) + 2
                worksheet.set_column(i, i, col_len)

        # ìƒì„± ì™„ë£Œ í™•ì¸
        if os.path.exists(output_path):
            file_size = os.path.getsize(output_path)
            output_filename = os.path.basename(output_path)
            logging.info(f"-> '{output_filename}' ìƒì„± ì™„ë£Œ (Polars): (íŒŒì¼ í¬ê¸°: {file_size:,} bytes)")
            return True
        else:
            logging.error(f"-> {store}({date}) íŒŒì¼ ìƒì„± ì‹¤íŒ¨")
            return False

    except Exception as e:
        logging.error(f"-> {store}({date}) ë¦¬í¬íŠ¸ ì €ì¥ ì¤‘ ì˜¤ë¥˜ (Polars): {e}")
        return False


# ê¸°ì¡´ í•¨ìˆ˜ì™€ì˜ í˜¸í™˜ì„±ì„ ìœ„í•œ ë˜í¼ í•¨ìˆ˜
def generate_individual_reports():
    """ê¸°ì¡´ pandas ë²„ì „ê³¼ì˜ í˜¸í™˜ì„±ì„ ìœ„í•œ ë˜í¼ í•¨ìˆ˜"""
    if USE_POLARS:
        logging.info("âš¡ Polars ì—”ì§„ì„ ì‚¬ìš©í•˜ì—¬ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
        return generate_individual_reports_polars()
    else:
        logging.info("ğŸŒ Pandas ì—”ì§„ì„ ì‚¬ìš©í•˜ì—¬ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
        # ê¸°ì¡´ pandas ë²„ì „ í˜¸ì¶œ (importëœ ê²½ìš°)
        try:
            from . import report_generator
            return report_generator.generate_individual_reports()
        except ImportError:
            logging.error("Pandas ë²„ì „ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Polars ë²„ì „ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            return generate_individual_reports_polars()